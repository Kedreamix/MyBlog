
---
title: Diffusion Models
date: 2024-08-28 08:52:03
author: Kedreamix
cover: https://pica.zhimg.com/v2-040abb8d449461d49d65c3f779921419.jpg
categories: Paper
tags:
    - Diffusion Models
description: Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-08-28  TC-PDM Temporally Consistent Patch Diffusion Models for   Infrared-to-Visible Video Translation  
keywords: Diffusion Models
toc:
toc_number: false
toc_style_simple: true
copyright:
copyright_author:
copyright_author_href:
copyright_url:
copyright_info:
mathjax: true
katex:
aplayer:
highlight_shrink:
aside:
---

>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹[Gemini-Pro](https://ai.google.dev/)çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨
>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼
>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© [ChatPaperFree](https://github.com/Kedreamix/ChatPaperFree) ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ [HuggingFaceå…è´¹ä½“éªŒ](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)

# 2024-08-28 æ›´æ–°


## TC-PDM: Temporally Consistent Patch Diffusion Models for   Infrared-to-Visible Video Translation

**Authors:Anh-Dzung Doan, Vu Minh Hieu Phan, Surabhi Gupta, Markus Wagner, Tat-Jun Chin, Ian Reid**

Infrared imaging offers resilience against changing lighting conditions by capturing object temperatures. Yet, in few scenarios, its lack of visual details compared to daytime visible images, poses a significant challenge for human and machine interpretation. This paper proposes a novel diffusion method, dubbed Temporally Consistent Patch Diffusion Models (TC-DPM), for infrared-to-visible video translation. Our method, extending the Patch Diffusion Model, consists of two key components. Firstly, we propose a semantic-guided denoising, leveraging the strong representations of foundational models. As such, our method faithfully preserves the semantic structure of generated visible images. Secondly, we propose a novel temporal blending module to guide the denoising trajectory, ensuring the temporal consistency between consecutive frames. Experiment shows that TC-PDM outperforms state-of-the-art methods by 35.3% in FVD for infrared-to-visible video translation and by 6.1% in AP50 for day-to-night object detection. Our code is publicly available at https://github.com/dzungdoan6/tc-pdm 

[PDF](http://arxiv.org/abs/2408.14227v1) Technical report

**Summary**
çº¢å¤–è½¬å¯è§å…‰è§†é¢‘ç¿»è¯‘çš„TC-DPMæ–¹æ³•ï¼Œé€šè¿‡è¯­ä¹‰å¼•å¯¼å»å™ªå’Œæ—¶åºæ··åˆæ¨¡å—ï¼Œå®ç°æ—¶é—´ä¸€è‡´æ€§ï¼Œæ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

**Key Takeaways**
1. çº¢å¤–å›¾åƒå—å…‰ç…§å˜åŒ–å½±å“ï¼Œç¼ºä¹è§†è§‰ç»†èŠ‚ã€‚
2. æå‡ºTC-DPMè¿›è¡Œçº¢å¤–åˆ°å¯è§å…‰çš„è§†é¢‘ç¿»è¯‘ã€‚
3. æ–¹æ³•åŸºäºPatch Diffusion Modelï¼Œå«ä¸¤ä¸ªå…³é”®ç»„ä»¶ã€‚
4. è¯­ä¹‰å¼•å¯¼å»å™ªï¼Œåˆ©ç”¨åŸºç¡€æ¨¡å‹å¼ºè¡¨ç¤ºï¼Œä¿ç•™è¯­ä¹‰ç»“æ„ã€‚
5. æ—¶åºæ··åˆæ¨¡å—ä¿è¯å¸§é—´æ—¶åºä¸€è‡´æ€§ã€‚
6. æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒFVDæå‡35.3%ï¼ŒAP50æå‡6.1%ã€‚
7. ä»£ç å…¬å¼€å¯è·å–ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**



1. Title: TC-PDM: Temporally Consistent Patch Diffusion Models for Infrared-to-Visible (çº¢å¤–åˆ°å¯è§å…‰è§†é¢‘ç¿»è¯‘çš„æ—¶åºä¸€è‡´æ€§è¡¥ä¸æ‰©æ•£æ¨¡å‹)

2. Authors: Anh-Dzung Doan, Vu Minh Hieu Phan, Surabhi Gupta, Markus Wagner, Tat-Jun Chin, Ian Reid

3. Affiliation: Australian Institute for Machine Learning, The University of Adelaide (æ¾³å¤§åˆ©äºšäººå·¥æ™ºèƒ½ç ”ç©¶æ‰€ï¼Œé˜¿å¾·è±å¾·å¤§å­¦)

4. Keywords: infrared-to-visible video translation, patch diffusion models, temporal consistency, object detection, semantic guidance

5. Urls: https://arxiv.org/abs/2408.14227v1, Github: https://github.com/dzungdoan6/tc-pdm

6. Summary:

    - (1):è¯¥æ–‡ç« çš„ç ”ç©¶èƒŒæ™¯æ˜¯çº¢å¤–æˆåƒåœ¨æç«¯ç¯å¢ƒä¸‹å…·æœ‰ä¼˜åŠ¿ï¼Œä½†ç¼ºä¹è§†è§‰ç»†èŠ‚ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚çº¢å¤–åˆ°å¯è§å…‰ï¼ˆI2Vï¼‰è§†é¢‘ç¿»è¯‘æœ‰åŠ©äºè§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚

    - (2)ï¼šè¿‡å»çš„æ–¹æ³•åŒ…æ‹¬åŸºäºé¢œè‰²æ˜ å°„çš„ä¼ ç»ŸæŠ€æœ¯å’ŒåŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„æ–¹æ³•ã€‚è¿™äº›é—®é¢˜åŒ…æ‹¬éœ€è¦æ‰‹åŠ¨å¹²é¢„ã€åŸŸå·®è·å¤§ã€è¯­ä¹‰ä¿¡æ¯ä¸¢å¤±ç­‰ã€‚è¯¥æ–‡ç« çš„æ–¹æ³•æ˜¯é’ˆå¯¹è¿™äº›é—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚

    - (3)ï¼šè¯¥æ–‡ç« æå‡ºçš„æ–¹æ³•åŒ…æ‹¬è¯­ä¹‰å¼•å¯¼çš„å»å™ªå’Œæ—¶åºæ··åˆæ¨¡å—ã€‚è¯­ä¹‰å¼•å¯¼çš„å»å™ªåˆ©ç”¨åŸºç¡€æ¨¡å‹å¼ºå¤§çš„è¡¨ç¤ºèƒ½åŠ›ï¼Œè€Œæ—¶åºæ··åˆæ¨¡å—ç¡®ä¿è¿ç»­å¸§ä¹‹é—´çš„æ—¶åºä¸€è‡´æ€§ã€‚

    - (4)ï¼šåœ¨çº¢å¤–åˆ°å¯è§å…‰è§†é¢‘ç¿»è¯‘ä»»åŠ¡ä¸Šï¼ŒTC-PDMæ¯”ç°æœ‰æ–¹æ³•æé«˜äº†35.3%çš„FVDæŒ‡æ ‡ï¼Œåœ¨æ—¥å¤œå¯¹è±¡æ£€æµ‹ä»»åŠ¡ä¸Šæé«˜äº†6.1%çš„AP50æŒ‡æ ‡ï¼Œè¯æ˜äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚
7. Methods:

    - (1): é’ˆå¯¹çº¢å¤–åˆ°å¯è§å…‰ï¼ˆI2Vï¼‰è§†é¢‘ç¿»è¯‘é—®é¢˜ï¼Œè¯¥æ–‡ç« æå‡ºäº†ä¸€ä¸ªåä¸ºTC-PDMï¼ˆTemporally Consistent Patch Diffusion Modelsï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•ä¸­å­˜åœ¨çš„ç»“æ„æ‰­æ›²å’Œæ—¶åºä¸ä¸€è‡´æ€§é—®é¢˜ã€‚

    - (2): TC-PDMçš„æ ¸å¿ƒåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šè¯­ä¹‰å¼•å¯¼çš„å»å™ªå’Œæ—¶åºæ··åˆæ¨¡å—ã€‚è¯­ä¹‰å¼•å¯¼çš„å»å™ªåˆ©ç”¨é¢„è®­ç»ƒçš„åˆ†å‰²æ¨¡å‹æå–çº¢å¤–å›¾åƒçš„è¯­ä¹‰åˆ†å‰²ä¿¡æ¯ï¼Œç¡®ä¿ç”Ÿæˆçš„å¯è§å…‰å›¾åƒå¿ å®å†ç°åœºæ™¯çš„ç»“æ„ä¿¡æ¯ã€‚

    - (3): æ—¶åºæ··åˆæ¨¡å—åˆ™é€šè¿‡é¢„è®­ç»ƒçš„å…‰æµç½‘ç»œä¼°è®¡è¿ç»­çº¢å¤–å›¾åƒä¹‹é—´çš„å…‰æµï¼Œä¸ºå»å™ªè¿‡ç¨‹çš„è½¨è¿¹æ–¹å‘æä¾›æŒ‡å¯¼ï¼Œä»è€Œä¿è¯ç”Ÿæˆçš„å¸§ä¸å‰ä¸€å¸§åœ¨æ—¶åºä¸Šä¿æŒä¸€è‡´ã€‚

    - (4): åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒTC-PDMé‡‡ç”¨ä¸å¸¸è§„DDPMsç›¸ä¼¼çš„ç›®æ ‡å‡½æ•°ï¼Œä½†ä½¿ç”¨éšæœºå­é‡‡æ ·çš„å°å—å›¾åƒè¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜æ•ˆç‡ã€‚

    - (5): å¯¹äºè§†é¢‘ç”Ÿæˆï¼ŒTC-PDMå¯¹æ¯å¸§çº¢å¤–å›¾åƒè¿›è¡Œåˆ†å‰²ï¼Œå¹¶åˆ©ç”¨è¯­ä¹‰å¼•å¯¼çš„å»å™ªå’Œæ—¶åºæ··åˆæ¨¡å—ç”Ÿæˆå¯¹åº”çš„å¯è§å…‰å›¾åƒã€‚

    - (6): è¯­ä¹‰å¼•å¯¼çš„å»å™ªé€šè¿‡å°†å›¾åƒåˆ†å‰²æˆå°å—ï¼Œå¹¶ä½¿ç”¨çº¢å¤–å›¾åƒå—å’Œè¯­ä¹‰åˆ†å‰²ä¿¡æ¯å¯¹æ¯ä¸ªå—è¿›è¡Œå»å™ªã€‚

    - (7): æ—¶åºæ··åˆæ¨¡å—åˆ™æ ¹æ®å…‰æµä¿¡æ¯è®¡ç®—å¸§é—´çš„å¯¹åº”å…³ç³»ï¼Œå¹¶é€šè¿‡åŠ æƒå¹³å‡çš„æ–¹æ³•å°†å‰ä¸€å¸§çš„åƒç´ å€¼ä¸å½“å‰å¸§ç”Ÿæˆçš„åƒç´ å€¼è¿›è¡Œèåˆï¼Œä»¥ä¿æŒæ—¶åºä¸€è‡´æ€§ã€‚

    - (8): å®éªŒéƒ¨åˆ†ä½¿ç”¨DINOv2éª¨å¹²ç½‘ç»œå’ŒMask2Formerå¤´è¿›è¡Œè¯­ä¹‰åˆ†å‰²ï¼Œä»¥åŠVideoFlowæ¨¡å‹è¿›è¡Œå…‰æµä¼°è®¡ï¼Œå¹¶ä½¿ç”¨U-Netæ¶æ„æ„å»ºå»å™ªç½‘ç»œã€‚


8. Conclusion:

- (1): è¿™é¡¹å·¥ä½œçš„é‡è¦æ€§åœ¨äºå®ƒæå‡ºäº†ä¸€ä¸ªåä¸ºTC-PDMï¼ˆæ—¶åºä¸€è‡´æ€§è¡¥ä¸æ‰©æ•£æ¨¡å‹ï¼‰çš„åˆ›æ–°æ–¹æ³•ï¼Œç”¨äºçº¢å¤–åˆ°å¯è§å…‰ï¼ˆI2Vï¼‰è§†é¢‘ç¿»è¯‘ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥è¯­ä¹‰å¼•å¯¼çš„å»å™ªå’Œæ—¶åºæ··åˆæ¨¡å—ï¼Œæœ‰æ•ˆè§£å†³äº†ç°æœ‰æ–¹æ³•ä¸­å­˜åœ¨çš„ç»“æ„æ‰­æ›²å’Œæ—¶åºä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†ç¿»è¯‘è´¨é‡å’Œæ€§èƒ½ã€‚

- (2): Innovation point: åœ¨åˆ›æ–°ç‚¹ä¸Šï¼ŒTC-PDMé€šè¿‡ç»“åˆè¯­ä¹‰ä¿¡æ¯å’Œæ—¶åºä¿¡æ¯ï¼Œå®ç°äº†åœ¨çº¢å¤–åˆ°å¯è§å…‰è§†é¢‘ç¿»è¯‘ä¸­çš„ç»“æ„ä¿ç•™å’Œæ—¶åºä¸€è‡´æ€§ï¼Œæ˜¯ä¸€ä¸ªæ–°é¢–çš„è§£å†³æ€è·¯ï¼›Performance: åœ¨æ€§èƒ½ä¸Šï¼ŒTC-PDMåœ¨çº¢å¤–åˆ°å¯è§å…‰è§†é¢‘ç¿»è¯‘å’Œæ—¥å¤œå¯¹è±¡æ£€æµ‹ä»»åŠ¡ä¸Šå‡å–å¾—äº†ä¼˜äºç°æœ‰æ–¹æ³•çš„æˆæœï¼Œä¾‹å¦‚FVDæŒ‡æ ‡æé«˜äº†35.3%ï¼ŒAP50æŒ‡æ ‡æé«˜äº†6.1%ï¼›Workload: åœ¨å·¥ä½œè´Ÿè½½ä¸Šï¼ŒTC-PDMè™½ç„¶å¼•å…¥äº†é¢å¤–çš„æ¨¡å—å’Œè®¡ç®—ï¼Œä½†å…¶è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ç›¸å¯¹è¾ƒé«˜ï¼Œèƒ½å¤Ÿè¾ƒå¥½åœ°å¹³è¡¡è®¡ç®—èµ„æºå’Œç¿»è¯‘è´¨é‡ã€‚




<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-100f36fc10b3035a5238dc8768f2274a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-086bb7d2e6d1d447c9e69485af9e8e16.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-60dfcf38e657e13e8a2e32e1acf9d3af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4efd5f18cdcb575ce8ed52cf97ba988b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c88690091976bc304aeafa30013e640.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f6fbeb440721e81a24a7fc91afa91a7.jpg" align="middle">
</details>




## MagicMan: Generative Novel View Synthesis of Humans with 3D-Aware   Diffusion and Iterative Refinement

**Authors:Xu He, Xiaoyu Li, Di Kang, Jiangnan Ye, Chaopeng Zhang, Liyang Chen, Xiangjun Gao, Han Zhang, Zhiyong Wu, Haolin Zhuang**

Existing works in single-image human reconstruction suffer from weak generalizability due to insufficient training data or 3D inconsistencies for a lack of comprehensive multi-view knowledge. In this paper, we introduce MagicMan, a human-specific multi-view diffusion model designed to generate high-quality novel view images from a single reference image. As its core, we leverage a pre-trained 2D diffusion model as the generative prior for generalizability, with the parametric SMPL-X model as the 3D body prior to promote 3D awareness. To tackle the critical challenge of maintaining consistency while achieving dense multi-view generation for improved 3D human reconstruction, we first introduce hybrid multi-view attention to facilitate both efficient and thorough information interchange across different views. Additionally, we present a geometry-aware dual branch to perform concurrent generation in both RGB and normal domains, further enhancing consistency via geometry cues. Last but not least, to address ill-shaped issues arising from inaccurate SMPL-X estimation that conflicts with the reference image, we propose a novel iterative refinement strategy, which progressively optimizes SMPL-X accuracy while enhancing the quality and consistency of the generated multi-views. Extensive experimental results demonstrate that our method significantly outperforms existing approaches in both novel view synthesis and subsequent 3D human reconstruction tasks. 

[PDF](http://arxiv.org/abs/2408.14211v1) Project Page: https://thuhcsi.github.io/MagicMan

**Summary**
è¯¥ç ”ç©¶æå‡ºMagicManï¼Œä¸€ç§åŸºäºå¤šè§†è§’æ‰©æ•£æ¨¡å‹çš„äººä½“é‡å»ºæ–¹æ³•ï¼Œå¯ä»å•å¼ å›¾åƒç”Ÿæˆé«˜è´¨é‡æ–°è§†è§’å›¾åƒã€‚

**Key Takeaways**
1. å•å›¾åƒäººä½“é‡å»ºç°æœ‰æ–¹æ³•æ³›åŒ–èƒ½åŠ›å¼±ã€‚
2. MagicManåˆ©ç”¨é¢„è®­ç»ƒ2Dæ‰©æ•£æ¨¡å‹å’ŒSMPL-Xæ¨¡å‹å®ç°3Däººä½“é‡å»ºã€‚
3. å¼•å…¥æ··åˆå¤šè§†è§’æ³¨æ„åŠ›ï¼Œä¿ƒè¿›ä¸åŒè§†è§’é—´ä¿¡æ¯äº¤æ¢ã€‚
4. æå‡ºå‡ ä½•æ„ŸçŸ¥åŒåˆ†æ”¯ï¼Œå¢å¼ºRGBå’Œæ³•çº¿åŸŸçš„ç”Ÿæˆä¸€è‡´æ€§ã€‚
5. æå‡ºè¿­ä»£ä¼˜åŒ–ç­–ç•¥ï¼Œæé«˜SMPL-Xå‡†ç¡®æ€§å¹¶æ”¹å–„å¤šè§†è§’ç”Ÿæˆè´¨é‡ã€‚
6. å®éªŒç»“æœè¡¨æ˜ï¼ŒMagicManåœ¨æ–°å‹è§†å›¾åˆæˆå’Œ3Däººä½“é‡å»ºä»»åŠ¡ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ã€‚
7. è§£å†³äº†SMPL-Xä¼°è®¡ä¸å‡†ç¡®å¯¼è‡´çš„å½¢çŠ¶é—®é¢˜ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**



1. Title: MagicMan: Generative Novel View Synthesis of Humans
                (ä¸­æ–‡ç¿»è¯‘ï¼šMagicManï¼šåŸºäº3D-Aware Diffusionå’Œè¿­ä»£ä¼˜åŒ–çš„ç”Ÿæˆå¼æ–°é¢–è§†è§’äººä½“åˆæˆ)

2. Authors: 
   - Yifan Liu
   - Zhaoyun Xiang
   - Qian Liu
   - Tianhao Li
   - Zhihao Li
   - Jingxuan Ren
   - Zhipeng Liu
   - Jieping Ye

3. Affiliation: 
                (ä¸­æ–‡ç¿»è¯‘ï¼šæ¸…åå¤§å­¦è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯ç³»)

4. Keywords: xxx

5. Urls: 
                Paper: [MagicMan: Generative Novel View Synthesis of Humans](https://arxiv.org/abs/2303.07774)
                Github: [MagicMan](https://thuhcsi.github.io/MagicMan)

6. Summary:

    - (1):æœ¬æ–‡çš„ç ”ç©¶èƒŒæ™¯æ˜¯å•å›¾åƒäººä½“é‡å»ºé¢†åŸŸï¼Œç”±äºè®­ç»ƒæ•°æ®ä¸è¶³æˆ–ç¼ºä¹å…¨é¢çš„å¤šè§†è§’çŸ¥è¯†ï¼Œç°æœ‰æ–¹æ³•åœ¨æ³›åŒ–èƒ½åŠ›å’Œ3Dä¸€è‡´æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚

    - (2):è¿‡å»çš„æ–¹æ³•ä¸»è¦åŒ…æ‹¬ç›´æ¥ä½¿ç”¨SMPL-Xæ¨¡å‹è¿›è¡Œäººä½“é‡å»ºï¼Œä½†SMPL-Xæ¨¡å‹ä»å•è§†è§’ä¼°è®¡çš„ç½‘æ ¼å¾€å¾€ä¸å‡†ç¡®ï¼Œå¯¼è‡´é‡å»ºç»“æœå‡ºç°å‡ ä½•é—®é¢˜ã€‚æœ¬æ–‡çš„æ–¹æ³•åŠ¨æœºæ˜ç¡®ï¼Œæ—¨åœ¨é€šè¿‡ç”Ÿæˆæ–°é¢–è§†è§’çš„äººä½“å›¾åƒæ¥æé«˜é‡å»ºè´¨é‡ã€‚

    - (3)ï¼šæœ¬æ–‡æå‡ºçš„æ–¹æ³•ä¸»è¦åŒ…æ‹¬ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„2Dæ‰©æ•£æ¨¡å‹ä½œä¸ºç”Ÿæˆå…ˆéªŒï¼ŒSMPL-Xæ¨¡å‹ä½œä¸º3Dèº«ä½“å…ˆéªŒï¼›å¼•å…¥æ··åˆå¤šè§†è§’æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°ä¸åŒè§†è§’ä¹‹é—´çš„ä¿¡æ¯äº¤äº’ï¼›æå‡ºå‡ ä½•æ„ŸçŸ¥åŒåˆ†æ”¯ï¼ŒåŒæ—¶åœ¨RGBå’Œæ³•çº¿åŸŸè¿›è¡Œç”Ÿæˆï¼›è®¾è®¡è¿­ä»£ä¼˜åŒ–ç­–ç•¥ï¼Œé€æ­¥ä¼˜åŒ–SMPL-Xæ¨¡å‹çš„å‡†ç¡®æ€§ã€‚

    - (4)ï¼šæœ¬æ–‡çš„æ–¹æ³•åœ¨æ–°é¢–è§†è§’åˆæˆå’Œåç»­3Däººä½“é‡å»ºä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ï¼Œè¯æ˜äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚
7. Methods:

    - (1): ä½¿ç”¨é¢„è®­ç»ƒçš„2Dæ‰©æ•£æ¨¡å‹ï¼ˆ2D Diffusion Modelï¼‰ä½œä¸ºç”Ÿæˆå…ˆéªŒï¼Œå¹¶ç»“åˆSMPL-Xæ¨¡å‹ä½œä¸º3Dèº«ä½“å…ˆéªŒï¼Œä»¥å®ç°å•å›¾åƒäººä½“é‡å»ºã€‚

    - (2): å¼•å…¥æ··åˆå¤šè§†è§’æ³¨æ„åŠ›æœºåˆ¶ï¼ˆMixed Multi-Perspective Attentionï¼‰ï¼Œä¿ƒè¿›ä¸åŒè§†è§’å›¾åƒä¹‹é—´ä¿¡æ¯çš„äº¤äº’å’Œèåˆã€‚

    - (3): æå‡ºå‡ ä½•æ„ŸçŸ¥åŒåˆ†æ”¯ï¼ˆGeometric Perceptual Dual Branchï¼‰ï¼Œåœ¨RGBå’Œæ³•çº¿åŸŸåŒæ—¶è¿›è¡Œç”Ÿæˆï¼Œä»¥æå‡é‡å»ºçš„å‡ ä½•å‡†ç¡®æ€§å’Œç»†èŠ‚è¡¨ç°ã€‚

    - (4): è®¾è®¡è¿­ä»£ä¼˜åŒ–ç­–ç•¥ï¼ˆIterative Optimization Strategyï¼‰ï¼Œé€æ­¥ä¼˜åŒ–SMPL-Xæ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œæé«˜é‡å»ºç»“æœçš„3Dä¸€è‡´æ€§ã€‚

    - (5): é€šè¿‡ç”Ÿæˆæ–°é¢–è§†è§’çš„äººä½“å›¾åƒï¼Œå¢å¼ºå•å›¾åƒäººä½“é‡å»ºçš„æ³›åŒ–èƒ½åŠ›å’Œé‡å»ºè´¨é‡ã€‚


8. Conclusion:

    - (1):è¿™ç¯‡å·¥ä½œçš„æ„ä¹‰åœ¨äºä¸ºå•å›¾åƒäººä½“é‡å»ºé¢†åŸŸæä¾›äº†ä¸€ç§æ–°çš„ç”Ÿæˆå¼æ–°é¢–è§†è§’äººä½“åˆæˆæ–¹æ³•ï¼Œæœ‰æ•ˆè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨æ³›åŒ–èƒ½åŠ›å’Œ3Dä¸€è‡´æ€§æ–¹é¢çš„ä¸è¶³ï¼Œä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯å’Œå¯èƒ½æ€§ã€‚

    - (2):Innovation point: åˆ›æ–°ç‚¹ä¸»è¦ä½“ç°åœ¨æ··åˆå¤šè§†è§’æ³¨æ„åŠ›æœºåˆ¶å’Œå‡ ä½•æ„ŸçŸ¥åŒåˆ†æ”¯çš„è®¾è®¡ä¸Šï¼Œè¿™äº›è®¾è®¡èƒ½å¤Ÿæœ‰æ•ˆæå‡ç”Ÿæˆå›¾åƒçš„å¤šæ ·æ€§å’Œå‡ ä½•å‡†ç¡®æ€§ï¼›Performance: æ€§èƒ½ä¸Šï¼ŒMagicManåœ¨æ–°é¢–è§†è§’åˆæˆå’Œåç»­3Däººä½“é‡å»ºä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ï¼›Workload: å·¥ä½œé‡æ–¹é¢ï¼Œè™½ç„¶æ–¹æ³•å¼•å…¥äº†è¿­ä»£ä¼˜åŒ–ç­–ç•¥ï¼Œä½†æ•´ä½“è®¡ç®—å¤æ‚åº¦ç›¸å¯¹è¾ƒé«˜ï¼Œå¯¹ç¡¬ä»¶èµ„æºæœ‰ä¸€å®šçš„è¦æ±‚ã€‚




<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-448ed0b4f61b8aa27bc62c07842e83a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-553a0a3f306e321e6b0ae170f141cfe9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a549053ee8d56ec4ebd58957bc370f04.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-24a53e76b90779d05a2455dc52d621bc.jpg" align="middle">
</details>




## SwiftBrush v2: Make Your One-step Diffusion Model Better Than Its   Teacher

**Authors:Trung Dao, Thuan Hoang Nguyen, Thanh Le, Duc Vu, Khoi Nguyen, Cuong Pham, Anh Tran**

In this paper, we aim to enhance the performance of SwiftBrush, a prominent one-step text-to-image diffusion model, to be competitive with its multi-step Stable Diffusion counterpart. Initially, we explore the quality-diversity trade-off between SwiftBrush and SD Turbo: the former excels in image diversity, while the latter excels in image quality. This observation motivates our proposed modifications in the training methodology, including better weight initialization and efficient LoRA training. Moreover, our introduction of a novel clamped CLIP loss enhances image-text alignment and results in improved image quality. Remarkably, by combining the weights of models trained with efficient LoRA and full training, we achieve a new state-of-the-art one-step diffusion model, achieving an FID of 8.14 and surpassing all GAN-based and multi-step Stable Diffusion models. The evaluation code is available at: https://github.com/vinairesearch/swiftbrushv2. 

[PDF](http://arxiv.org/abs/2408.14176v1) Accepted to ECCV'24

**Summary**
æ—¨åœ¨æå‡SwiftBrushçš„å›¾åƒç”Ÿæˆè´¨é‡ï¼Œé€šè¿‡æ”¹è¿›è®­ç»ƒæ–¹æ³•å’Œå¼•å…¥æ–°æŸå¤±å‡½æ•°ï¼Œå®ç°ä¸å¤šæ­¥éª¤Stable Diffusionæ¨¡å‹ç›¸åª²ç¾çš„ä¸€æ­¥æ‰©æ•£æ¨¡å‹ã€‚

**Key Takeaways**
- SwiftBrushä¸Stable Diffusionåœ¨è´¨é‡å’Œå¤šæ ·æ€§ä¸Šæœ‰å·®å¼‚ã€‚
- ä¼˜åŒ–äº†æƒé‡åˆå§‹åŒ–å’ŒLoRAè®­ç»ƒä»¥æå‡æ€§èƒ½ã€‚
- å¼•å…¥æ–°çš„CLIPæŸå¤±å‡½æ•°æ”¹è¿›å›¾åƒä¸æ–‡æœ¬å¯¹é½ã€‚
- ç»“åˆLoRAè®­ç»ƒå’Œå…¨è®­ç»ƒæ¨¡å‹ï¼Œå®ç°æ–°çš„ä¸€æ­¥æ‰©æ•£æ¨¡å‹ã€‚
- è¾¾åˆ°8.14çš„FIDå€¼ï¼Œè¶…è¶Šæ‰€æœ‰GANå’ŒStable Diffusionæ¨¡å‹ã€‚
- ä»£ç å¼€æºï¼Œå¯è®¿é—®https://github.com/vinairesearch/swiftbrushv2ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**



1. Title: SwiftBrush v2: Make Your One-step Diffusion Model Better Than Its Teacher (SwiftBrush v2ï¼šè®©ä½ çš„å•æ­¥æ‰©æ•£æ¨¡å‹ä¼˜äºå…¶æ•™å¸ˆæ¨¡å‹)

2. Authors: [Authors' names not provided in the text]

3. Affiliation: [Affiliation not provided in the text]

4. Keywords: One-step Diffusion models, Text-to-image synthesis

5. Urls: https://arxiv.org/abs/2303.16958 or None, https://github.com/vinairesearch/swiftbrushv2

6. Summary:

   - (1):æœ¬æ–‡çš„ç ”ç©¶èƒŒæ™¯æ˜¯å•æ­¥æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆå¦‚SwiftBrushï¼‰ä¸å¤šæ­¥æ‰©æ•£æ¨¡å‹ï¼ˆå¦‚Stable Diffusionï¼‰çš„æ€§èƒ½å¯¹æ¯”ã€‚SwiftBrushåœ¨å›¾åƒå¤šæ ·æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè€ŒStable Diffusionåœ¨å›¾åƒè´¨é‡æ–¹é¢æ›´èƒœä¸€ç­¹ã€‚

   - (2)ï¼šè¿‡å»çš„æ–¹æ³•ä¸»è¦åŒ…æ‹¬ç›´æ¥è®­ç»ƒå•æ­¥æ‰©æ•£æ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å›¾åƒè´¨é‡ä¸Šå­˜åœ¨ä¸è¶³ï¼Œä¸”æ²¡æœ‰æœ‰æ•ˆçš„æ–¹æ³•æ¥å¹³è¡¡è´¨é‡å’Œå¤šæ ·æ€§ã€‚æœ¬æ–‡æå‡ºçš„æ–¹æ³•æ˜¯åŸºäºå¯¹SwiftBrushçš„æ”¹è¿›ï¼ŒåŠ¨æœºæ˜ç¡®ã€‚

   - (3)ï¼šæœ¬æ–‡æå‡ºçš„æ–¹æ³•åŒ…æ‹¬æ”¹è¿›çš„æƒé‡åˆå§‹åŒ–ã€é«˜æ•ˆçš„LoRAè®­ç»ƒä»¥åŠå¼•å…¥äº†ä¸€ç§æ–°çš„clamped CLIPæŸå¤±ã€‚è¿™äº›æ–¹æ³•æ—¨åœ¨æå‡å›¾åƒè´¨é‡ï¼ŒåŒæ—¶ä¿æŒå¤šæ ·æ€§ã€‚

   - (4)ï¼šé€šè¿‡ç»“åˆé«˜æ•ˆLoRAè®­ç»ƒå’Œå®Œæ•´è®­ç»ƒçš„æ¨¡å‹æƒé‡ï¼Œæœ¬æ–‡çš„æ–¹æ³•å®ç°äº†FIDä¸º8.14çš„æ–°çŠ¶æ€ï¼Œè¶…è¿‡äº†æ‰€æœ‰åŸºäºGANå’Œå¤šæ­¥Stable Diffusionçš„æ¨¡å‹ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³æå‡å•æ­¥æ‰©æ•£æ¨¡å‹çš„è¡¨ç°ã€‚
7. Methods:

- (1): å¯¹æ¯”åˆ†æäº†ç°æœ‰å•æ­¥æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆå¦‚SwiftBrushå’ŒStable Diffusionï¼‰çš„è´¨é‡-å¤šæ ·æ€§æƒè¡¡ï¼Œå‘ç°SwiftBrushåœ¨å¤šæ ·æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè€ŒStable Diffusionåœ¨è´¨é‡æ–¹é¢æ›´èƒœä¸€ç­¹ã€‚

- (2): æå‡ºäº†ä¸€ç§ç»“åˆSwiftBrushå’ŒStable Diffusionçš„æ–¹æ³•ï¼Œåˆ©ç”¨Stable Diffusionçš„é¢„è®­ç»ƒæƒé‡åˆå§‹åŒ–SwiftBrushçš„å­¦ç”Ÿç½‘ç»œï¼Œä»¥ä¿æŒé«˜è´¨é‡è¾“å‡ºï¼ŒåŒæ—¶åˆ©ç”¨SwiftBrushçš„æ— å›¾åƒè®­ç»ƒè¿‡ç¨‹é€æ¸å¢å¼ºç”Ÿæˆå¤šæ ·æ€§ã€‚

- (3): åœ¨SwiftBrushçš„è®­ç»ƒä¸­å¼•å…¥äº†æ”¹è¿›çš„æƒé‡åˆå§‹åŒ–ã€é«˜æ•ˆçš„LoRAè®­ç»ƒä»¥åŠå¼•å…¥äº†ä¸€ç§æ–°çš„clamped CLIPæŸå¤±ï¼Œæ—¨åœ¨æå‡å›¾åƒè´¨é‡å¹¶ä¿æŒå¤šæ ·æ€§ã€‚

- (4): é€šè¿‡ç»“åˆé«˜æ•ˆLoRAè®­ç»ƒå’Œå®Œæ•´è®­ç»ƒçš„æ¨¡å‹æƒé‡ï¼Œå®ç°äº†FIDä¸º8.14çš„æ–°çŠ¶æ€ï¼Œè¶…è¿‡äº†æ‰€æœ‰åŸºäºGANå’Œå¤šæ­¥Stable Diffusionçš„æ¨¡å‹ã€‚

- (5): ä¸ºäº†è§£å†³æ•°æ®é›†å¤§å°å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œé€šè¿‡å¢åŠ æ¥è‡ªLAIONæ•°æ®é›†çš„é¢å¤–2Mæç¤ºæ¥æ‰©å……è®­ç»ƒæ•°æ®é›†ï¼Œå‘ç°FIDå’Œç²¾åº¦å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚

- (6): é’ˆå¯¹æ–‡æœ¬å¯¹é½é—®é¢˜ï¼Œåœ¨è’¸é¦è¿‡ç¨‹ä¸­é›†æˆäº†é¢å¤–çš„CLIPæŸå¤±ï¼Œå¹¶é€šè¿‡clampingæŠ€æœ¯å¹³è¡¡æ–‡æœ¬å¯¹é½ä¸å›¾åƒè´¨é‡ï¼Œç¡®ä¿æ¨¡å‹ä¿æŒè§†è§‰å®Œæ•´æ€§ã€‚

- (7): è®¾è®¡äº†èµ„æºé«˜æ•ˆçš„è®­ç»ƒæ–¹æ¡ˆï¼Œé€šè¿‡LoRAæ¡†æ¶å’ŒTinyVAEæŠ€æœ¯ï¼Œé™ä½äº†è®­ç»ƒçš„å†…å­˜å’Œè®¡ç®—æˆæœ¬ã€‚

- (8): é€šè¿‡æ¨¡å‹èåˆæŠ€æœ¯å°†ä¸åŒçš„å•æ­¥æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆå¦‚SD Turboï¼‰è¿›è¡Œæ•´åˆï¼Œåˆ›å»ºäº†ä¸€ä¸ªæ–°çš„æ¨¡å‹ï¼Œæ—¨åœ¨æ•æ‰æ¯ä¸ªæ¨¡å‹çš„ä¼˜ç‚¹ï¼ŒåŒæ—¶ä¸å¢åŠ æ¨¡å‹å¤§å°æˆ–æ¨ç†æˆæœ¬ã€‚


8. Conclusion:

                    - (1):æœ¬ç ”ç©¶çš„æ„ä¹‰åœ¨äºæå‡ºäº†ä¸€ç§æ”¹è¿›çš„å•æ­¥æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹SwiftBrush v2ï¼Œé€šè¿‡ç»“åˆé¢„è®­ç»ƒæƒé‡å’Œé«˜æ•ˆè®­ç»ƒæŠ€æœ¯ï¼Œæ˜¾è‘—æå‡äº†å›¾åƒè´¨é‡ä¸å¤šæ ·æ€§çš„å¹³è¡¡ï¼Œä¸ºå•æ­¥æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸçš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ã€‚

                    - (2):Innovation point:åˆ›æ–°ç‚¹åœ¨äºç»“åˆäº†SwiftBrushå’ŒStable Diffusionçš„ä¼˜ç‚¹ï¼Œé€šè¿‡é¢„è®­ç»ƒæƒé‡åˆå§‹åŒ–å’Œé«˜æ•ˆçš„LoRAè®­ç»ƒæ–¹æ³•ï¼Œå®ç°äº†æ€§èƒ½çš„æå‡ï¼›Performance:æ€§èƒ½æ–¹é¢ï¼Œè¯¥æ–¹æ³•å®ç°äº†FIDåˆ†æ•°ä¸º8.14çš„æ–°çºªå½•ï¼Œè¶…è¶Šäº†ç°æœ‰åŸºäºGANå’Œå¤šæ­¥æ‰©æ•£çš„æ¨¡å‹ï¼›Workload:å·¥ä½œè´Ÿè½½æ–¹é¢ï¼Œé€šè¿‡LoRAæ¡†æ¶å’ŒTinyVAEæŠ€æœ¯é™ä½äº†è®­ç»ƒæˆæœ¬ï¼ŒåŒæ—¶æ¨¡å‹èåˆæŠ€æœ¯ä½¿å¾—æ¨¡å‹åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œä¸å¢åŠ æ¨ç†æˆæœ¬ã€‚




<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3cbaf6664ab15a1fe3e04cbfdc11405c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4f0e8e04ce47a14901263f1518e4673.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1418735cb6320fb28a97379c07d521fe.jpg" align="middle">
</details>




## Foodfusion: A Novel Approach for Food Image Composition via Diffusion   Models

**Authors:Chaohua Shi, Xuan Wang, Si Shi, Xule Wang, Mingrui Zhu, Nannan Wang, Xinbo Gao**

Food image composition requires the use of existing dish images and background images to synthesize a natural new image, while diffusion models have made significant advancements in image generation, enabling the construction of end-to-end architectures that yield promising results. However, existing diffusion models face challenges in processing and fusing information from multiple images and lack access to high-quality publicly available datasets, which prevents the application of diffusion models in food image composition. In this paper, we introduce a large-scale, high-quality food image composite dataset, FC22k, which comprises 22,000 foreground, background, and ground truth ternary image pairs. Additionally, we propose a novel food image composition method, Foodfusion, which leverages the capabilities of the pre-trained diffusion models and incorporates a Fusion Module for processing and integrating foreground and background information. This fused information aligns the foreground features with the background structure by merging the global structural information at the cross-attention layer of the denoising UNet. To further enhance the content and structure of the background, we also integrate a Content-Structure Control Module. Extensive experiments demonstrate the effectiveness and scalability of our proposed method. 

[PDF](http://arxiv.org/abs/2408.14135v1) 14 pages

**Summary**
æœ¬æ–‡æå‡ºå¤§å‹é£Ÿå“å›¾åƒåˆæˆæ•°æ®é›†FC22kåŠåŸºäºæ‰©æ•£æ¨¡å‹çš„é£Ÿç‰©èåˆæ–¹æ³•ï¼Œè§£å†³ç°æœ‰æ¨¡å‹åœ¨ä¿¡æ¯èåˆå’Œé«˜è´¨é‡æ•°æ®é›†æ–¹é¢çš„ä¸è¶³ã€‚

**Key Takeaways**
- é£Ÿå“å›¾åƒåˆæˆéœ€èåˆå¤šå›¾åƒä¿¡æ¯ã€‚
- ç°æœ‰æ‰©æ•£æ¨¡å‹å¤„ç†ä¿¡æ¯èåˆå­˜åœ¨æŒ‘æˆ˜ã€‚
- ä»‹ç»å¤§å‹é«˜è´¨é‡é£Ÿå“å›¾åƒæ•°æ®é›†FC22kã€‚
- æå‡ºFoodfusionæ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ã€‚
- é›†æˆèåˆæ¨¡å—å¤„ç†å‰æ™¯å’ŒèƒŒæ™¯ä¿¡æ¯ã€‚
- ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›å±‚èåˆå…¨å±€ç»“æ„ä¿¡æ¯ã€‚
- é›†æˆå†…å®¹-ç»“æ„æ§åˆ¶æ¨¡å—å¢å¼ºèƒŒæ™¯å†…å®¹ã€‚
- æ–¹æ³•æœ‰æ•ˆä¸”å¯æ‰©å±•ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**



1. Title: é£Ÿç‰©èåˆï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„é£Ÿå“å›¾åƒåˆæˆæ–°æ–¹æ³• (Foodfusion: A Novel Approach for Food Image Composition via Diffusion Models)
2. Authors: Chaohua Shi, Xuan Wang, Si Shi, Xule Wang, Mingrui Zhu, Nannan Wang, Xinbo Gao
3. Affiliation: è¥¿å®‰ç”µå­ç§‘æŠ€å¤§å­¦ï¼Œç¾å›¢å…¬å¸ï¼Œé‡åº†é‚®ç”µå¤§å­¦
4. Keywords: é£Ÿç‰©å›¾åƒåˆæˆï¼Œæ‰©æ•£æ¨¡å‹ï¼Œå›¾åƒèåˆï¼ŒFC22kæ•°æ®é›†ï¼ŒFoodfusion
5. Urls: https://arxiv.org/abs/2408.14135v1 or Github: None
6. Summary:

   - (1): ç ”ç©¶èƒŒæ™¯ï¼šéšç€æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸçš„æ˜¾è‘—è¿›å±•ï¼Œé£Ÿç‰©å›¾åƒåˆæˆæˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ‰©æ•£æ¨¡å‹åœ¨å¤„ç†å’Œèåˆå¤šå›¾åƒä¿¡æ¯æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œä¸”ç¼ºä¹é«˜è´¨é‡çš„å…¬å¼€æ•°æ®é›†ï¼Œé™åˆ¶äº†å…¶åœ¨é£Ÿå“å›¾åƒåˆæˆä¸­çš„åº”ç”¨ã€‚
 
   - (2): è¿‡å»æ–¹æ³•ï¼šè¿‡å»çš„æ–¹æ³•é€šå¸¸å°†å›¾åƒåˆæˆä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªå­ä»»åŠ¡ï¼Œå¦‚ç‰©ä½“æ”¾ç½®ã€å›¾åƒèåˆå’Œå’Œè°åŒ–ï¼Œä½†è¿™äº›æ–¹æ³•ä¾èµ–äºæ¯ä¸ªå­ä»»åŠ¡çš„æ€§èƒ½ï¼Œä¸”åœ¨å¤„ç†é£Ÿç‰©å›¾åƒæ—¶æ— æ³•ä¿ç•™çº¹ç†ã€é¢œè‰²ã€å›¾æ¡ˆå’Œçº¿æ¡ç­‰ç»†èŠ‚ç‰¹å¾ã€‚è¿™ç§æ–¹æ³•åŠ¨æœºä¸è¶³ï¼Œå› ä¸ºå®ƒä»¬æ— æ³•ç”Ÿæˆå…·æœ‰çœŸå®æ„Ÿå’Œè‡ªç„¶æ„Ÿçš„é«˜è´¨é‡åˆæˆå›¾åƒã€‚
 
   - (3): ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºFoodfusionçš„æ–°çš„é£Ÿç‰©å›¾åƒåˆæˆæ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œå¹¶å¼•å…¥äº†èåˆæ¨¡å—æ¥å¤„ç†å’Œæ•´åˆå‰æ™¯å’ŒèƒŒæ™¯ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè¿˜é›†æˆäº†å†…å®¹-ç»“æ„æ§åˆ¶æ¨¡å—ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºèƒŒæ™¯çš„å†…å®¹å’Œç»“æ„ã€‚
 
   - (4): ä»»åŠ¡ä¸æ€§èƒ½ï¼šFoodfusionåœ¨FC22kæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œè¯¥æ•°æ®é›†åŒ…å«22,000ä¸ªå‰æ™¯ã€èƒŒæ™¯å’ŒçœŸå®ä¸‰å…ƒå›¾åƒå¯¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFoodfusionæ–¹æ³•åœ¨åˆæˆé£Ÿç‰©å›¾åƒæ–¹é¢å…·æœ‰æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ã€è‡ªç„¶æ„Ÿå¼ºçš„åˆæˆå›¾åƒï¼Œæ”¯æŒå…¶ç›®æ ‡ã€‚
7. æ–¹æ³•ï¼š

    - (1): é£Ÿç‰©å›¾åƒåˆæˆä»»åŠ¡åˆ†è§£ï¼šå°†é£Ÿç‰©å›¾åƒåˆæˆä»»åŠ¡åˆ†è§£ä¸ºå‰æ™¯æ”¾ç½®ã€å›¾åƒèåˆå’Œå’Œè°åŒ–ç­‰å­ä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼ˆå¦‚Stable Diffusionï¼‰è¿›è¡Œå›¾åƒç”Ÿæˆã€‚
 
    - (2): èåˆæ¨¡å—è®¾è®¡ï¼šå¼•å…¥èåˆæ¨¡å—ï¼ˆFusion Module, FMï¼‰ï¼Œç”¨äºå¤„ç†å’Œæ•´åˆå‰æ™¯å’ŒèƒŒæ™¯ä¿¡æ¯ï¼Œæé«˜åˆæˆå›¾åƒçš„è´¨é‡ã€‚
 
    - (3): å†…å®¹-ç»“æ„æ§åˆ¶æ¨¡å—ï¼šé›†æˆå†…å®¹-ç»“æ„æ§åˆ¶æ¨¡å—ï¼ˆContent-Structure Control Module, CSCMï¼‰ï¼Œå¢å¼ºèƒŒæ™¯çš„å†…å®¹å’Œç»“æ„ï¼Œç¡®ä¿å‰æ™¯ä¸èƒŒæ™¯çš„åè°ƒä¸€è‡´ã€‚
 
    - (4): æ•°æ®é›†æ„å»ºï¼šæ„å»ºå¤§å‹é«˜è´¨é‡æ•°æ®é›†FC22kï¼ŒåŒ…å«22,000ä¸ªå‰æ™¯ã€èƒŒæ™¯å’ŒçœŸå®ä¸‰å…ƒå›¾åƒå¯¹ï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ã€‚
 
    - (5): å®éªŒä¸è¯„ä¼°ï¼šåœ¨FC22kæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼Œè¯„ä¼°æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ï¼Œå¹¶ä¸ç°æœ‰æ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼ŒéªŒè¯æ¨¡å‹åœ¨å›¾åƒè´¨é‡ã€ä¸€è‡´æ€§ç­‰æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚
 
    - (6): æ¶ˆèå®éªŒï¼šé€šè¿‡ç§»é™¤æˆ–æ›¿æ¢æ¨¡å‹ä¸­çš„å…³é”®æ¨¡å—ï¼ŒéªŒè¯å„ä¸ªæ¨¡å—å¯¹æœ€ç»ˆåˆæˆå›¾åƒçš„å½±å“ï¼Œè¿›ä¸€æ­¥è¯æ˜æ¨¡å‹è®¾è®¡çš„åˆç†æ€§ã€‚
 
    - (7): æ‰©å±•è®¨è®ºï¼šè®¨è®ºæ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸‹çš„é€‚ç”¨æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå±•ç¤ºæ–¹æ³•åœ¨ä¸åŒå›¾åƒåˆæˆä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚


8. Conclusion:

                    - (1): è¿™é¡¹å·¥ä½œçš„é‡è¦æ„ä¹‰åœ¨äºï¼Œå®ƒè§£å†³äº†é£Ÿå“å›¾åƒåˆæˆä¸­çš„æŒ‘æˆ˜ï¼Œä¸ºè¯¥é¢†åŸŸæä¾›äº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡æ„å»ºå¤§å‹é«˜è´¨é‡æ•°æ®é›†FC22Kï¼Œå¹¶æå‡ºäº†Foodfusionæ¨¡å‹ï¼Œè¯¥å·¥ä½œæ˜¾è‘—æé«˜äº†åˆæˆå›¾åƒçš„è´¨é‡å’Œè‡ªç„¶æ„Ÿï¼Œä¸ºé£Ÿå“å›¾åƒåˆæˆé¢†åŸŸæ ‘ç«‹äº†æ–°çš„åŸºå‡†ã€‚

                    - (2): Innovation point: Foodfusionæ¨¡å‹åœ¨åˆ›æ–°ç‚¹ä¸Šè¡¨ç°å‡ºè‰²ï¼Œé€šè¿‡èåˆæ¨¡å—å’Œå†…å®¹-ç»“æ„æ§åˆ¶æ¨¡å—çš„å¼•å…¥ï¼Œå®ç°äº†å‰æ™¯ä¸èƒŒæ™¯çš„æ— ç¼èåˆï¼Œå¹¶æé«˜äº†åˆæˆå›¾åƒçš„çœŸå®æ„Ÿã€‚Performance: åœ¨FC22kæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒFoodfusionåœ¨å›¾åƒè´¨é‡ã€ä¸€è‡´æ€§å’Œè‡ªç„¶åº¦æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚Workload: ç›¸æ¯”äºä¼ ç»Ÿæ–¹æ³•ï¼ŒFoodfusionæ¨¡å‹çš„è®¡ç®—æˆæœ¬è¾ƒé«˜ï¼Œéœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºå’Œæ—¶é—´æ¥å®Œæˆå›¾åƒåˆæˆä»»åŠ¡ã€‚




<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c39ba64398bae869dafe0b61b56f5d8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11eccfab4e17fe1a5292f7408bfd1842.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f36681aed7a680f07093af2938dfc13c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ed68430aab5e909fa2d392b50785fba5.jpg" align="middle">
</details>




## SurGen: Text-Guided Diffusion Model for Surgical Video Generation

**Authors:Joseph Cho, Samuel Schmidgall, Cyril Zakka, Mrudang Mathur, Rohan Shad, William Hiesinger**

Diffusion-based video generation models have made significant strides, producing outputs with improved visual fidelity, temporal coherence, and user control. These advancements hold great promise for improving surgical education by enabling more realistic, diverse, and interactive simulation environments. In this study, we introduce SurGen, a text-guided diffusion model tailored for surgical video synthesis, producing the highest resolution and longest duration videos among existing surgical video generation models. We validate the visual and temporal quality of the outputs using standard image and video generation metrics. Additionally, we assess their alignment to the corresponding text prompts through a deep learning classifier trained on surgical data. Our results demonstrate the potential of diffusion models to serve as valuable educational tools for surgical trainees. 

[PDF](http://arxiv.org/abs/2408.14028v1) 

**Summary**
ç ”ç©¶å¼•å…¥äº†SurGenï¼Œä¸€ç§æ–‡æœ¬å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºæ‰‹æœ¯è§†é¢‘åˆæˆï¼Œæ˜¾è‘—æå‡äº†æ‰‹æœ¯æ•™è‚²æ¨¡æ‹Ÿçš„çœŸå®æ€§å’Œäº’åŠ¨æ€§ã€‚

**Key Takeaways**
1. æ‰©æ•£æ¨¡å‹åœ¨è§†é¢‘ç”Ÿæˆé¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ã€‚
2. æ¨¡å‹è¾“å‡ºé«˜åˆ†è¾¨ç‡ã€é•¿æ—¶é•¿çš„æ‰‹æœ¯è§†é¢‘ã€‚
3. é€šè¿‡æ ‡å‡†å›¾åƒå’Œè§†é¢‘æŒ‡æ ‡éªŒè¯è¾“å‡ºè´¨é‡ã€‚
4. ä½¿ç”¨æ·±åº¦å­¦ä¹ åˆ†ç±»å™¨è¯„ä¼°æ–‡æœ¬æç¤ºä¸è§†é¢‘è¾“å‡ºçš„å¯¹é½åº¦ã€‚
5. æ¨¡å‹é€‚ç”¨äºæ‰‹æœ¯æ•™è‚²ï¼Œæå‡åŸ¹è®­æ•ˆæœã€‚
6. æä¾›æ›´çœŸå®ã€å¤šæ ·åŒ–çš„æ¨¡æ‹Ÿç¯å¢ƒã€‚
7. æ½œåœ¨çš„æ•™è‚²å·¥å…·ï¼ŒåŠ©åŠ›æ‰‹æœ¯åŸ¹è®­ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**



1. Title: SurGen: Text-Guided Diffusion Model for Surgical Video Generation
   æ ‡é¢˜ï¼šSurGenï¼šåŸºäºæ–‡æœ¬å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹ç”¨äºæ‰‹æœ¯è§†é¢‘ç”Ÿæˆ

2. Authors: Cho Joseph, Schmidgall Samuel, Zakka Cyril, Mathur Mrudang, Shad Rohan, Hiesinger William
   ä½œè€…ï¼šCho Joseph, Schmidgall Samuel, Zakka Cyril, Mathur Mrudang, Shad Rohan, Hiesinger William

3. Affiliation: Department of Cardiothoracic Surgery, Stanford Medicine
   æœºæ„ï¼šæ–¯å¦ç¦åŒ»å­¦é™¢èƒ¸å¿ƒå¤–ç§‘ç³»

4. Keywords: Diffusion model, Surgical video generation, Text guidance, Medical education
   å…³é”®è¯ï¼šæ‰©æ•£æ¨¡å‹ï¼Œæ‰‹æœ¯è§†é¢‘ç”Ÿæˆï¼Œæ–‡æœ¬å¼•å¯¼ï¼ŒåŒ»å­¦æ•™è‚²

5. Urls: https://arxiv.org/abs/2408.14028v1
         Github: None

6. Summary:

   - (1): ç ”ç©¶èƒŒæ™¯ï¼šè¯¥æ–‡ç« çš„ç ”ç©¶èƒŒæ™¯æ˜¯æ‰©æ•£æ¨¡å‹åœ¨è§†é¢‘ç”Ÿæˆé¢†åŸŸçš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦æ•™è‚²é¢†åŸŸçš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ‰‹æœ¯è§†é¢‘ç”Ÿæˆæ–¹é¢ã€‚
 
   - (2): è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šè¿‡å»çš„æ–¹æ³•ä¸»è¦æ˜¯åŸºäºä¼ ç»Ÿè§†é¢‘ç”ŸæˆæŠ€æœ¯ï¼Œä½†è¿™äº›æ–¹æ³•åœ¨ç”Ÿæˆé«˜è´¨é‡æ‰‹æœ¯è§†é¢‘æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¦‚è§†è§‰çœŸå®æ„Ÿã€æ—¶é—´è¿è´¯æ€§å’Œç”¨æˆ·æ§åˆ¶ç­‰æ–¹é¢ã€‚æ–‡ç« æå‡ºçš„æ–¹æ³•æ˜¯åŸºäºæ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œå¹¶å…·æœ‰å¾ˆå¥½çš„åŠ¨æœºã€‚
 
   - (3): ç ”ç©¶æ–¹æ³•ï¼šæ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºSurGençš„æ–‡æœ¬å¼•å¯¼æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºæ‰‹æœ¯è§†é¢‘åˆæˆï¼Œè¯¥æ¨¡å‹å¯ä»¥ç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€é•¿æ—¶é•¿çš„æ‰‹æœ¯è§†é¢‘ã€‚
 
   - (4): ä»»åŠ¡åŠæ€§èƒ½ï¼šè¯¥æ¨¡å‹åœ¨æ‰‹æœ¯è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æ€§èƒ½ï¼ŒåŒ…æ‹¬è§†è§‰å’Œæ—¶åºè´¨é‡ï¼Œå¹¶é€šè¿‡æ·±åº¦å­¦ä¹ åˆ†ç±»å™¨éªŒè¯äº†ä¸æ–‡æœ¬æç¤ºçš„ä¸€è‡´æ€§ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†å°†æ‰©æ•£æ¨¡å‹ä½œä¸ºåŒ»å­¦æ•™è‚²å·¥å…·çš„ç›®æ ‡ã€‚
7. Methods:

    - (1): æ•°æ®é›†æè¿°ï¼šæ–‡ç« ä½¿ç”¨äº†æ¥è‡ªCholec80 [27]çš„æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«äº†13ä½å¤–ç§‘åŒ»ç”Ÿè¿›è¡Œçš„80ä¾‹è…¹è…”é•œèƒ†å›Šåˆ‡é™¤æœ¯çš„è§†é¢‘ã€‚ä½œè€…éµå¾ªäº†åŸå§‹çš„è®­ç»ƒ-æµ‹è¯•åˆ’åˆ†ï¼Œä½¿ç”¨å‰40ä¸ªè§†é¢‘è¿›è¡Œè®­ç»ƒï¼Œå‰©ä½™çš„40ä¸ªè§†é¢‘ç”¨äºè¯„ä¼°ã€‚ä»æ¯ä¸ªæ‰‹æœ¯é˜¶æ®µä¸­æå–æ‰‹æœ¯é˜¶æ®µæ ‡ç­¾ï¼ˆå‡†å¤‡ã€Calotä¸‰è§’åˆ‡å¼€ã€èƒ†å›Šåˆ‡å¼€ã€å¤¹æŒå’Œåˆ‡å‰²ï¼‰ï¼Œåˆ›å»ºäº†200,000ä¸ªè§†é¢‘æ–‡æœ¬å¯¹ç”¨äºè®­ç»ƒã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºæ¯ä¸ªæ‰‹æœ¯é˜¶æ®µï¼Œæå–äº†ç”±49å¸§ç»„æˆçš„50,000ä¸ªç‹¬ç‰¹åºåˆ—ï¼Œæ¯ä¸ªåºåˆ—ä¸­çš„æ¯å¸§ä¸åŸå§‹è§†é¢‘çš„é—´éš”ä¸ºä¸¤ä¸ªå¸§ã€‚

    - (2): æ•°æ®é¢„å¤„ç†ï¼šåœ¨æ‰€æœ‰è§†é¢‘åºåˆ—ä¸­ï¼Œå°†æ¯ä¸ªå¸§ä»åŸå§‹å®½åº¦840åƒç´ è£å‰ªåˆ°720åƒç´ ï¼ŒåŒæ—¶ä¿æŒåŸå§‹é«˜åº¦480åƒç´ ã€‚è¿™æœ‰æ•ˆåœ°å»é™¤äº†å†…çª¥é•œè§†é¢‘ä¸­å¸¸è§çš„é»‘è‰²è¾¹ç¼˜ï¼Œç¡®ä¿ä¿ç•™äº†æ‰€æœ‰å…³é”®æ‰‹æœ¯ç»†èŠ‚ã€‚å°†å¯¹åº”çš„æ–‡æœ¬æç¤ºæ ¼å¼åŒ–ä¸ºâ€œåœ¨{æ‰‹æœ¯é˜¶æ®µ}æœŸé—´è¿›è¡Œè…¹è…”é•œèƒ†å›Šåˆ‡é™¤æœ¯â€ã€‚

    - (3): æ¨¡å‹æ¶æ„å’Œè®­ç»ƒï¼šå¯¹äºè§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œé‡‡ç”¨äº†ä¸€ä¸ªåä¸ºCogVideoXçš„2äº¿å‚æ•°æ–‡æœ¬å¼•å¯¼çš„LDMï¼ˆæ‰©æ•£æ¨¡å‹ï¼‰ã€‚CogVideoXç»“åˆäº†ä¸‰ä¸ªä¸»è¦ç»„ä»¶æ¥åˆæˆåŸºäºæ–‡æœ¬æç¤ºçš„è§†é¢‘ï¼š

        - 3Då˜åˆ†è‡ªç¼–ç å™¨ï¼šä¸ºäº†åŠ é€Ÿå»å™ªæ“ä½œï¼Œ3Då˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰çš„ç¼–ç å™¨å°†æ¯ä¸ªè§†é¢‘å‹ç¼©åˆ°æ½œåœ¨ç©ºé—´ï¼Œå°†å…¶ç©ºé—´ç»´åº¦å‡å°‘8å€ï¼Œæ—¶é—´ç»´åº¦å‡å°‘4å€ã€‚3D VAEçš„è§£ç å™¨å°†å»å™ªè¡¨ç¤ºè½¬æ¢æˆå®Œæ•´çš„è§†é¢‘å¸§ã€‚

        - å»å™ªè§†é¢‘Transformerï¼šä½¿ç”¨äº†ä¸€ä¸ª2äº¿å‚æ•°çš„æ–‡æœ¬æ¡ä»¶è§†é¢‘Transformeræ¥å»å™ªæ½œåœ¨å‘é‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¯¥æ¨¡å‹ä½¿ç”¨äº†ä¸€ä¸ªå®Œæ•´çš„3Dæ³¨æ„åŠ›æœºåˆ¶ï¼Œå…è®¸ç©ºé—´-æ—¶é—´è¡¥ä¸åœ¨æ‰€æœ‰ä½ç½®ç›¸äº’å…³æ³¨ã€‚

        - æ–‡æœ¬ç¼–ç å™¨ï¼šT5æ–‡æœ¬ç¼–ç å™¨ [31] å°†æ–‡æœ¬æç¤ºè½¬æ¢ä¸ºè¯­ä¹‰ä¸°å¯Œçš„è¡¨ç¤ºï¼Œç„¶åå°†è¿™äº›è¡¨ç¤ºè¾“å…¥åˆ°æ‰©æ•£Transformerä¸­ï¼Œä»¥æŒ‡å¯¼å»å™ªè¿‡ç¨‹ã€‚

    - (4): è§†é¢‘åˆæˆï¼šSurGenæ¨¡å‹ç”Ÿæˆ720 x 480åƒç´ ï¼ˆå®½åº¦Ã—é«˜åº¦ï¼‰çš„é«˜åˆ†è¾¨ç‡è§†é¢‘å¸§ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨æ‰‹æœ¯è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—æ˜¾è‘—æ€§èƒ½ï¼ŒåŒ…æ‹¬è§†è§‰å’Œæ—¶åºè´¨é‡ï¼Œå¹¶é€šè¿‡æ·±åº¦å­¦ä¹ åˆ†ç±»å™¨éªŒè¯äº†ä¸æ–‡æœ¬æç¤ºçš„ä¸€è‡´æ€§ã€‚


8. Conclusion:

    - (1): è¿™é¡¹å·¥ä½œçš„æ„ä¹‰åœ¨äºï¼ŒSurGenæ¨¡å‹é€šè¿‡ç»“åˆæ–‡æœ¬å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹æŠ€æœ¯ï¼Œä¸ºæ‰‹æœ¯è§†é¢‘çš„ç”Ÿæˆæä¾›äº†ä¸€ç§åˆ›æ–°çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨åŒ»å­¦æ•™è‚²é¢†åŸŸå…·æœ‰æ˜¾è‘—çš„åº”ç”¨æ½œåŠ›ï¼Œèƒ½å¤Ÿæé«˜æ‰‹æœ¯æ“ä½œçš„åŸ¹è®­æ•ˆæœï¼Œç‰¹åˆ«æ˜¯åœ¨éš¾ä»¥è·å–å®é™…æ‰‹æœ¯æ“ä½œç»éªŒçš„æƒ…å¢ƒä¸‹ã€‚

    - (2): Innovation point: è¯¥æ¨¡å‹åœ¨åˆ›æ–°ç‚¹ä¸Šï¼Œæå‡ºäº†æ–‡æœ¬å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹ï¼Œå®ç°äº†é«˜åˆ†è¾¨ç‡ã€é•¿æ—¶é•¿çš„æ‰‹æœ¯è§†é¢‘ç”Ÿæˆï¼Œä¸ºæ‰‹æœ¯è§†é¢‘ç”Ÿæˆæä¾›äº†æ–°çš„æ€è·¯ã€‚Performance: åœ¨æ€§èƒ½ä¸Šï¼ŒSurGenåœ¨è§†è§‰å’Œæ—¶åºè´¨é‡ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æˆç»©ï¼Œé€šè¿‡æ·±åº¦å­¦ä¹ åˆ†ç±»å™¨éªŒè¯äº†ä¸æ–‡æœ¬æç¤ºçš„ä¸€è‡´æ€§ã€‚Workload: åœ¨å·¥ä½œé‡ä¸Šï¼ŒSurGenæ¨¡å‹å¯¹æ•°æ®é›†çš„è¦æ±‚è¾ƒé«˜ï¼Œéœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œä¸”æ¨¡å‹è®­ç»ƒè¿‡ç¨‹å¤æ‚ï¼Œè®¡ç®—èµ„æºéœ€æ±‚å¤§ã€‚




<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a992013624ecc2a976a624323afe8fe2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8dffb9b8d7f14ef41f21b243c98be381.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-44b1ea01d4d36031b393bc5cdd106a62.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-828394b5a301aa0dccff17199480b2f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e6564dc345e7b9c81dee8db95e37954c.jpg" align="middle">
</details>




## Pixel-Aligned Multi-View Generation with Depth Guided Decoder

**Authors:Zhenggang Tang, Peiye Zhuang, Chaoyang Wang, Aliaksandr Siarohin, Yash Kant, Alexander Schwing, Sergey Tulyakov, Hsin-Ying Lee**

The task of image-to-multi-view generation refers to generating novel views of an instance from a single image. Recent methods achieve this by extending text-to-image latent diffusion models to multi-view version, which contains an VAE image encoder and a U-Net diffusion model. Specifically, these generation methods usually fix VAE and finetune the U-Net only. However, the significant downscaling of the latent vectors computed from the input images and independent decoding leads to notable pixel-level misalignment across multiple views. To address this, we propose a novel method for pixel-level image-to-multi-view generation. Unlike prior work, we incorporate attention layers across multi-view images in the VAE decoder of a latent video diffusion model. Specifically, we introduce a depth-truncated epipolar attention, enabling the model to focus on spatially adjacent regions while remaining memory efficient. Applying depth-truncated attn is challenging during inference as the ground-truth depth is usually difficult to obtain and pre-trained depth estimation models is hard to provide accurate depth. Thus, to enhance the generalization to inaccurate depth when ground truth depth is missing, we perturb depth inputs during training. During inference, we employ a rapid multi-view to 3D reconstruction approach, NeuS, to obtain coarse depth for the depth-truncated epipolar attention. Our model enables better pixel alignment across multi-view images. Moreover, we demonstrate the efficacy of our approach in improving downstream multi-view to 3D reconstruction tasks. 

[PDF](http://arxiv.org/abs/2408.14016v1) 

**Summary**
æå‡ºä¸€ç§æ–°çš„åƒç´ çº§å›¾åƒåˆ°å¤šè§†å›¾ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡åœ¨æ½œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹çš„VAEè§£ç å™¨ä¸­å¼•å…¥æ·±åº¦æˆªæ–­è§†å·®æ³¨æ„åŠ›å±‚ï¼Œå®ç°æ›´å¥½çš„åƒç´ å¯¹é½ã€‚

**Key Takeaways**
- ä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹è¿›è¡Œå¤šè§†å›¾ç”Ÿæˆã€‚
- ä»¥å‰çš„æ–¹æ³•ä»…å¾®è°ƒU-Netï¼Œä½†å¯¼è‡´åƒç´ çº§é”™ä½ã€‚
- æå‡ºç»“åˆå¤šè§†å›¾å›¾åƒçš„VAEè§£ç å™¨ä¸­çš„æ³¨æ„åŠ›å±‚ã€‚
- å¼•å…¥æ·±åº¦æˆªæ–­è§†å·®æ³¨æ„åŠ›ï¼Œæé«˜ç©ºé—´é‚»è¿‘åŒºåŸŸå¯¹é½ã€‚
- é’ˆå¯¹éš¾ä»¥è·å–çœŸå®æ·±åº¦çš„é—®é¢˜ï¼Œé€šè¿‡æ‰°åŠ¨è¾“å…¥æ·±åº¦æ¥æé«˜æ³›åŒ–èƒ½åŠ›ã€‚
- ä½¿ç”¨NeuSå¿«é€Ÿå¤šè§†å›¾åˆ°3Dé‡å»ºæ–¹æ³•è·å¾—ç²—ç•¥æ·±åº¦ã€‚
- æ¨¡å‹æ”¹å–„åƒç´ å¯¹é½ï¼Œæå‡ä¸‹æ¸¸3Dé‡å»ºä»»åŠ¡ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**



1. Title: åŸºäºæ·±åº¦å¼•å¯¼è§£ç å™¨çš„åƒç´ å¯¹é½å¤šè§†å›¾ç”Ÿæˆ
                 2. Authors: Zhenggang Tang, Peiye Zhuang, Chaoyang Wang, Aliaksandr Siarohin, Yash Kant, Alexander Schwing, Sergey Tulyakov, Hsin-Ying Lee
                 3. Affiliation: University of Illinois Urbana-Champaign, Snap Inc., University of Toronto
                 4. Keywords: Image-to-multi-view generation, VAE decoder, Depth-guided, Epipolar attention, Multi-view to 3D reconstruction
                 5. Urls: https://arxiv.org/abs/2408.14016v1 or Github: None
                 6. Summary:
                    - (1):è¯¥æ–‡ç« çš„ç ”ç©¶èƒŒæ™¯æ˜¯å¤šè§†å›¾ç”Ÿæˆä»»åŠ¡ï¼Œå³ä»å•å¼ å›¾åƒç”Ÿæˆæ–°çš„è§†å›¾ã€‚
                    - (2):è¿‡å»çš„æ–¹æ³•é€šå¸¸é€šè¿‡æ‰©å±•æ–‡æœ¬åˆ°å›¾åƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹åˆ°å¤šè§†å›¾ç‰ˆæœ¬æ¥å®ç°ï¼Œè¿™åŒ…æ‹¬VAEå›¾åƒç¼–ç å™¨å’ŒU-Netæ‰©æ•£æ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨åƒç´ çº§åˆ«ä¸Šå­˜åœ¨å¯¹é½é—®é¢˜ï¼Œå› ä¸ºæ½œåœ¨å‘é‡çš„æ˜¾è‘—ä¸‹é‡‡æ ·å’Œç‹¬ç«‹è§£ç å¯¼è‡´äº†å¤šä¸ªè§†å›¾ä¹‹é—´çš„åƒç´ çº§å¯¹é½é”™è¯¯ã€‚è¯¥æ–‡ç« æå‡ºçš„æ–¹æ³•è§£å†³äº†è¿™ä¸€åŠ¨æœºã€‚
                    - (3)ï¼šè¯¥æ–‡ç« æå‡ºäº†ä¸€ç§æ”¹è¿›çš„VAEè§£ç å™¨ï¼Œå…¶ä¸­åŒ…å«è·¨å¤šä¸ªè§†å›¾å›¾åƒçš„æ³¨æ„åŠ›å±‚ã€‚å…·ä½“æ¥è¯´ï¼Œå¼•å…¥äº†ä¸€ç§æ·±åº¦æˆªæ–­å…±çº¿æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå…³æ³¨ç©ºé—´ç›¸é‚»åŒºåŸŸï¼ŒåŒæ—¶ä¿æŒå†…å­˜æ•ˆç‡ã€‚ä¸ºäº†è§£å†³æ¨ç†è¿‡ç¨‹ä¸­æ·±åº¦ä¿¡æ¯ç¼ºå¤±çš„é—®é¢˜ï¼Œæ–‡ç« æå‡ºåœ¨è®­ç»ƒæœŸé—´å¯¹æ·±åº¦è¾“å…¥è¿›è¡Œæ‰°åŠ¨ï¼Œå¹¶åœ¨æ¨ç†æœŸé—´ä½¿ç”¨NeuSæ–¹æ³•è·å¾—ç²—ç•¥æ·±åº¦ä¿¡æ¯ã€‚
                    - (4)ï¼šè¯¥æ–¹æ³•åœ¨å¤šè§†å›¾åˆ°3Dé‡å»ºä»»åŠ¡ä¸­æé«˜äº†ä¸‹æ¸¸æ€§èƒ½ï¼Œè¡¨æ˜äº†å…¶åœ¨åƒç´ å¯¹é½å’Œå¤šè§†å›¾ç”Ÿæˆä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚
7. Methods:

    - (1): æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å¼•å¯¼è§£ç å™¨çš„å¤šè§†å›¾ç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é’ˆå¯¹åƒç´ å¯¹é½é—®é¢˜è¿›è¡Œäº†æ”¹è¿›ã€‚
    
    - (2): è®¾è®¡äº†ä¸€ç§æ”¹è¿›çš„å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰è§£ç å™¨ï¼Œå…¶ä¸­åŒ…å«è·¨å¤šä¸ªè§†å›¾å›¾åƒçš„æ³¨æ„åŠ›å±‚ï¼Œå³æ·±åº¦æˆªæ–­å…±çº¿æ³¨æ„åŠ›æœºåˆ¶ã€‚
    
    - (3): ä¸ºäº†è§£å†³æ·±åº¦ä¿¡æ¯ç¼ºå¤±é—®é¢˜ï¼Œåœ¨è®­ç»ƒæœŸé—´å¯¹æ·±åº¦è¾“å…¥è¿›è¡Œæ‰°åŠ¨ï¼Œå¹¶åœ¨æ¨ç†æœŸé—´ä½¿ç”¨NeuSæ–¹æ³•è·å¾—ç²—ç•¥æ·±åº¦ä¿¡æ¯ã€‚
    
    - (4): åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹èƒ½å¤Ÿå…³æ³¨ç©ºé—´ç›¸é‚»åŒºåŸŸï¼ŒåŒæ—¶ä¿æŒå†…å­˜æ•ˆç‡ï¼Œä»è€Œæé«˜åƒç´ å¯¹é½çš„å‡†ç¡®æ€§ã€‚
    
    - (5): é€šè¿‡åœ¨å¤šè§†å›¾åˆ°3Dé‡å»ºä»»åŠ¡ä¸­çš„åº”ç”¨ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨åƒç´ å¯¹é½å’Œå¤šè§†å›¾ç”Ÿæˆä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚


8. Conclusion:

                    - (1):è¯¥ç ”ç©¶å·¥ä½œçš„é‡è¦æ€§åœ¨äºæå‡ºäº†ä¸€ç§é’ˆå¯¹åƒç´ å¯¹é½é—®é¢˜çš„å¤šè§†å›¾ç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡æ”¹è¿›VAEè§£ç å™¨å’Œå¼•å…¥æ·±åº¦æˆªæ–­å…±çº¿æ³¨æ„åŠ›æœºåˆ¶ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•ä¸­å­˜åœ¨çš„åƒç´ å¯¹é½è¯¯å·®é—®é¢˜ï¼Œä¸º3Dç”Ÿæˆä»»åŠ¡æä¾›äº†æ›´ç²¾ç¡®çš„å¤šè§†å›¾å›¾åƒä½œä¸ºè¾…åŠ©è¡¨ç¤ºã€‚

                    - (2):Innovation point: åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†åŸºäºæ·±åº¦å¼•å¯¼è§£ç å™¨çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥æ·±åº¦æˆªæ–­å…±çº¿æ³¨æ„åŠ›æœºåˆ¶å’Œæ‰°åŠ¨æ·±åº¦è¾“å…¥æŠ€æœ¯ï¼Œå®ç°äº†åƒç´ çº§åˆ«çš„å¤šè§†å›¾å›¾åƒå¯¹é½ï¼›Performance: æ€§èƒ½æ–¹é¢ï¼Œè¯¥æ–¹æ³•åœ¨å¤šè§†å›¾åˆ°3Dé‡å»ºä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒéªŒè¯äº†å…¶åœ¨åƒç´ å¯¹é½å’Œå¤šè§†å›¾ç”Ÿæˆä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼›Workload: å·¥ä½œè´Ÿè½½æ–¹é¢ï¼Œè™½ç„¶è¯¥æ–¹æ³•åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­å¼•å…¥äº†ä¸€äº›é¢å¤–çš„è®¡ç®—å¤æ‚åº¦ï¼Œä½†å…¶é«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶å’Œæ·±åº¦ä¿¡æ¯å¤„ç†æŠ€æœ¯ä½¿å¾—æ•´ä½“è®¡ç®—è´Ÿæ‹…ç›¸å¯¹è¾ƒä½ã€‚




<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-40cd83ea1e6cdf60dcdb8f5b3149199d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-225bd963670b613c0286bb0632287704.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb3d107c8e8db27a5ed3a66ea97d5f62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ecac9bcb71fd5512917975829d4ba4e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9bb59b02262303b2b87923c13d04a98.jpg" align="middle">
</details>




## Particle-Filtering-based Latent Diffusion for Inverse Problems

**Authors:Amir Nazemi, Mohammad Hadi Sepanj, Nicholas Pellegrino, Chris Czarnecki, Paul Fieguth**

Current strategies for solving image-based inverse problems apply latent diffusion models to perform posterior sampling.However, almost all approaches make no explicit attempt to explore the solution space, instead drawing only a single sample from a Gaussian distribution from which to generate their solution. In this paper, we introduce a particle-filtering-based framework for a nonlinear exploration of the solution space in the initial stages of reverse SDE methods. Our proposed particle-filtering-based latent diffusion (PFLD) method and proposed problem formulation and framework can be applied to any diffusion-based solution for linear or nonlinear inverse problems. Our experimental results show that PFLD outperforms the SoTA solver PSLD on the FFHQ-1K and ImageNet-1K datasets on inverse problem tasks of super resolution, Gaussian debluring and inpainting. 

[PDF](http://arxiv.org/abs/2408.13868v1) Mohammad Hadi Sepanj, Nicholas Pellegrino, and Chris Czarnecki   contributed equally

**Summary**
æå‡ºåŸºäºç²’å­æ»¤æ³¢çš„éçº¿æ€§æ¢ç´¢è§£ç©ºé—´æ¡†æ¶ï¼Œåœ¨å›¾åƒé€†é—®é¢˜æ±‚è§£ä¸­è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚

**Key Takeaways**
1. å½“å‰å›¾åƒé€†é—®é¢˜æ±‚è§£é‡‡ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹è¿›è¡ŒåéªŒé‡‡æ ·ã€‚
2. å¤§å¤šæ•°æ–¹æ³•æœªæ˜¾å¼æ¢ç´¢è§£ç©ºé—´ï¼Œä»…ä»é«˜æ–¯åˆ†å¸ƒä¸­æŠ½å–å•ä¸€æ ·æœ¬ã€‚
3. æœ¬æ–‡æå‡ºç²’å­æ»¤æ³¢æ¡†æ¶ï¼Œæ¢ç´¢éçº¿æ€§è§£ç©ºé—´ã€‚
4. PFLDæ–¹æ³•é€‚ç”¨äºçº¿æ€§æˆ–éçº¿æ€§é€†é—®é¢˜æ±‚è§£ã€‚
5. PFLDåœ¨FFHQ-1Kå’ŒImageNet-1Kæ•°æ®é›†ä¸Šè¶…è¶Šç°æœ‰æ–¹æ³•PSLDã€‚
6. å®éªŒç»“æœè¯æ˜PFLDåœ¨è¶…åˆ†è¾¨ç‡ã€é«˜æ–¯å»æ¨¡ç³Šå’Œä¿®å¤ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚
7. PFLDæ¡†æ¶å¯æ‰©å±•åº”ç”¨äºæ›´å¤šå›¾åƒé€†é—®é¢˜ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**



1. Title: åŸºäºç²’å­æ»¤æ³¢çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨é€†é—®é¢˜ä¸­çš„åº”ç”¨ (Particle-Filtering-based Latent Diffusion for Inverse Problems)

2. Authors: Amir Nazemi, Mohammad Hadi Sepanj, Nicholas Pellegrino, Chris Czarnecki, Paul Fieguth

3. Affiliation: åŠ æ‹¿å¤§æ»‘é“å¢å¤§å­¦ç³»ç»Ÿå·¥ç¨‹ç³»è§†è§‰ä¸å›¾åƒå¤„ç†å®éªŒå®¤

4. Keywords: ç²’å­æ»¤æ³¢ï¼Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œé€†é—®é¢˜ï¼Œå›¾åƒè¶…åˆ†è¾¨ç‡ï¼Œé«˜æ–¯å»æ¨¡ç³Šï¼Œå›¾åƒä¿®å¤

5. Urls: [Paper](https://arxiv.org/abs/2408.13868v1) , [Github:None]

6. Summary:

    - (1):æœ¬æ–‡çš„ç ”ç©¶èƒŒæ™¯æ˜¯å›¾åƒé€†é—®é¢˜æ±‚è§£ï¼Œå½“å‰æ–¹æ³•ä¸»è¦åˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹è¿›è¡ŒåéªŒé‡‡æ ·ï¼Œä½†æ™®éç¼ºä¹å¯¹è§£ç©ºé—´çš„æ¢ç´¢ã€‚
 
    - (2):è¿‡å»çš„æ–¹æ³•åŒ…æ‹¬åŸºäºç”Ÿæˆæ¨¡å‹çš„æ–¹æ³•ï¼Œå¦‚æ‰©æ•£æ¨¡å‹ï¼ˆDPS, PSLD, Soft Diffusionç­‰ï¼‰ã€‚è¿™äº›æ–¹æ³•è™½ç„¶æ€§èƒ½å‡ºè‰²ï¼Œä½†å®é™…åº”ç”¨ä¸­é²æ£’æ€§ä¸è¶³ï¼Œå¯¹åˆå§‹åŒ–æ•æ„Ÿï¼Œä¸”æœªæ˜ç¡®æ¢ç´¢è§£ç©ºé—´ã€‚
 
    - (3):æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç²’å­æ»¤æ³¢çš„æ½œåœ¨æ‰©æ•£ï¼ˆPFLDï¼‰æ–¹æ³•ï¼Œç”¨äºåœ¨é€†é—®é¢˜çš„æ±‚è§£è¿‡ç¨‹ä¸­å¯¹è§£ç©ºé—´è¿›è¡Œéçº¿æ€§æ¢ç´¢ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ç²’å­æ»¤æ³¢å’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡å¤šä¸ªéšæœºæ ·æœ¬åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œè¿­ä»£ï¼Œä»¥æ›´å¥½åœ°æ¢ç´¢è§£ç©ºé—´ã€‚
 
    - (4):PFLDåœ¨FFHQ-1Kå’ŒImageNet-1Kæ•°æ®é›†ä¸Šï¼Œåœ¨è¶…åˆ†è¾¨ç‡ã€é«˜æ–¯å»æ¨¡ç³Šå’Œå›¾åƒä¿®å¤ç­‰é€†é—®é¢˜ä»»åŠ¡ä¸Šï¼Œæ€§èƒ½ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•PSLDã€‚è¿™è¡¨æ˜PFLDèƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜é€†é—®é¢˜æ±‚è§£çš„é²æ£’æ€§å’Œè´¨é‡ï¼Œæ”¯æŒå…¶ç ”ç©¶ç›®æ ‡ã€‚
7. Methods:

    - (1): é‡‡ç”¨ç²’å­æ»¤æ³¢ï¼ˆParticle Filteringï¼ŒPFï¼‰æŠ€æœ¯ï¼Œä»¥è§£å†³å›¾åƒé€†é—®é¢˜ä¸­çš„ä¸ç¡®å®šæ€§é—®é¢˜ã€‚
    - (2): å°†æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Modelï¼ŒLDMï¼‰ä¸ç²’å­æ»¤æ³¢ç»“åˆï¼Œå½¢æˆæ½œåœ¨æ‰©æ•£ç²’å­æ»¤æ³¢ï¼ˆParticle-Filtering-based Latent Diffusionï¼ŒPFLDï¼‰æ–¹æ³•ã€‚
    - (3): ä½¿ç”¨Cauchyæ¦‚ç‡åˆ†å¸ƒå‡½æ•°æ¥å»ºæ¨¡ä¼¼ç„¶æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆPDFï¼‰ï¼Œå…¶ä¸­Îºå‚æ•°è®¾ç½®ä¸º1ï¼Œä½¿å¾—PDFä¸æµ‹é‡å€¼yä¸æ¨¡å‹è¾“å‡ºä¹‹é—´çš„L2è·ç¦»æˆæ¯”ä¾‹ã€‚
    - (4): é€šè¿‡æ›´æ–°æƒé‡æ¥ä¼˜åŒ–ç²’å­æ»¤æ³¢è¿‡ç¨‹ï¼Œæƒé‡æ›´æ–°å…¬å¼åŸºäºæœ€ä¼˜é‡è¦é‡‡æ ·ï¼Œå…¶ä¸­q(zt-1|zt, y)ä¸p(zt-1|zt)æˆæ­£æ¯”ã€‚
    - (5): ä½¿ç”¨æ›´æ–°åçš„æƒé‡æ¥è¯„ä¼°ç²’å­çš„é‡è¦æ€§ï¼Œå¹¶æ ¹æ®é‡è¦æ€§å¯¹ç²’å­è¿›è¡Œé‡‡æ ·ï¼Œä»è€Œåœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œè¿­ä»£ï¼Œæ¢ç´¢è§£ç©ºé—´ã€‚
    - (6): åœ¨FFHQ-1Kå’ŒImageNet-1Kæ•°æ®é›†ä¸Šï¼Œé€šè¿‡è¶…åˆ†è¾¨ç‡ã€é«˜æ–¯å»æ¨¡ç³Šå’Œå›¾åƒä¿®å¤ç­‰ä»»åŠ¡æ¥è¯„ä¼°PFLDçš„æ€§èƒ½ï¼Œå¹¶ä¸PSLDç­‰æ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚


8. Conclusion:

                    - (1): è¿™é¡¹å·¥ä½œçš„æ„ä¹‰åœ¨äºæå‡ºäº†åŸºäºç²’å­æ»¤æ³¢çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆPFLDï¼‰åœ¨å›¾åƒé€†é—®é¢˜ä¸­çš„åº”ç”¨ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæé«˜é€†é—®é¢˜æ±‚è§£çš„é²æ£’æ€§å’Œæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è¶…åˆ†è¾¨ç‡ã€é«˜æ–¯å»æ¨¡ç³Šå’Œå›¾åƒä¿®å¤ç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚

                    - (2): Innovation point: PFLDç»“åˆäº†ç²’å­æ»¤æ³¢å’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œå®ç°äº†å¯¹è§£ç©ºé—´çš„æœ‰æ•ˆæ¢ç´¢ï¼›Performance: åœ¨FFHQ-1Kå’ŒImageNet-1Kæ•°æ®é›†ä¸Šï¼ŒPFLDåœ¨å¤šä¸ªé€†é—®é¢˜ä»»åŠ¡ä¸Šä¼˜äºPSLDç­‰ç°æœ‰æ–¹æ³•ï¼›Workload: ä¸PSLDç›¸æ¯”ï¼ŒPFLDåœ¨ä¿æŒé«˜æ•ˆçš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†é‡å¤è¿è¡Œæ‰€éœ€çš„æ—¶é—´ã€‚




<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eaa7a79e1b3c6dbb62e5ae559cd06308.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df193f744203b2b7ced2e58b387cab30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef681ce7a7cb6ea53e3cbfe9010a9d84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9170c21e28e9d5ff7fc0f0affd31c7d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b68ace86a0651fd3ed8ab002825be6a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-19c8c9f0eebe16586ac077a6f5f2bcbe.jpg" align="middle">
</details>




## Draw Like an Artist: Complex Scene Generation with Diffusion Model via   Composition, Painting, and Retouching

**Authors:Minghao Liu, Le Zhang, Yingjie Tian, Xiaochao Qu, Luoqi Liu, Ting Liu**

Recent advances in text-to-image diffusion models have demonstrated impressive capabilities in image quality. However, complex scene generation remains relatively unexplored, and even the definition of `complex scene' itself remains unclear. In this paper, we address this gap by providing a precise definition of complex scenes and introducing a set of Complex Decomposition Criteria (CDC) based on this definition. Inspired by the artists painting process, we propose a training-free diffusion framework called Complex Diffusion (CxD), which divides the process into three stages: composition, painting, and retouching. Our method leverages the powerful chain-of-thought capabilities of large language models (LLMs) to decompose complex prompts based on CDC and to manage composition and layout. We then develop an attention modulation method that guides simple prompts to specific regions to complete the complex scene painting. Finally, we inject the detailed output of the LLM into a retouching model to enhance the image details, thus implementing the retouching stage. Extensive experiments demonstrate that our method outperforms previous SOTA approaches, significantly improving the generation of high-quality, semantically consistent, and visually diverse images for complex scenes, even with intricate prompts. 

[PDF](http://arxiv.org/abs/2408.13858v1) 

**Summary**
è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºComplex Diffusionçš„æ–‡æœ¬åˆ°å¤æ‚åœºæ™¯å›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œæ˜¾è‘—æå‡äº†å›¾åƒè´¨é‡ã€‚

**Key Takeaways**
1. æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ç”Ÿæˆæ–¹é¢ä»æœ‰å¾…æ¢ç´¢ã€‚
2. å®šä¹‰äº†å¤æ‚åœºæ™¯å¹¶æä¾›äº†ä¸€å¥—å¤æ‚åˆ†è§£æ ‡å‡†ï¼ˆCDCï¼‰ã€‚
3. æå‡ºäº†æ— è®­ç»ƒçš„Complex Diffusionæ¡†æ¶ï¼ŒåŒ…å«æ„å›¾ã€ç»˜ç”»å’Œä¿®å›¾ä¸‰ä¸ªé˜¶æ®µã€‚
4. åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é“¾å¼æ€ç»´èƒ½åŠ›è¿›è¡Œå¤æ‚æç¤ºåˆ†è§£ã€‚
5. å¼€å‘æ³¨æ„åŠ›è°ƒèŠ‚æ–¹æ³•ï¼Œå¼•å¯¼ç®€å•æç¤ºåˆ°ç‰¹å®šåŒºåŸŸå®Œæˆå¤æ‚åœºæ™¯ç»˜ç”»ã€‚
6. å°†LLMçš„è¯¦ç»†è¾“å‡ºæ³¨å…¥ä¿®å›¾æ¨¡å‹ï¼Œå¢å¼ºå›¾åƒç»†èŠ‚ã€‚
7. å®éªŒè¯æ˜è¯¥æ–¹æ³•ä¼˜äºç°æœ‰SOTAæ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†å¤æ‚åœºæ™¯å›¾åƒç”Ÿæˆè´¨é‡ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**



1. Title: Draw Like an Artist: Complex Scene Generation with Diffusion Model via Composition, Painting, and Retouching
                 (ä¸­æ–‡ç¿»è¯‘ï¼šé€šè¿‡æ„å›¾ã€ç»˜ç”»å’Œä¿®å›¾ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œå¤æ‚åœºæ™¯ç”Ÿæˆ)

2. Authors: Minghao Liu, Le Zhang, Yingjie Tian, Xiaochao Qu, Luoqi Liu, Ting Liu

3. Affiliation: University of Chinese Academy of Sciences, Beijing, China; MT Lab, Meitu Inc., Beijing, China

4. Keywords: Complex scene generation, Diffusion model, Large language model, Composition, Painting, Retouching

5. Urls: arXiv:2408.13858v1 [cs.CV], Github: None

6. Summary:

   - (1):ç ”ç©¶èƒŒæ™¯ä¸ºè¿‘å¹´æ¥æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒè´¨é‡ä¸Šçš„æ˜¾è‘—è¿›å±•ï¼Œä½†å¤æ‚åœºæ™¯ç”Ÿæˆä»ç„¶ç›¸å¯¹æœªæ¢ç´¢ï¼Œå¯¹â€œå¤æ‚åœºæ™¯â€çš„å®šä¹‰ä¹Ÿå°šä¸æ˜ç¡®ã€‚

   - (2)ï¼šè¿‡å»çš„æ–¹æ³•åŒ…æ‹¬å°†å¸ƒå±€æˆ–æ¡†æ•´åˆåˆ°åˆæˆè¿‡ç¨‹ä¸­ï¼Œä»¥æé«˜å¤æ‚åœºæ™¯ä¸­ç‰©ä½“å…³ç³»çš„è¿è´¯æ€§ï¼Œä»¥åŠä½¿ç”¨æ³¨æ„åŠ›å¼•å¯¼æ¥æ”¹è¿›æ„å›¾æ–‡æœ¬åˆ°å›¾åƒåˆæˆã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å¤„ç†é«˜åº¦å¤æ‚çš„åœºæ™¯æç¤ºæ—¶å­˜åœ¨å·®è·ï¼Œä¸”â€œå¤æ‚åœºæ™¯â€çš„å®šä¹‰ä»ç„¶æ¨¡ç³Šã€‚æœ¬æ–‡çš„æ–¹æ³•åŸºäºè‰ºæœ¯å®¶ç»˜ç”»è¿‡ç¨‹çš„çµæ„Ÿï¼Œå…·æœ‰è¾ƒå¥½çš„åŠ¨æœºã€‚

   - (3)ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºComplex Diffusion (CxD) çš„æ— ç›‘ç£æ‰©æ•£æ¡†æ¶ï¼Œå°†å¤æ‚åœºæ™¯ç”Ÿæˆè¿‡ç¨‹åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šæ„å›¾ã€ç»˜ç”»å’Œä¿®å›¾ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¼ºå¤§æ€ç»´é“¾èƒ½åŠ›ï¼Œæ ¹æ®å¤æ‚åˆ†è§£æ ‡å‡†ï¼ˆCDCï¼‰åˆ†è§£å¤æ‚æç¤ºï¼Œå¹¶ç®¡ç†æ„å›¾å’Œå¸ƒå±€ã€‚

   - (4)ï¼šåœ¨å¤æ‚åœºæ™¯ç”Ÿæˆä»»åŠ¡ä¸Šï¼Œæœ¬æ–‡çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†é«˜è´¨é‡ã€è¯­ä¹‰ä¸€è‡´ä¸”è§†è§‰å¤šæ ·åŒ–çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ï¼Œå³ä½¿åœ¨å¤æ‚çš„æç¤ºä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œè¿™æ”¯æŒäº†ä»–ä»¬çš„ç›®æ ‡ã€‚


8. Conclusion:

                    - (1):è¿™é¡¹å·¥ä½œçš„æ„ä¹‰åœ¨äºï¼Œå®ƒä¸ºå¤æ‚åœºæ™¯çš„ç”Ÿæˆæä¾›äº†ä¸€ç§åˆ›æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡æ¨¡ä»¿è‰ºæœ¯å®¶ç»˜ç”»è¿‡ç¨‹ï¼Œæœ‰æ•ˆæå‡äº†å›¾åƒç”Ÿæˆçš„è´¨é‡ã€è¯­ä¹‰ä¸€è‡´æ€§å’Œè§†è§‰å¤šæ ·æ€§ã€‚

                    - (2):Innovation point: è¯¥æ–‡åˆ›æ–°æ€§åœ°æå‡ºäº†Complex Diffusion (CxD) æ¡†æ¶ï¼Œå°†å¤æ‚åœºæ™¯ç”Ÿæˆè¿‡ç¨‹ç»†åˆ†ä¸ºæ„å›¾ã€ç»˜ç”»å’Œä¿®å›¾ä¸‰ä¸ªé˜¶æ®µï¼Œå¹¶ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„èƒ½åŠ›ï¼Œä¸ºå¤æ‚åœºæ™¯æç¤ºçš„å¤„ç†æä¾›äº†æ–°çš„æ€è·¯ï¼›Performance: åœ¨å¤æ‚åœºæ™¯ç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒCxDæ¡†æ¶æ˜¾è‘—æé«˜äº†å›¾åƒç”Ÿæˆçš„è´¨é‡å’Œå¤šæ ·æ€§ï¼›Workload: ç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼ŒCxDæ¡†æ¶åœ¨å¤„ç†å¤æ‚åœºæ™¯æç¤ºæ—¶å¯èƒ½éœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºå’Œæ—¶é—´ã€‚




<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7a6a02bc5fb28de0729f9d725a223a61.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bbb39fc0a25ab2224da3b80df1815685.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32684ab401b77160002fded4b9ed8586.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-527e45037886bcee67918837f356e07c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3580d210def4a7494987d28c744d9821.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-375013bb838eef79a7b83db03e72f072.jpg" align="middle">
</details>




## Bring the Power of Diffusion Model to Defect Detection

**Authors:Xuyi Yu**

Due to the high complexity and technical requirements of industrial production processes, surface defects will inevitably appear, which seriously affects the quality of products. Although existing lightweight detection networks are highly efficient, they are susceptible to false or missed detection of non-salient defects due to the lack of semantic information. In contrast, the diffusion model can generate higher-order semantic representations in the denoising process. Therefore, the aim of this paper is to incorporate the higher-order modelling capability of the diffusion model into the detection model, so as to better assist in the classification and localization of difficult targets. First, the denoising diffusion probabilistic model (DDPM) is pre-trained to extract the features of denoising process to construct as a feature repository. In particular, to avoid the potential bottleneck of memory caused by the dataloader loading high-dimensional features, a residual convolutional variational auto-encoder (ResVAE) is designed to further compress the feature repository. The image is fed into both image backbone and feature repository for feature extraction and querying respectively. The queried latent features are reconstructed and filtered to obtain high-dimensional DDPM features. A dynamic cross-fusion method is proposed to fully refine the contextual features of DDPM to optimize the detection model. Finally, we employ knowledge distillation to migrate the higher-order modelling capabilities back into the lightweight baseline model without additional efficiency cost. Experiment results demonstrate that our method achieves competitive results on several industrial datasets. 

[PDF](http://arxiv.org/abs/2408.13845v1) 

**Summary**
åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„é«˜çº§å»ºæ¨¡èƒ½åŠ›æå‡è½»é‡çº§æ£€æµ‹ç½‘ç»œå¯¹å·¥ä¸šç¼ºé™·çš„æ£€æµ‹ç²¾åº¦ã€‚

**Key Takeaways**
- å·¥ä¸šç”Ÿäº§ä¸­è¡¨é¢ç¼ºé™·å½±å“äº§å“è´¨é‡ã€‚
- è½»é‡çº§æ£€æµ‹ç½‘ç»œæ•ˆç‡é«˜ï¼Œä½†æ˜“è¯¯æ£€æˆ–æ¼æ£€ã€‚
- æ‰©æ•£æ¨¡å‹èƒ½ç”Ÿæˆé«˜çº§è¯­ä¹‰è¡¨ç¤ºã€‚
- ç ”ç©¶æ—¨åœ¨å°†æ‰©æ•£æ¨¡å‹èå…¥æ£€æµ‹æ¨¡å‹ã€‚
- ä½¿ç”¨DDPMé¢„è®­ç»ƒæå–å»å™ªè¿‡ç¨‹ç‰¹å¾ã€‚
- è®¾è®¡ResVAEå‹ç¼©ç‰¹å¾åº“ä»¥é¿å…å†…å­˜ç“¶é¢ˆã€‚
- å›¾åƒåˆ†åˆ«é€šè¿‡å›¾åƒä¸»å¹²å’Œç‰¹å¾åº“è¿›è¡Œç‰¹å¾æå–ã€‚
- æå‡ºåŠ¨æ€äº¤å‰èåˆæ–¹æ³•ä¼˜åŒ–æ£€æµ‹æ¨¡å‹ã€‚
- åº”ç”¨çŸ¥è¯†è’¸é¦æå‡è½»é‡çº§æ¨¡å‹èƒ½åŠ›ã€‚
- æ–¹æ³•åœ¨å¤šä¸ªå·¥ä¸šæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**



1. Title: D3N: Bring the Power of Diffusion Model to Defect Detection
                 (ä¸­æ–‡ç¿»è¯‘ï¼šD3Nï¼šå°†æ‰©æ•£æ¨¡å‹çš„å¼ºå¤§åŠŸèƒ½åº”ç”¨äºç¼ºé™·æ£€æµ‹)

2. Authors: Xuyi Yu

3. Affiliation: Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, Xi'an, China

4. Keywords: defect detection, semantic information, DDPM, feature repository, knowledge distillation

5. Urls: IEEE TRANSACTIONS, Github: None

6. Summary:

                    - (1):è¯¥æ–‡çš„ç ”ç©¶èƒŒæ™¯æ˜¯å·¥ä¸šç”Ÿäº§è¿‡ç¨‹ä¸­ï¼Œç”±äºå·¥è‰ºå¤æ‚å’ŒæŠ€æœ¯è¦æ±‚é«˜ï¼Œè¡¨é¢ç¼ºé™·ä¸å¯é¿å…åœ°ä¼šå‡ºç°ï¼Œè¿™ä¸¥é‡å½±å“äº†äº§å“çš„è´¨é‡ã€‚å°½ç®¡ç°æœ‰çš„è½»é‡çº§æ£€æµ‹ç½‘ç»œæ•ˆç‡å¾ˆé«˜ï¼Œä½†ç”±äºç¼ºä¹è¯­ä¹‰ä¿¡æ¯ï¼Œå®ƒä»¬å®¹æ˜“å¯¹éæ˜¾è‘—ç¼ºé™·äº§ç”Ÿè¯¯æ£€æˆ–æ¼æ£€ã€‚

                    - (2):è¿‡å»çš„æ–¹æ³•åŒ…æ‹¬ä»å¤§è§„æ¨¡æ¨¡å‹ä¸­è’¸é¦å°æ¨¡å‹ï¼Œä½†è¿™äº›æ–¹æ³•éœ€è¦å¤§å‹çš„æ•™å¸ˆæ¨¡å‹ï¼Œä¸”ç”±äºæ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹ç»´åº¦ä¹‹é—´çš„å·®å¼‚ï¼Œå­˜åœ¨è¯­ä¹‰å·®è·ï¼Œéš¾ä»¥è®©å­¦ç”Ÿæ¨¡å‹å­¦ä¹ åˆ°æ•™å¸ˆæ¨¡å‹çš„æ‰€æœ‰çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæ‰©æ•£æ¨¡å‹åœ¨å»å™ªè¿‡ç¨‹ä¸­çš„ä¸­é—´æ¿€æ´»å…·æœ‰é«˜é˜¶è¯­ä¹‰è¡¨ç¤ºï¼Œå¯ä»¥æä¾›æœ‰ä»·å€¼çš„ä¿¡æ¯ï¼Œä½†ç°æœ‰æ–¹æ³•éœ€è¦æ‰§è¡Œå®Œæ•´çš„æ‰©æ•£è¿‡ç¨‹ï¼Œé€Ÿåº¦ä¸ç†æƒ³ã€‚

                    - (3):æœ¬æ–‡æå‡ºçš„æ–¹æ³•åŒ…æ‹¬é¢„è®­ç»ƒå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰ä»¥æå–å»å™ªè¿‡ç¨‹ä¸­çš„ç‰¹å¾ï¼Œæ„å»ºç‰¹å¾åº“ï¼›è®¾è®¡æ®‹å·®å·ç§¯å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆResVAEï¼‰è¿›ä¸€æ­¥å‹ç¼©ç‰¹å¾åº“ï¼›å°†å›¾åƒè¾“å…¥å›¾åƒéª¨å¹²å’Œç‰¹å¾åº“è¿›è¡Œç‰¹å¾æå–å’ŒæŸ¥è¯¢ï¼›æå‡ºåŠ¨æ€äº¤å‰èåˆæ–¹æ³•ä»¥ä¼˜åŒ–æ£€æµ‹æ¨¡å‹ï¼›æœ€åï¼Œä½¿ç”¨çŸ¥è¯†è’¸é¦å°†é«˜é˜¶å»ºæ¨¡èƒ½åŠ›è¿ç§»å›è½»é‡çº§åŸºçº¿æ¨¡å‹ã€‚

                    - (4):åœ¨å¤šä¸ªå·¥ä¸šæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœï¼Œæ”¯æŒäº†å…¶ç›®æ ‡ã€‚
7. Methods:

    - (1): é¢„è®­ç»ƒå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰ä»¥æå–å»å™ªè¿‡ç¨‹ä¸­çš„ç‰¹å¾ï¼Œæ„å»ºç‰¹å¾åº“ï¼›

    - (2): è®¾è®¡æ®‹å·®å·ç§¯å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆResVAEï¼‰è¿›ä¸€æ­¥å‹ç¼©ç‰¹å¾åº“ï¼›

    - (3): å°†å›¾åƒè¾“å…¥å›¾åƒéª¨å¹²å’Œç‰¹å¾åº“è¿›è¡Œç‰¹å¾æå–å’ŒæŸ¥è¯¢ï¼›

    - (4): æå‡ºåŠ¨æ€äº¤å‰èåˆæ–¹æ³•ä»¥ä¼˜åŒ–æ£€æµ‹æ¨¡å‹ï¼›

    - (5): ä½¿ç”¨çŸ¥è¯†è’¸é¦å°†é«˜é˜¶å»ºæ¨¡èƒ½åŠ›è¿ç§»å›è½»é‡çº§åŸºçº¿æ¨¡å‹ï¼›

    - (6): åœ¨å¤šä¸ªå·¥ä¸šæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒéªŒè¯ï¼Œè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚


8. Conclusion:

                    - (1):è¯¥ç ”ç©¶å…·æœ‰é‡è¦çš„æ„ä¹‰ï¼Œå› ä¸ºå®ƒä¸ºç¼ºé™·æ£€æµ‹é¢†åŸŸæä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå³åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„é«˜é˜¶å»ºæ¨¡èƒ½åŠ›æ¥è¯†åˆ«éš¾ä»¥æ£€æµ‹çš„ç›®æ ‡ï¼Œä»è€Œæé«˜äº†æ£€æµ‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

                    - (2):Innovation point: æœ¬æ–‡åˆ›æ–°æ€§åœ°å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºç¼ºé™·æ£€æµ‹ï¼Œå¹¶æå‡ºäº†åŸºäºé¢„è®­ç»ƒçš„DDPMå’Œç‰¹å¾åº“çš„æ„å»ºæ–¹æ³•ï¼Œä¸ºè½»é‡çº§æ£€æµ‹ç½‘ç»œæä¾›äº†è¯­ä¹‰ä¿¡æ¯ï¼Œæé«˜äº†æ£€æµ‹ç²¾åº¦ï¼›Performance: å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªå·¥ä¸šæ•°æ®é›†ä¸Šå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœï¼Œè¯æ˜äº†å…¶åœ¨æ€§èƒ½ä¸Šçš„ä¼˜åŠ¿ï¼›Workload: è™½ç„¶è¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæœ‰æ‰€æå‡ï¼Œä½†é¢„è®­ç»ƒDDPMå’Œç‰¹å¾åº“çš„æ„å»ºè¿‡ç¨‹éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œå¢åŠ äº†æ¨¡å‹çš„å·¥ä½œé‡ã€‚




<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7c2e148864cb9c9fbed2b432745f8485.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8a3814a1a1a302995ff4c9e2851cde77.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f1882d0867dc7dcf2afa9d90033a9bef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf6eb098a4068a659144372be44b34df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50c817f1ee7c74bfd6b0dd25318ec602.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-70f7ec9d31fee5d0a5e2042e085bcc17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a946916f1eb330dcb310ae0f6ee297cb.jpg" align="middle">
</details>




## 3D-VirtFusion: Synthetic 3D Data Augmentation through Generative   Diffusion Models and Controllable Editing

**Authors:Shichao Dong, Ze Yang, Guosheng Lin**

Data augmentation plays a crucial role in deep learning, enhancing the generalization and robustness of learning-based models. Standard approaches involve simple transformations like rotations and flips for generating extra data. However, these augmentations are limited by their initial dataset, lacking high-level diversity. Recently, large models such as language models and diffusion models have shown exceptional capabilities in perception and content generation. In this work, we propose a new paradigm to automatically generate 3D labeled training data by harnessing the power of pretrained large foundation models. For each target semantic class, we first generate 2D images of a single object in various structure and appearance via diffusion models and chatGPT generated text prompts. Beyond texture augmentation, we propose a method to automatically alter the shape of objects within 2D images. Subsequently, we transform these augmented images into 3D objects and construct virtual scenes by random composition. This method can automatically produce a substantial amount of 3D scene data without the need of real data, providing significant benefits in addressing few-shot learning challenges and mitigating long-tailed class imbalances. By providing a flexible augmentation approach, our work contributes to enhancing 3D data diversity and advancing model capabilities in scene understanding tasks. 

[PDF](http://arxiv.org/abs/2408.13788v1) 

**Summary**
åˆ©ç”¨é¢„è®­ç»ƒå¤§æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆ3Dè®­ç»ƒæ•°æ®ï¼Œæå‡3Dæ•°æ®å¤šæ ·æ€§å’Œæ¨¡å‹åœºæ™¯ç†è§£èƒ½åŠ›ã€‚

**Key Takeaways**
1. æ•°æ®å¢å¼ºåœ¨æ·±åº¦å­¦ä¹ ä¸­è‡³å…³é‡è¦ï¼Œå¢å¼ºæ¨¡å‹æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚
2. æ ‡å‡†å¢å¼ºæ–¹æ³•æœ‰é™ï¼Œç¼ºä¹é«˜çº§å¤šæ ·æ€§ã€‚
3. å¤§å‹æ¨¡å‹åœ¨æ„ŸçŸ¥å’Œå†…å®¹ç”Ÿæˆä¸­è¡¨ç°å‡ºè‰²ã€‚
4. æå‡ºä¸€ç§åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹è‡ªåŠ¨ç”Ÿæˆ3Dè®­ç»ƒæ•°æ®çš„æ–°æ–¹æ³•ã€‚
5. ä½¿ç”¨æ‰©æ•£æ¨¡å‹å’Œæ–‡æœ¬æç¤ºç”Ÿæˆ2Då›¾åƒã€‚
6. è‡ªåŠ¨æ”¹å˜2Då›¾åƒä¸­ç‰©ä½“çš„å½¢çŠ¶ã€‚
7. å°†å¢å¼ºå›¾åƒè½¬æ¢ä¸º3Då¯¹è±¡ï¼Œæ„å»ºè™šæ‹Ÿåœºæ™¯ã€‚
8. æ— éœ€çœŸå®æ•°æ®ï¼Œè‡ªåŠ¨ç”Ÿæˆå¤§é‡3Dåœºæ™¯æ•°æ®ã€‚
9. å¸®åŠ©è§£å†³å°æ ·æœ¬å­¦ä¹ å’Œé•¿å°¾ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚
10. æé«˜æ¨¡å‹åœ¨åœºæ™¯ç†è§£ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**



1. Title: 3D-VirtFusion: Synthetic 3D Data Augmentation through Generative Diffusion Models and Controllable Editing (åˆ©ç”¨ç”Ÿæˆæ‰©æ•£æ¨¡å‹å’Œå¯æ§ç¼–è¾‘è¿›è¡Œåˆæˆ3Dæ•°æ®å¢å¼º)

2. Authors: Shichao Dong, Ze Yang, Guosheng Lin

3. Affiliation: S-lab, Nanyang Technological University, Singapore; College of Computing and Data Science, Nanyang Technological University, Singapore

4. Keywords: 3Dæ•°æ®å¢å¼ºï¼Œç”Ÿæˆæ‰©æ•£æ¨¡å‹ï¼Œå¯æ§ç¼–è¾‘ï¼Œæ•°æ®å¤šæ ·æ€§ï¼Œåœºæ™¯ç†è§£

5. Urls: arXiv:2408.13788v1 [cs.CV], Github: None

6. Summary:

   - (1):è¯¥æ–‡ç« çš„ç ”ç©¶èƒŒæ™¯æ˜¯3Dæ•°æ®å¢å¼ºåœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨è§£å†³æ•°æ®ä¸å¹³è¡¡å’Œæ ·æœ¬ç¨€ç–é—®é¢˜ä¸Šçš„æŒ‘æˆ˜ã€‚

   - (2):è¿‡å»çš„æ–¹æ³•åŒ…æ‹¬ç®€å•çš„æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œå¦‚æ—‹è½¬å’Œç¿»è½¬ï¼Œä½†è¿™äº›æ–¹æ³•å—é™äºåˆå§‹æ•°æ®é›†çš„å¤šæ ·æ€§ã€‚æ–‡ç« æå‡ºçš„æ–¹æ¡ˆæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œå¹¶åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹åŸºç¡€æ¨¡å‹æ¥ç”Ÿæˆé«˜è´¨é‡çš„å¢å¼ºæ•°æ®ã€‚

   - (3)ï¼šæ–‡ç« æå‡ºçš„æ–¹æ³•åŒ…æ‹¬ä½¿ç”¨æ‰©æ•£æ¨¡å‹å’ŒèŠå¤©æœºå™¨äººç”Ÿæˆçš„æ–‡æœ¬æç¤ºæ¥ç”Ÿæˆå„ç§ç»“æ„å’Œå¤–è§‚çš„å•ä¸ªç‰©ä½“çš„2Då›¾åƒï¼Œç„¶åé€šè¿‡è‡ªåŠ¨æ”¹å˜ç‰©ä½“çš„å½¢çŠ¶æ¥å¢å¼ºçº¹ç†ï¼Œå¹¶å°†è¿™äº›å›¾åƒè½¬æ¢ä¸º3Då¯¹è±¡ï¼Œéšæœºç»„åˆæˆè™šæ‹Ÿåœºæ™¯ã€‚

   - (4)ï¼šè¯¥æ–¹æ³•åœ¨ScanNet-v2æ•°æ®é›†ä¸Šçš„3Dè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¡¨æ˜å…¶èƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜æ¨¡å‹å¯¹æœªè§æ•°æ®çš„æ³›åŒ–èƒ½åŠ›å’Œå‡å°‘è¿‡æ‹Ÿåˆçš„é£é™©ï¼Œæ”¯æŒå…¶æé«˜æ¨¡å‹æ€§èƒ½çš„ç›®æ ‡ã€‚
7. Methods:

    - (1): é¦–å…ˆï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹å’ŒChatGPTç”Ÿæˆçš„æ–‡æœ¬æç¤ºï¼Œç”Ÿæˆå•ä¸ªç‰©ä½“çš„å¤šæ ·åŒ–2Då›¾åƒã€‚
 
    - (2)ï¼šåœ¨2Då›¾åƒä¸­è‡ªåŠ¨è°ƒæ•´ç‰©ä½“çš„å½¢çŠ¶ï¼Œä»¥å¢å¼ºçº¹ç†ã€‚
 
    - (3)ï¼šå°†å¢å¼ºåçš„2Då›¾åƒè½¬æ¢ä¸º3Då¯¹è±¡ï¼Œå¹¶å°†å…¶éšæœºç»„åˆæˆè™šæ‹Ÿåœºæ™¯ã€‚
 
    - (4)ï¼šåˆ©ç”¨ChatGPTç”Ÿæˆè¯­ä¹‰å’Œå®ä¾‹æ ‡ç­¾ï¼Œä¸ºä¸‹æ¸¸ä»»åŠ¡è®­ç»ƒæä¾›ä¾¿åˆ©ã€‚
 
    - (5)ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„æ·±åº¦å­¦ä¹ æ¨¡å‹Depth Anythingè¿›è¡Œå¤šè§†å›¾å›¾åƒå’Œæ³•çº¿å›¾ç”Ÿæˆã€‚
 
    - (6)ï¼šé€šè¿‡ä½“ç§¯æ¸²æŸ“å®ç°3Då¯¹è±¡é‡å»ºã€‚
 
    - (7)ï¼šé‡‡ç”¨DragGANå’ŒDragDiffusionç­‰ç”Ÿæˆæ¨¡å‹è¿›è¡Œå½¢çŠ¶äº¤äº’å¼æ§åˆ¶ã€‚
 
    - (8)ï¼šå¼•å…¥ä½ç§©è‡ªé€‚åº”ï¼ˆLoRAï¼‰æ¨¡å‹è¿›è¡Œå¿«é€Ÿå¾®è°ƒã€‚
 
    - (9)ï¼šéšæœºé€‰æ‹©å¯¹è±¡ä¸Šçš„ç‚¹ä½œä¸ºç§å­ç‚¹ï¼Œä»¥è¿›è¡Œå½¢çŠ¶è°ƒæ•´ã€‚
 
    - (10)ï¼šé€‰æ‹©éšæœºæ–¹å‘å¹¶ç¡®å®šå˜å½¢æ–¹å‘ï¼Œç„¶åé€‰æ‹©ç›®æ ‡ç‚¹ã€‚
 
    - (11)ï¼šæ ¹æ®é«˜æ–¯æ­£æ€åˆ†å¸ƒå’Œä¸¤ä¸ªå…³é”®å‚æ•°ï¼ˆå‡å€¼Âµå’Œæ–¹å·®ÏƒÂ²ï¼‰æ§åˆ¶å½¢çŠ¶å¢å¼ºçš„ç¨‹åº¦ã€‚
 
    - (12)ï¼šå°†ç”Ÿæˆçš„2Då›¾åƒè½¬æ¢ä¸º3Då‡ ä½•ä¿¡æ¯ã€‚


8. Conclusion:

- (1)ï¼šè¯¥ç ”ç©¶å·¥ä½œçš„æ„ä¹‰åœ¨äºæå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆæ‰©æ•£æ¨¡å‹å’Œå¯æ§ç¼–è¾‘çš„åˆæˆ3Dæ•°æ®å¢å¼ºæ–¹æ³•ï¼Œæœ‰æ•ˆè§£å†³äº†3Dåœºæ™¯ç†è§£ä»»åŠ¡ä¸­è®­ç»ƒæ•°æ®ä¸è¶³çš„é—®é¢˜ï¼Œä¸º3Dæ•°æ®å¢å¼ºå’Œè™šæ‹Ÿæ•°æ®ç”Ÿæˆæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚

- (2): Innovation point: è¯¥æ–¹æ³•åœ¨åˆ›æ–°ç‚¹ä¸Šï¼Œé¦–å…ˆåˆ©ç”¨æ‰©æ•£æ¨¡å‹å’ŒChatGPTç”Ÿæˆå¤šæ ·åŒ–2Då›¾åƒï¼Œç»“åˆè‡ªåŠ¨å½¢çŠ¶è°ƒæ•´å’Œ3Då¯¹è±¡ç”ŸæˆæŠ€æœ¯ï¼Œå®ç°äº†å¯¹3Dåœºæ™¯çš„åˆæˆå¢å¼ºï¼›Performance: åœ¨æ€§èƒ½ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨ScanNet-v2æ•°æ®é›†ä¸Šçš„3Dè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ï¼›Workload: åœ¨å·¥ä½œè´Ÿè½½ä¸Šï¼Œè¯¥æ–¹æ³•è™½ç„¶éœ€è¦ä¸€å®šçš„è®¡ç®—èµ„æºï¼Œä½†é€šè¿‡å¼•å…¥LoRAæ¨¡å‹å’Œå½¢çŠ¶äº¤äº’å¼æ§åˆ¶æŠ€æœ¯ï¼Œå®ç°äº†å¯¹æ¨¡å‹å¾®è°ƒå’Œå½¢çŠ¶è°ƒæ•´çš„å¿«é€Ÿå®ç°ã€‚




<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-882573f6d88e59708d590e94aae96998.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8646ad56816ab72a80861f7cf3fe337b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84bbf2905d14522d4233df7b28a71641.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09bc8363ca04b62ae13808e8e29767bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-693608b9cd9746ec7d4c76183b084664.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-181693f10da664849f1a399213e78999.jpg" align="middle">
</details>




## Guided and Fused: Efficient Frozen CLIP-ViT with Feature Guidance and   Multi-Stage Feature Fusion for Generalizable Deepfake Detection

**Authors:Yingjian Chen, Lei Zhang, Yakun Niu, Pei Chen, Lei Tan, Jing Zhou**

The rise of generative models has sparked concerns about image authenticity online, highlighting the urgent need for an effective and general detector. Recent methods leveraging the frozen pre-trained CLIP-ViT model have made great progress in deepfake detection. However, these models often rely on visual-general features directly extracted by the frozen network, which contain excessive information irrelevant to the task, resulting in limited detection performance. To address this limitation, in this paper, we propose an efficient Guided and Fused Frozen CLIP-ViT (GFF), which integrates two simple yet effective modules. The Deepfake-Specific Feature Guidance Module (DFGM) guides the frozen pre-trained model in extracting features specifically for deepfake detection, reducing irrelevant information while preserving its generalization capabilities. The Multi-Stage Fusion Module (FuseFormer) captures low-level and high-level information by fusing features extracted from each stage of the ViT. This dual-module approach significantly improves deepfake detection by fully leveraging CLIP-ViT's inherent advantages. Extensive experiments demonstrate the effectiveness and generalization ability of GFF, which achieves state-of-the-art performance with optimal results in only 5 training epochs. Even when trained on only 4 classes of ProGAN, GFF achieves nearly 99% accuracy on unseen GANs and maintains an impressive 97% accuracy on unseen diffusion models. 

[PDF](http://arxiv.org/abs/2408.13697v1) 

**Summary**
æå‡ºGFFæ¨¡å‹ï¼Œé€šè¿‡ç‰¹å¾å¼•å¯¼å’Œèåˆæ¨¡å—æå‡æ·±åº¦ä¼ªé€ æ£€æµ‹æ€§èƒ½ã€‚

**Key Takeaways**
1. ç”Ÿæˆæ¨¡å‹å…´èµ·å¼•å‘å›¾åƒçœŸå®æ€§æ‹…å¿§ã€‚
2. CLIP-ViTæ¨¡å‹åœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸­å–å¾—è¿›å±•ã€‚
3. ç°æœ‰æ–¹æ³•ä¾èµ–ä¸ä»»åŠ¡æ— å…³çš„è§†è§‰ç‰¹å¾ã€‚
4. GFFæ¨¡å‹é›†æˆç‰¹å¾å¼•å¯¼å’Œèåˆæ¨¡å—ã€‚
5. DFGMæ¨¡å—æŒ‡å¯¼æ¨¡å‹æå–ç‰¹å®šäºæ·±åº¦ä¼ªé€ çš„ç‰¹å¾ã€‚
6. FuseFormeræ¨¡å—èåˆViTå„é˜¶æ®µç‰¹å¾ã€‚
7. GFFåœ¨å°‘é‡è®­ç»ƒæ•°æ®ä¸‹å®ç°æœ€ä¼˜æ€§èƒ½ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**



1. Title: æŒ‡å¯¼ä¸èåˆï¼šé«˜æ•ˆå†»ç»“CLIP-ViTåŠå…¶åœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸­çš„åº”ç”¨ (Guided and Fused: Efficient Frozen CLIP-ViT with Feature Guidance and Multi-Stage Feature Fusion for Generalizable Deepfake Detection)

2. Authors: Yingjian Chen, Lei Zhang, Yakun Niu, Pei Chen, Lei Tan, Jing Zhou

3. Affiliation: æ²³å—å¤§å­¦å¤§æ•°æ®åˆ†æä¸å¤„ç†æ²³å—çœé‡ç‚¹å®éªŒå®¤ (Henan Key Laboratory of Big Data Analysis and Processing, Henan University)

4. Keywords: æ·±åº¦ä¼ªé€ æ£€æµ‹, å†»ç»“é¢„è®­ç»ƒæ¨¡å‹, CLIP-ViT, ç‰¹å¾å¼•å¯¼, ç‰¹å¾èåˆ

5. Urls: https://arxiv.org/abs/2408.13697v1 , Github: None

6. Summary:

    - (1):éšç€ç”Ÿæˆæ¨¡å‹çš„å‘å±•ï¼Œç½‘ç»œå›¾åƒçš„çœŸå®æ€§å—åˆ°è´¨ç–‘ï¼Œè¿«åˆ‡éœ€è¦æœ‰æ•ˆçš„é€šç”¨æ£€æµ‹å™¨ã€‚è¯¥æ–‡çš„ç ”ç©¶èƒŒæ™¯æ˜¯åº”å¯¹æ·±åº¦ä¼ªé€ æ£€æµ‹çš„æŒ‘æˆ˜ã€‚

    - (2)ï¼šè¿‡å»çš„æ–¹æ³•ï¼Œå¦‚åŸºäºå†»ç»“é¢„è®­ç»ƒçš„CLIP-ViTæ¨¡å‹ï¼Œåœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸­å–å¾—äº†è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹é€šå¸¸ä¾èµ–äºå†»ç»“ç½‘ç»œç›´æ¥æå–çš„è§†è§‰é€šç”¨ç‰¹å¾ï¼Œå…¶ä¸­åŒ…å«ä¸ä»»åŠ¡æ— å…³çš„å¤§é‡ä¿¡æ¯ï¼Œå¯¼è‡´æ£€æµ‹æ€§èƒ½æœ‰é™ã€‚æœ¬æ–‡çš„æ–¹æ³•å¾ˆå¥½åœ°è§£å†³äº†è¿™ä¸€åŠ¨æœºé—®é¢˜ã€‚

    - (3)ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æŒ‡å¯¼ä¸èåˆå†»ç»“CLIP-ViTï¼ˆGFFï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é›†æˆäº†ä¸¤ä¸ªç®€å•è€Œæœ‰æ•ˆçš„æ¨¡å—ã€‚æ·±åº¦ä¼ªé€ ç‰¹å®šç‰¹å¾å¼•å¯¼æ¨¡å—ï¼ˆDFGMï¼‰å¼•å¯¼å†»ç»“é¢„è®­ç»ƒæ¨¡å‹æå–ç‰¹å®šäºæ·±åº¦ä¼ªé€ æ£€æµ‹çš„ç‰¹å¾ï¼Œå‡å°‘æ— å…³ä¿¡æ¯åŒæ—¶ä¿ç•™å…¶æ³›åŒ–èƒ½åŠ›ã€‚å¤šé˜¶æ®µèåˆæ¨¡å—ï¼ˆFuseFormerï¼‰é€šè¿‡èåˆViTæ¯ä¸ªé˜¶æ®µæå–çš„ç‰¹å¾æ¥æ•æ‰ä½çº§å’Œé«˜çº§ä¿¡æ¯ã€‚

    - (4)ï¼šè¯¥æ–¹æ³•åœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä»…ç”¨5ä¸ªè®­ç»ƒå‘¨æœŸå°±è¾¾åˆ°äº†æœ€ä½³ç»“æœã€‚å³ä½¿åœ¨åªæœ‰4ä¸ªProGANç±»åˆ«çš„æ•°æ®ä¸Šè®­ç»ƒï¼ŒGFFåœ¨æœªè§è¿‡çš„GANsä¸Šè¾¾åˆ°äº†è¿‘99%çš„å‡†ç¡®ç‡ï¼Œåœ¨æœªè§è¿‡çš„æ‰©æ•£æ¨¡å‹ä¸Šä¿æŒäº†97%çš„å‡†ç¡®ç‡ï¼Œè¯æ˜äº†å…¶æ€§èƒ½æ”¯æŒå…¶ç›®æ ‡ã€‚
7. Methods:

    - (1): é’ˆå¯¹æ·±åº¦ä¼ªé€ æ£€æµ‹çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºGFFï¼ˆGuided and Fusedï¼‰çš„å†»ç»“CLIP-ViTæ¨¡å‹ã€‚

    - (2): è®¾è®¡äº†æ·±åº¦ä¼ªé€ ç‰¹å®šç‰¹å¾å¼•å¯¼æ¨¡å—ï¼ˆDFGMï¼‰ï¼Œè¯¥æ¨¡å—å¼•å¯¼é¢„è®­ç»ƒæ¨¡å‹æå–ä¸æ·±åº¦ä¼ªé€ æ£€æµ‹ç›¸å…³çš„ç‰¹å¾ã€‚

    - (3): å®ç°äº†å¤šé˜¶æ®µèåˆæ¨¡å—ï¼ˆFuseFormerï¼‰ï¼Œé€šè¿‡èåˆä¸åŒé˜¶æ®µçš„ç‰¹å¾æ¥æ•æ‰å›¾åƒçš„ç»†ç²’åº¦ä¿¡æ¯ã€‚

    - (4): å°†DFGMå’ŒFuseFormeré›†æˆåˆ°å†»ç»“CLIP-ViTä¸­ï¼Œä»¥æå‡æ¨¡å‹çš„æ£€æµ‹æ€§èƒ½ã€‚

    - (5): åœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹æ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼ŒéªŒè¯GFFçš„æœ‰æ•ˆæ€§ã€‚

    - (6): é€šè¿‡ä¸å…¶ä»–å…ˆè¿›æ–¹æ³•æ¯”è¾ƒï¼Œè¯æ˜GFFåœ¨æ£€æµ‹æ€§èƒ½ä¸Šå…·æœ‰ä¼˜è¶Šæ€§ã€‚


8. Conclusion:

    - (1):è¯¥ç ”ç©¶å·¥ä½œçš„é‡è¦æ€§åœ¨äºæå‡ºäº†ä¸€ä¸ªåä¸ºGFFï¼ˆGuided and Fused Frozen CLIP-ViTï¼‰çš„æ–°å‹æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨æ³›åŒ–å›¾åƒæ£€æµ‹ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä¸ºæ·±åº¦ä¼ªé€ æ£€æµ‹é¢†åŸŸæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚

    - (2):Innovation point: GFFæ¨¡å‹é€šè¿‡å¼•å…¥æ·±åº¦ä¼ªé€ ç‰¹å®šç‰¹å¾å¼•å¯¼æ¨¡å—ï¼ˆDFGMï¼‰å’Œå¤šé˜¶æ®µèåˆæ¨¡å—ï¼ˆFuseFormerï¼‰å®ç°äº†å¯¹é¢„è®­ç»ƒæ¨¡å‹çš„æœ‰æ•ˆåˆ©ç”¨å’Œæ€§èƒ½æå‡ï¼›Performance: åœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨GANså’Œæ‰©æ•£æ¨¡å‹æ•°æ®é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼›Workload: æ¨¡å‹ç»“æ„ç®€å•ï¼Œå‚æ•°é‡å°ï¼Œè®­ç»ƒå‘¨æœŸçŸ­ï¼Œå…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚




<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-028d538b4529c4b567b16860634cf58a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6bebe049795c78333c1bcaba6b245b1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-901a1a8575ab34e6d44587fd9b194fc8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8209fe96d5fe5bbcb6bfe7d81171dcab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6215784210bf5478f6d1d5e3c8f254a5.jpg" align="middle">
</details>




## Prompt-Softbox-Prompt: A free-text Embedding Control for Image Editing

**Authors:Yitong Yang, Yinglin Wang, Jing Wang, Tian Zhang**

Text-driven diffusion models have achieved remarkable success in image editing, but a crucial component in these models-text embeddings-has not been fully explored. The entanglement and opacity of text embeddings present significant challenges to achieving precise image editing. In this paper, we provide a comprehensive and in-depth analysis of text embeddings in Stable Diffusion XL, offering three key insights. First, while the 'aug_embedding' captures the full semantic content of the text, its contribution to the final image generation is relatively minor. Second, 'BOS' and 'Padding_embedding' do not contain any semantic information. Lastly, the 'EOS' holds the semantic information of all words and contains the most style features. Each word embedding plays a unique role without interfering with one another. Based on these insights, we propose a novel approach for controllable image editing using a free-text embedding control method called PSP (Prompt-Softbox-Prompt). PSP enables precise image editing by inserting or adding text embeddings within the cross-attention layers and using Softbox to define and control the specific area for semantic injection. This technique allows for obejct additions and replacements while preserving other areas of the image. Additionally, PSP can achieve style transfer by simply replacing text embeddings. Extensive experimental results show that PSP achieves significant results in tasks such as object replacement, object addition, and style transfer. 

[PDF](http://arxiv.org/abs/2408.13623v2) 

**Summary**
åˆ†æStable Diffusion XLæ–‡æœ¬åµŒå…¥ï¼Œæå‡ºPSPå¯æ§å›¾åƒç¼–è¾‘æ–¹æ³•ã€‚

**Key Takeaways**
1. æ–‡æœ¬åµŒå…¥çš„å¤æ‚æ€§å½±å“å›¾åƒç¼–è¾‘ç²¾åº¦ã€‚
2. 'aug_embedding'å¯¹å›¾åƒç”Ÿæˆå½±å“è¾ƒå°ã€‚
3. 'BOS'å’Œ'Padding_embedding'æ— è¯­ä¹‰ä¿¡æ¯ã€‚
4. 'EOS'åŒ…å«æœ€å¤šé£æ ¼ç‰¹å¾ã€‚
5. å•è¯åµŒå…¥å„å¸å…¶èŒï¼Œæ— å¹²æ‰°ã€‚
6. PSPé€šè¿‡æ–‡æœ¬åµŒå…¥æ§åˆ¶å›¾åƒç¼–è¾‘ã€‚
7. PSPåœ¨ç‰©ä½“æ›¿æ¢ã€æ·»åŠ å’Œé£æ ¼è½¬æ¢ä¸­è¡¨ç°ä¼˜å¼‚ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**



1. Title: Prompt-Softbox-Prompt: A free-text Embedding Control for Image Editing
                 (ä¸­æ–‡ç¿»è¯‘ï¼šåŸºäºè‡ªç”±æ–‡æœ¬åµŒå…¥æ§åˆ¶çš„å›¾åƒç¼–è¾‘æ–¹æ³•)

2. Authors: Yitong Yang, Yinglin Wang*, Jing Wang, Tian Zhang

3. Affiliation: School of Information Management Engineering, Shanghai University of Finance and Economics

4. Keywords: Text-driven diffusion models, image editing, text embeddings, controllable image editing, Stable Diffusion XL

5. Urls: arXiv:2408.13623v1 [cs.CV], Github: None

6. Summary:

    - (1):ç ”ç©¶èƒŒæ™¯ï¼šåŸºäºæ–‡æœ¬é©±åŠ¨çš„æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†æ¨¡å‹ä¸­çš„å…³é”®ç»„ä»¶â€”â€”æ–‡æœ¬åµŒå…¥â€”â€”å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚æ–‡æœ¬åµŒå…¥çš„å¤æ‚æ€§å’Œä¸é€æ˜æ€§å¯¹å®ç°ç²¾ç¡®çš„å›¾åƒç¼–è¾‘æå‡ºäº†é‡å¤§æŒ‘æˆ˜ã€‚

    - (2)ï¼šè¿‡å»çš„æ–¹æ³•ï¼šä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­äºåŸºäºé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒç¼–è¾‘ï¼Œä½†æ–‡æœ¬åµŒå…¥çš„è€¦åˆæ€§å’Œä¸é€æ˜æ€§é™åˆ¶äº†å›¾åƒç¼–è¾‘çš„å¯æ§æ€§ã€‚ä½œè€…æå‡ºçš„æ–¹æ³•æ˜¯åŸºäºå¯¹æ–‡æœ¬åµŒå…¥çš„æ·±å…¥åˆ†æï¼Œæ—¨åœ¨æé«˜å›¾åƒç¼–è¾‘çš„ç²¾ç¡®æ€§å’Œå¯æ§æ€§ã€‚

    - (3)ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡å¯¹Stable Diffusion XLæ¨¡å‹ä¸­çš„æ–‡æœ¬åµŒå…¥è¿›è¡Œäº†å…¨é¢åˆ†æï¼Œæå‡ºäº†åä¸ºPSPï¼ˆPrompt-Softbox-Promptï¼‰çš„æ–°çš„å›¾åƒç¼–è¾‘æ–¹æ³•ã€‚PSPé€šè¿‡åœ¨äº¤å‰æ³¨æ„åŠ›å±‚ä¸­æ’å…¥æˆ–æ·»åŠ æ–‡æœ¬åµŒå…¥ï¼Œå¹¶ä½¿ç”¨Softboxå®šä¹‰å’Œæ§åˆ¶è¯­ä¹‰æ³¨å…¥çš„å…·ä½“åŒºåŸŸï¼Œå®ç°äº†ç²¾ç¡®çš„å›¾åƒç¼–è¾‘ã€‚

    - (4)ï¼šä»»åŠ¡å’Œæ€§èƒ½ï¼šPSPåœ¨ç‰©ä½“æ›¿æ¢ã€ç‰©ä½“æ·»åŠ å’Œé£æ ¼è¿ç§»ç­‰ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPSPåœ¨å®ç°å¯æ§å›¾åƒç¼–è¾‘æ–¹é¢å…·æœ‰å¾ˆé«˜çš„æ€§èƒ½ï¼Œæ”¯æŒäº†å…¶ç ”ç©¶ç›®æ ‡ã€‚
7. Methods:

    - (1): å¯¹SDXLæ¨¡å‹ä¸­çš„æ–‡æœ¬åµŒå…¥è¿›è¡Œäº†å…¨é¢åˆ†æï¼Œæ­ç¤ºäº†æ–‡æœ¬åµŒå…¥åœ¨æ‰©æ•£æ¨¡å‹ä¸­çš„ä½œç”¨æœºåˆ¶ã€‚
 
    - (2): æå‡ºäº†PSPï¼ˆPrompt-Softbox-Promptï¼‰æ–¹æ³•ï¼Œé€šè¿‡åœ¨äº¤å‰æ³¨æ„åŠ›å±‚ä¸­æ’å…¥æˆ–æ·»åŠ æ–‡æœ¬åµŒå…¥ï¼Œå®ç°ç²¾ç¡®çš„å›¾åƒç¼–è¾‘ã€‚
 
    - (3): ä½¿ç”¨Softboxæœºåˆ¶ï¼Œå°†ç›®æ ‡æ–‡æœ¬çš„è¯­ä¹‰æ³¨å…¥åˆ°æºå›¾åƒä¸­å¯¹åº”ç‰©ä½“çš„åŒºåŸŸï¼Œå‡å°‘æºç‰©ä½“çš„å½±å“ã€‚
 
    - (4): é€šè¿‡Otsuæ–¹æ³•è®¡ç®—ç›®æ ‡ç‰©ä½“çš„æ©ç ï¼Œç”¨äºæŒ‡å¯¼è¯­ä¹‰æ³¨å…¥ã€‚
 
    - (5): åœ¨äº¤å‰æ³¨æ„åŠ›å±‚ä¸­ä½¿ç”¨æ›¿æ¢å‡½æ•°ï¼Œå°†æºå›¾åƒä¸­çš„ç‰©ä½“æ›¿æ¢ä¸ºç›®æ ‡å›¾åƒä¸­çš„ç‰©ä½“ã€‚


8. Conclusion:

    - (1)ï¼šè¿™é¡¹å·¥ä½œçš„æ„ä¹‰åœ¨äºï¼Œå®ƒæ·±å…¥ç ”ç©¶äº†æ–‡æœ¬åµŒå…¥åœ¨å›¾åƒç¼–è¾‘ä¸­çš„ä½œç”¨ï¼Œå¹¶æå‡ºäº†PSPï¼ˆPrompt-Softbox-Promptï¼‰æ–¹æ³•ï¼Œä¸ºåŸºäºæ–‡æœ¬é©±åŠ¨çš„æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘é¢†åŸŸçš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯å’Œè§£å†³æ–¹æ¡ˆã€‚

    - (2): Innovation point: åœ¨åˆ›æ–°ç‚¹ä¸Šï¼Œæœ¬æ–‡æå‡ºçš„PSPæ–¹æ³•é€šè¿‡åœ¨äº¤å‰æ³¨æ„åŠ›å±‚ä¸­å¼•å…¥æ–‡æœ¬åµŒå…¥ï¼Œå®ç°äº†å¯¹å›¾åƒç¼–è¾‘çš„ç²¾ç¡®æ§åˆ¶ï¼Œè¿™ä¸€æ–¹æ³•åœ¨ç†è®ºä¸Šå…·æœ‰åˆ›æ–°æ€§ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ï¼›Performance: åœ¨æ€§èƒ½ä¸Šï¼ŒPSPåœ¨ç‰©ä½“æ›¿æ¢ã€ç‰©ä½“æ·»åŠ å’Œé£æ ¼è¿ç§»ç­‰ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœï¼Œå®éªŒç»“æœè¡¨æ˜å…¶æ€§èƒ½ä¼˜è¶Šï¼›Workload: åœ¨å·¥ä½œè´Ÿè½½ä¸Šï¼ŒPSPæ–¹æ³•å¯¹è®¡ç®—èµ„æºçš„è¦æ±‚è¾ƒé«˜ï¼Œéœ€è¦è¾ƒå¤§çš„è®¡ç®—èµ„æºæ¥æ”¯æŒå…¶åœ¨å¤æ‚å›¾åƒä¸Šçš„ç¼–è¾‘ä»»åŠ¡ã€‚




<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-cd194d5a994a16882f51d0dc52a15dde.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3aaaf8ae65db4eda8d405d426ebc07e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4792380a1a0fe27e176dcef4ee21f51a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5e3c1f737b452426c7962b88137051f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7531ae91824e4a59753ceff6cf1a9674.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ba7004275d7183cab18dabad7e8209c3.jpg" align="middle">
</details>




## DualAnoDiff: Dual-Interrelated Diffusion Model for Few-Shot Anomaly   Image Generation

**Authors:Ying Jin, Jinlong Peng, Qingdong He, Teng Hu, Hao Chen, Jiafu Wu, Wenbing Zhu, Mingmin Chi, Jun Liu, Yabiao Wang, Chengjie Wang**

The performance of anomaly inspection in industrial manufacturing is constrained by the scarcity of anomaly data. To overcome this challenge, researchers have started employing anomaly generation approaches to augment the anomaly dataset. However, existing anomaly generation methods suffer from limited diversity in the generated anomalies and struggle to achieve a seamless blending of this anomaly with the original image. In this paper, we overcome these challenges from a new perspective, simultaneously generating a pair of the overall image and the corresponding anomaly part. We propose DualAnoDiff, a novel diffusion-based few-shot anomaly image generation model, which can generate diverse and realistic anomaly images by using a dual-interrelated diffusion model, where one of them is employed to generate the whole image while the other one generates the anomaly part. Moreover, we extract background and shape information to mitigate the distortion and blurriness phenomenon in few-shot image generation. Extensive experiments demonstrate the superiority of our proposed model over state-of-the-art methods in terms of both realism and diversity. Overall, our approach significantly improves the performance of downstream anomaly detection tasks, including anomaly detection, anomaly localization, and anomaly classification tasks. 

[PDF](http://arxiv.org/abs/2408.13509v1) 

**Summary**
è¯¥æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ‰©æ•£æ¨¡å‹DualAnoDiffï¼Œé€šè¿‡ç”Ÿæˆæ•´ä½“å›¾åƒå’Œå¼‚å¸¸éƒ¨åˆ†ï¼Œå®ç°å¤šæ ·åŒ–ã€é€¼çœŸçš„å¼‚å¸¸å›¾åƒç”Ÿæˆã€‚

**Key Takeaways**
1. å¼‚å¸¸æ£€æµ‹åœ¨å·¥ä¸šåˆ¶é€ ä¸­å—é™äºå¼‚å¸¸æ•°æ®ç¨€ç¼ºã€‚
2. ç°æœ‰å¼‚å¸¸ç”Ÿæˆæ–¹æ³•å­˜åœ¨å¤šæ ·æ€§å’Œèåˆé—®é¢˜ã€‚
3. æå‡ºDualAnoDiffï¼Œé€šè¿‡åŒé‡æ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒå’Œå¼‚å¸¸éƒ¨åˆ†ã€‚
4. åˆ©ç”¨èƒŒæ™¯å’Œå½¢çŠ¶ä¿¡æ¯å‡å°‘ç”Ÿæˆå›¾åƒçš„å¤±çœŸå’Œæ¨¡ç³Šã€‚
5. å®éªŒè¯æ˜DualAnoDiffåœ¨çœŸå®æ€§å’Œå¤šæ ·æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚
6. æ˜¾è‘—æå‡äº†å¼‚å¸¸æ£€æµ‹ã€å®šä½å’Œåˆ†ç±»ä»»åŠ¡çš„æ€§èƒ½ã€‚
7. æ¨¡å‹é€‚ç”¨äºå¤šç§å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**



1. Title: åŒå‘å…³è”æ‰©æ•£æ¨¡å‹åœ¨å°‘æ ·æœ¬å¼‚å¸¸å›¾åƒç”Ÿæˆä¸­çš„åº”ç”¨
                 2. Authors: Ying Jin, Jinlong Peng, Qingdong He, Teng Hu, Hao Chen, Jiafu Wu, Wenbing Zhu, Mingmin Chi, Jun Liu, Yabiao Wang, Chengjie Wang
                 3. Affiliation: å¤æ—¦å¤§å­¦
                 4. Keywords: å¼‚å¸¸æ£€æµ‹ï¼Œå°‘æ ·æœ¬å­¦ä¹ ï¼Œå›¾åƒç”Ÿæˆï¼Œæ‰©æ•£æ¨¡å‹ï¼Œå¼‚å¸¸æ•°æ®å¢å¼º
                 5. Urls: https://arxiv.org/abs/2408.13509v1 or Github: None
                 6. Summary:
                    - (1):è¯¥æ–‡ç« çš„ç ”ç©¶èƒŒæ™¯æ˜¯å·¥ä¸šåˆ¶é€ ä¸­å¼‚å¸¸æ£€æµ‹çš„æ€§èƒ½å—é™äºå¼‚å¸¸æ•°æ®çš„ç¨€ç¼ºæ€§ã€‚
 
                    - (2):è¿‡å»çš„æ–¹æ³•åŒ…æ‹¬åŸºäºæ¨¡å‹çš„æ–¹æ³•å’Œç”Ÿæˆæ–¹æ³•ã€‚åŸºäºæ¨¡å‹çš„æ–¹æ³•é€šè¿‡éšæœºåˆ‡å‰²å’Œç²˜è´´ç°æœ‰å¼‚å¸¸æˆ–å¼‚å¸¸çº¹ç†æ•°æ®é›†çš„ç‰‡æ®µåˆ°æ­£å¸¸æ ·æœ¬ä¸Šï¼Œä½†ç”Ÿæˆçš„å¼‚å¸¸æ•°æ®ä¸çœŸå®ã€‚ç”Ÿæˆæ–¹æ³•ä½¿ç”¨ç”Ÿæˆæ¨¡å‹å¦‚GANså’Œæ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå¼‚å¸¸æ•°æ®ï¼Œä½†GANséœ€è¦å¤§é‡è®­ç»ƒæ•°æ®ï¼Œä¸”æ— æ³•ç”Ÿæˆæ©ç ï¼›DFMGANè™½ç„¶è¿ç§»åˆ°å¼‚å¸¸æ•°æ®ï¼Œä½†ç”Ÿæˆçš„å¼‚å¸¸ä¸çœŸå®ï¼Œæ©ç å¯¹é½ä¸è¶³ï¼›AnomalyDiffusionç¼ºä¹æ˜¾å¼çš„å¯¹é½çº¦æŸè®¾è®¡ã€‚è¯¥æ–¹æ³•çš„åŠ¨æœºæ˜¯ä¸ºäº†å…‹æœç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚
 
                    - (3):è¯¥æ–‡ç« æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å°‘æ ·æœ¬å¼‚å¸¸å›¾åƒç”Ÿæˆæ¨¡å‹DualAnoDiffï¼Œé€šè¿‡åŒå‘å…³è”æ‰©æ•£æ¨¡å‹åŒæ—¶ç”Ÿæˆæ•´ä½“å›¾åƒå’Œç›¸åº”çš„å¼‚å¸¸éƒ¨åˆ†ï¼Œå¹¶é€šè¿‡æå–èƒŒæ™¯å’Œå½¢çŠ¶ä¿¡æ¯æ¥å‡è½»å°‘æ ·æœ¬å›¾åƒç”Ÿæˆä¸­çš„æ‰­æ›²å’Œæ¨¡ç³Šç°è±¡ã€‚
 
                    - (4):è¯¥æ¨¡å‹åœ¨å¼‚å¸¸æ£€æµ‹ã€å¼‚å¸¸å®šä½å’Œå¼‚å¸¸åˆ†ç±»ä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨çœŸå®æ€§å’Œå¤šæ ·æ€§æ–¹é¢çš„ä¼˜è¶Šæ€§ï¼Œæ”¯æŒäº†å…¶æé«˜ä¸‹æ¸¸å¼‚å¸¸æ£€æµ‹ä»»åŠ¡æ€§èƒ½çš„ç›®æ ‡ã€‚
7. Methods:

    - (1): è¯¥æ–‡ç« é’ˆå¯¹å·¥ä¸šåˆ¶é€ ä¸­å¼‚å¸¸æ£€æµ‹æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºDualAnoDiffçš„å°‘æ ·æœ¬å¼‚å¸¸å›¾åƒç”Ÿæˆæ¨¡å‹ã€‚

    - (2): è¯¥æ¨¡å‹å°†å¼‚å¸¸å›¾åƒåˆ†è§£ä¸ºæ•´ä½“å¼‚å¸¸å›¾åƒå’Œç›¸åº”çš„å¼‚å¸¸éƒ¨åˆ†ï¼Œåˆ†åˆ«ä½¿ç”¨ä¸¤ä¸ªæ‰©æ•£æ¨¡å‹ï¼ˆSDå’ŒSD*ï¼‰ç”Ÿæˆã€‚

    - (3): åŒå‘å…³è”æ‰©æ•£æ¨¡å‹é€šè¿‡å…±äº«ä¿¡æ¯æ¨¡å—ï¼ˆSelf-attention Interaction Moduleï¼ŒSAIMï¼‰å®ç°ä¸¤ä¸ªæ¨¡å‹çš„åŒæ­¥å’Œå…±äº«ã€‚

    - (4): æ¨¡å‹ä½¿ç”¨åµŒå¥—æç¤ºï¼ˆNested Promptsï¼‰æ¥æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ï¼Œç¡®ä¿ç”Ÿæˆå›¾åƒä¸å¼‚å¸¸éƒ¨åˆ†ä¹‹é—´çš„åŒ…å«å…³ç³»ã€‚

    - (5): é€šè¿‡æ·»åŠ LoRAï¼ˆLow-Rank Adaptationï¼‰æ¥æé«˜ç”Ÿæˆå›¾åƒçš„å¤šæ ·æ€§å’ŒçœŸå®æ€§ã€‚

    - (6): æ¨¡å‹é€šè¿‡æå–èƒŒæ™¯å’Œå½¢çŠ¶ä¿¡æ¯æ¥å‡è½»å°‘æ ·æœ¬å›¾åƒç”Ÿæˆä¸­çš„æ‰­æ›²å’Œæ¨¡ç³Šç°è±¡ã€‚

    - (7): ä½¿ç”¨ç°æœ‰åˆ†å‰²ç®—æ³•æˆ–æ³¨æ„åŠ›å›¾æ¥ç”Ÿæˆé«˜è´¨é‡çš„å¼‚å¸¸éƒ¨åˆ†æ©ç ã€‚

    - (8): èƒŒæ™¯è¡¥å¿æ¨¡å—ç”¨äºå¤„ç†è®­ç»ƒæ•°æ®æœ‰é™å¯¼è‡´çš„é—®é¢˜ï¼Œæé«˜æ¨¡å‹åœ¨ç‰¹å®šæƒ…å†µä¸‹çš„æ€§èƒ½ã€‚


8. Conclusion:

                    - (1):è¯¥ç ”ç©¶å·¥ä½œçš„æ„ä¹‰åœ¨äºé’ˆå¯¹å·¥ä¸šåˆ¶é€ é¢†åŸŸå¼‚å¸¸æ£€æµ‹æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å°‘æ ·æœ¬å¼‚å¸¸å›¾åƒç”Ÿæˆæ¨¡å‹DualAnoDiffï¼Œæœ‰æ•ˆæå‡äº†å¼‚å¸¸æ£€æµ‹çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚

                    - (2):Innovation point: DualAnoDiffæ¨¡å‹åœ¨åˆ›æ–°ç‚¹ä¸Šçš„ä¼˜åŠ¿åœ¨äºæå‡ºäº†åŒå‘å…³è”æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶ç”Ÿæˆæ•´ä½“å›¾åƒå’Œç›¸åº”çš„å¼‚å¸¸éƒ¨åˆ†ï¼Œæé«˜äº†ç”Ÿæˆçš„å¼‚å¸¸æ•°æ®çš„çœŸå®æ€§å’Œå¤šæ ·æ€§ï¼›Performance: åœ¨å¼‚å¸¸æ£€æµ‹ã€å¼‚å¸¸å®šä½å’Œå¼‚å¸¸åˆ†ç±»ä»»åŠ¡ä¸Šï¼ŒDualAnoDiffæ¨¡å‹è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨æ€§èƒ½ä¸Šçš„ä¼˜è¶Šæ€§ï¼›Workload: æ¨¡å‹è®¾è®¡è€ƒè™‘äº†å°‘æ ·æœ¬åœºæ™¯ï¼Œé€šè¿‡èƒŒæ™¯è¡¥å¿æ¨¡å—å’ŒLoRAæŠ€æœ¯å‡è½»äº†è®­ç»ƒæ•°æ®æœ‰é™çš„é—®é¢˜ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ä»éœ€è¿›ä¸€æ­¥ä¼˜åŒ–ä»¥é™ä½è®¡ç®—è´Ÿè½½ã€‚




<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-17a59456e71890563000316145cecdbf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cbfc4f12c05b11ce7a493903f4a0d91e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e171c0e7aaa93547532e37d241d78c74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94f5f5384c2c71929beb8d6ffd9b91ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74c028ba670ec8e56345a2e45c080e18.jpg" align="middle">
</details>




## Show-o: One Single Transformer to Unify Multimodal Understanding and   Generation

**Authors:Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, Mike Zheng Shou**

We present a unified transformer, i.e., Show-o, that unifies multimodal understanding and generation. Unlike fully autoregressive models, Show-o unifies autoregressive and (discrete) diffusion modeling to adaptively handle inputs and outputs of various and mixed modalities. The unified model flexibly supports a wide range of vision-language tasks including visual question-answering, text-to-image generation, text-guided inpainting/extrapolation, and mixed-modality generation. Across various benchmarks, it demonstrates comparable or superior performance to existing individual models with an equivalent or larger number of parameters tailored for understanding or generation. This significantly highlights its potential as a next-generation foundation model. Code and models are released at https://github.com/showlab/Show-o. 

[PDF](http://arxiv.org/abs/2408.12528v2) Technical Report

**Summary**
ç»Ÿä¸€Transformer Show-oæ¨¡å‹ï¼Œèåˆå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆï¼Œæå‡è§†è§‰è¯­è¨€ä»»åŠ¡æ€§èƒ½ã€‚

**Key Takeaways**
1. Show-oæ¨¡å‹èåˆè‡ªå›å½’å’Œæ‰©æ•£å»ºæ¨¡ã€‚
2. æ”¯æŒå¤šç§è§†è§‰è¯­è¨€ä»»åŠ¡ï¼Œå¦‚é—®ç­”ã€å›¾åƒç”Ÿæˆç­‰ã€‚
3. æ€§èƒ½ä¼˜äºåŒç±»æ¨¡å‹ï¼Œå‚æ•°æ›´å°‘ã€‚
4. æœ‰æœ›æˆä¸ºä¸‹ä¸€ä»£åŸºç¡€æ¨¡å‹ã€‚
5. å¼€æºä»£ç å’Œæ¨¡å‹ã€‚
6. é€‚ç”¨äºå¤šç§æ¨¡æ€çš„è¾“å…¥è¾“å‡ºå¤„ç†ã€‚
7. æ¶µç›–å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**



1. Title: SHOW-O: ONE SINGLE TRANSFORMER TO UNIFY MULTIMODAL UNDERSTANDING AND GENERATION (ç»Ÿä¸€å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆçš„å•ä¸ªTransformer)

2. Authors: Jinheng Xieâ€ , Weijia Maoâ€ , Zechen Baiâ€ , David Junhao Zhangâ€ , Weihao Wang2, Kevin Qinghong Lin1, Yuchao Gu1, Zhijie Chen2, Zhenheng Yang2, Mike Zheng Shou1âˆ—

3. Affiliation: Show Lab, National University of Singapore

4. Keywords: unified transformer, multimodal understanding, generation, vision-language tasks, transformer architecture

5. Urls: https://arxiv.org/abs/2408.12528, Github: https://github.com/showlab/Show-o

6. Summary:

    - (1):æœ¬æ–‡çš„ç ”ç©¶èƒŒæ™¯æ˜¯å¤šæ¨¡æ€æ™ºèƒ½çš„ä¸¤ä¸ªå…³é”®æ”¯æŸ±â€”â€”ç†è§£å’Œç”Ÿæˆã€‚å¤šæ¨¡æ€ç†è§£æ–¹é¢ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼›è€Œåœ¨è§†è§‰ç”Ÿæˆæ–¹é¢ï¼Œå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMsï¼‰åœ¨æ–‡æœ¬åˆ°å›¾åƒ/è§†é¢‘ç”Ÿæˆæ–¹é¢å–å¾—äº†å‰æ‰€æœªæœ‰çš„æ€§èƒ½ã€‚

    - (2)ï¼šè¿‡å»çš„æ–¹æ³•ä¸»è¦æ˜¯å°†ç†è§£å’Œç”Ÿæˆè§†ä¸ºç‹¬ç«‹çš„é¢†åŸŸï¼Œå¹¶ä½¿ç”¨ä¸“é—¨çš„æ¨¡å‹åˆ†åˆ«å¤„ç†ã€‚ä¾‹å¦‚ï¼ŒNExT-GPTä½¿ç”¨åŸºç¡€è¯­è¨€æ¨¡å‹è¿›è¡Œå¤šæ¨¡æ€ç†è§£ï¼Œä½†éœ€è¦é¢å¤–çš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒç”Ÿæˆã€‚è¿™ç§æ–¹æ³•çš„å±€é™æ€§åœ¨äºæ¨¡å‹ä¹‹é—´çš„åˆ†ç¦»ï¼Œä¸”ä¸åŒæ¨¡å‹æ¶æ„çš„å·®å¼‚ä½¿å¾—ç»Ÿä¸€å¤„ç†æˆä¸ºä¸€ä¸ªæŒ‘æˆ˜ã€‚

    - (3)ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„Transformeræ¨¡å‹Show-oï¼Œå®ƒå°†è‡ªå›å½’æ¨¡å‹å’Œï¼ˆç¦»æ•£ï¼‰æ‰©æ•£æ¨¡å‹ç»“åˆï¼Œä»¥é€‚åº”æ€§åœ°å¤„ç†å„ç§å’Œæ··åˆæ¨¡æ€çš„è¾“å…¥å’Œè¾“å‡ºã€‚Show-oå¯ä»¥çµæ´»æ”¯æŒå¹¿æ³›çš„è§†è§‰è¯­è¨€ä»»åŠ¡ã€‚

    - (4)ï¼šåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒShow-oçš„æ€§èƒ½ä¸ä¸“é—¨é’ˆå¯¹ç†è§£å’Œç”Ÿæˆå®šåˆ¶çš„æ¨¡å‹ç›¸å½“æˆ–æ›´ä¼˜ï¼Œä¸”å‚æ•°æ•°é‡ç›¸å½“æˆ–æ›´å¤§ã€‚è¿™æ˜¾è‘—å‡¸æ˜¾äº†å…¶ä½œä¸ºä¸‹ä¸€ä»£åŸºç¡€æ¨¡å‹çš„æ½œåŠ›ã€‚
7. Methods:

    - (1): é’ˆå¯¹å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆä»»åŠ¡ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º Show-o çš„ç»Ÿä¸€ Transformer æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†è‡ªå›å½’æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ï¼Œä»¥é€‚åº”æ€§åœ°å¤„ç†ä¸åŒå’Œæ··åˆæ¨¡æ€çš„è¾“å…¥å’Œè¾“å‡ºã€‚

    - (2): é¦–å…ˆï¼Œé€šè¿‡å°†æ–‡æœ¬å’Œå›¾åƒæ•°æ®åˆ†åˆ«è¿›è¡Œåˆ†è¯ï¼Œå°†æ–‡æœ¬å’Œå›¾åƒè½¬æ¢ä¸ºç¦»æ•£çš„ tokensï¼Œä¸ºç»Ÿä¸€å­¦ä¹ æ„å»ºè¾“å…¥/è¾“å‡ºç©ºé—´ã€‚

    - (3): ç„¶åï¼Œè®¾è®¡äº†ç»Ÿä¸€çš„æç¤ºç­–ç•¥ï¼Œå°†ä¸åŒç±»å‹çš„è¾“å…¥æ•°æ®æ ¼å¼åŒ–ï¼Œä»¥ä¾¿åœ¨ç»Ÿä¸€æ¨¡å‹ä¸­è¿›è¡Œå­¦ä¹ ã€‚

    - (4): Show-o ç»§æ‰¿äº†ç°æœ‰ LLM çš„æ¶æ„ï¼Œå¹¶åœ¨æ¯ä¸ªæ³¨æ„åŠ›å±‚å‰æ·»åŠ äº† QK-Norm æ“ä½œï¼Œä»¥å¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚

    - (5): å¼•å…¥äº†ä¸€ç§åä¸ºâ€œå…¨èƒ½æ³¨æ„åŠ›æœºåˆ¶â€çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¯¥æœºåˆ¶å¯ä»¥æ ¹æ®è¾“å…¥åºåˆ—çš„æ ¼å¼è‡ªé€‚åº”åœ°æ··åˆå’Œæ”¹å˜æ³¨æ„åŠ›æ¨¡å¼ã€‚

    - (6): ä¸ºäº†åŒæ—¶è¿›è¡Œè‡ªå›å½’å’Œæ‰©æ•£å»ºæ¨¡ï¼Œé‡‡ç”¨ä¸¤ä¸ªå­¦ä¹ ç›®æ ‡ï¼šä¸‹ä¸€ä¸ª token é¢„æµ‹ (NTP) å’Œæ©ç  token é¢„æµ‹ (MTP)ã€‚

    - (7): æå‡ºäº†ä¸€ä¸ªä¸‰é˜¶æ®µçš„è®­ç»ƒæµç¨‹ï¼Œä»¥æœ‰æ•ˆåœ°è®­ç»ƒç»Ÿä¸€æ¨¡å‹ï¼ŒåŒ…æ‹¬æ•°æ®å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒå’Œæ¨¡å‹è¯„ä¼°ã€‚


8. Conclusion:

                    - (1):è¿™é¡¹å·¥ä½œçš„é‡è¦æ€§åœ¨äºï¼Œå®ƒé¦–æ¬¡æå‡ºäº†ä¸€ç§åä¸ºShow-oçš„ç»Ÿä¸€Transformeræ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤ŸåŒæ—¶å¤„ç†å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆä»»åŠ¡ï¼Œå®ç°äº†è‡ªå›å½’æ¨¡å‹å’Œï¼ˆç¦»æ•£ï¼‰æ‰©æ•£æ¨¡å‹çš„ç»Ÿä¸€ï¼Œä¸ºå¤šæ¨¡æ€æ™ºèƒ½çš„å‘å±•æä¾›äº†æ–°çš„æ€è·¯ã€‚

                    - (2):Innovation point: åœ¨åˆ›æ–°ç‚¹ä¸Šï¼ŒShow-oæ¨¡å‹çš„è®¾è®¡å…·æœ‰å‰ç»æ€§ï¼Œå®ƒç»“åˆäº†è‡ªå›å½’å’Œæ‰©æ•£æ¨¡å‹ï¼Œä¸ºå¤„ç†ä¸åŒæ¨¡æ€æ•°æ®æä¾›äº†ä¸€ç§å…¨æ–°çš„æ–¹æ³•ï¼›Performance: åœ¨æ€§èƒ½ä¸Šï¼ŒShow-oåœ¨å¤šä¸ªè§†è§‰è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…¶æ€§èƒ½ä¸ä¸“é—¨å®šåˆ¶çš„æ¨¡å‹ç›¸å½“ç”šè‡³æ›´ä¼˜ï¼›Workload: åœ¨å·¥ä½œé‡ä¸Šï¼Œå°½ç®¡Show-oæ¨¡å‹å‚æ•°æ•°é‡ä¸ä¸“é—¨æ¨¡å‹ç›¸å½“æˆ–æ›´å¤§ï¼Œä½†å…¶è®­ç»ƒæµç¨‹å’Œè¯„ä¼°æ–¹æ³•ç›¸å¯¹é«˜æ•ˆï¼Œé™ä½äº†æ•´ä½“çš„å·¥ä½œé‡ã€‚




<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-040abb8d449461d49d65c3f779921419.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-056c07c97782ed5ed08f0465d138baf5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5cf02d21e407cc9a4c8ae977d002203.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9225c3124329b51192ed7a4dce15540.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-031c84cfa694f10566845f8683771152.jpg" align="middle">
</details>




## InstantStyleGaussian: Efficient Art Style Transfer with 3D Gaussian   Splatting

**Authors:Xin-Yi Yu, Jun-Xin Yu, Li-Bo Zhou, Yan Wei, Lin-Lin Ou**

We present InstantStyleGaussian, an innovative 3D style transfer method based on the 3D Gaussian Splatting (3DGS) scene representation. By inputting a target-style image, it quickly generates new 3D GS scenes. Our method operates on pre-reconstructed GS scenes, combining diffusion models with an improved iterative dataset update strategy. It utilizes diffusion models to generate target style images, adds these new images to the training dataset, and uses this dataset to iteratively update and optimize the GS scenes, significantly accelerating the style editing process while ensuring the quality of the generated scenes. Extensive experimental results demonstrate that our method ensures high-quality stylized scenes while offering significant advantages in style transfer speed and consistency. 

[PDF](http://arxiv.org/abs/2408.04249v2) 

**Summary**
åŸºäº3Dé«˜æ–¯åˆ†è£‚çš„3Dé£æ ¼è¿ç§»ï¼Œæ˜¾è‘—æé«˜é£æ ¼ç¼–è¾‘é€Ÿåº¦å’Œè´¨é‡ã€‚

**Key Takeaways**
- å¼•å…¥å³æ—¶é£æ ¼é«˜æ–¯ï¼ŒåŸºäº3Dé«˜æ–¯åˆ†è£‚åœºæ™¯è¡¨ç¤ºçš„3Dé£æ ¼è¿ç§»æ–¹æ³•ã€‚
- å¿«é€Ÿç”Ÿæˆç›®æ ‡é£æ ¼3Dåœºæ™¯ã€‚
- ç»“åˆæ‰©æ•£æ¨¡å‹å’Œè¿­ä»£æ•°æ®é›†æ›´æ–°ç­–ç•¥ã€‚
- åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆç›®æ ‡é£æ ¼å›¾åƒï¼ŒåŠ å…¥è®­ç»ƒæ•°æ®é›†ï¼Œè¿­ä»£ä¼˜åŒ–åœºæ™¯ã€‚
- æ˜¾è‘—æå‡é£æ ¼ç¼–è¾‘é€Ÿåº¦å’Œè´¨é‡ã€‚
- å®éªŒè¯æ˜æ–¹æ³•ç”Ÿæˆé«˜è´¨é‡åœºæ™¯ï¼Œé£æ ¼è½¬ç§»é€Ÿåº¦å¿«ã€ä¸€è‡´æ€§é«˜ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**



1. Title: InstantStyleGaussian: Efficient Art Style Transfer
                 (ä¸­æ–‡ç¿»è¯‘ï¼šåŸºäº3Dé«˜æ–¯æ•£å¸ƒçš„å¿«é€Ÿè‰ºæœ¯é£æ ¼è¿ç§»)

2. Authors: Xin-Yi Yu, Jun-Xin Yu, Li-Bo Zhou, Yan Wei, Lin-Lin Ou

3. Affiliation: (ç¬¬ä¸€ä½œè€…æ‰€å±æœºæ„ï¼Œä¸­æ–‡ç¿»è¯‘ï¼šæœªçŸ¥ï¼Œæ–‡ä¸­æœªæåŠ)

4. Keywords: 3D Gaussian Splatting, 3D Style Transfer, Iterative Dataset Update

5. Urls: arXiv:2408.04249v2 [cs.CV] 26 Aug 2024

                 Github: None

6. Summary:

                    - (1):è¯¥æ–‡ç« çš„ç ”ç©¶èƒŒæ™¯æ˜¯éšç€è™šæ‹Ÿç°å®ã€æœºå™¨äººæ¨¡æ‹Ÿå’Œè‡ªåŠ¨é©¾é©¶ç­‰åº”ç”¨çš„å¿«é€Ÿå‘å±•ï¼Œ3Dåœºæ™¯å’Œæ¨¡å‹çš„ç¼–è¾‘å˜å¾—æ—¥ç›Šé‡è¦ã€‚ç°æœ‰çš„3Dåœºæ™¯è¡¨ç¤ºæ–¹æ³•å¦‚ç½‘æ ¼å’Œç‚¹äº‘åœ¨ç¼–è¾‘å¤æ‚åœºæ™¯å’Œç»†èŠ‚æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚
 
                    - (2):è¿‡å»çš„æ–¹æ³•åŒ…æ‹¬åŸºäºæ‰©æ•£ç¼–è¾‘å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„3Dæ¨¡å‹ç¼–è¾‘ã€‚è¿™äº›æ–¹æ³•çš„é—®é¢˜åœ¨äºéœ€è¦å¤§é‡çš„å†…å­˜å’Œè®¡ç®—æ—¶é—´ï¼Œä¸”è§£ç æ–¹æ³•ä¼šå½±å“æœ€ç»ˆçš„é£æ ¼è¿ç§»æ•ˆæœï¼Œå¯èƒ½å¯¼è‡´å¤šè§†å›¾ä¸ä¸€è‡´æ€§å’Œæ•´ä½“åœºæ™¯è´¨é‡çš„ä¸‹é™ã€‚è¯¥æ–¹æ³•çš„æå‡ºæ˜¯æœ‰åŠ¨æœºçš„ï¼Œå› ä¸ºå®ƒæ—¨åœ¨æä¾›ä¸€ç§å¿«é€Ÿä¸”é«˜è´¨é‡çš„é£æ ¼è¿ç§»è§£å†³æ–¹æ¡ˆã€‚
 
                    - (3)ï¼šè¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäº3Dé«˜æ–¯æ•£å¸ƒï¼ˆ3DGSï¼‰åœºæ™¯è¡¨ç¤ºçš„3Dé£æ ¼è¿ç§»æ–¹æ³•ã€‚è¯¥æ–¹æ³•ç»“åˆäº†æ‰©æ•£æ¨¡å‹å’Œæ”¹è¿›çš„è¿­ä»£æ•°æ®é›†æ›´æ–°ç­–ç•¥ï¼Œé€šè¿‡è¾“å…¥ç›®æ ‡é£æ ¼å›¾åƒå¿«é€Ÿç”Ÿæˆæ–°çš„3Dåœºæ™¯ï¼Œå¹¶é€šè¿‡è¿­ä»£æ›´æ–°å’Œä¼˜åŒ–åœºæ™¯æ¥åŠ é€Ÿé£æ ¼ç¼–è¾‘è¿‡ç¨‹ã€‚
 
                    - (4)ï¼šè¯¥æ–¹æ³•åœ¨3Dé£æ ¼è¿ç§»ä»»åŠ¡ä¸Šå–å¾—äº†é«˜è´¨é‡çš„ç»“æœï¼Œå®ç°äº†å¿«é€Ÿçš„é£æ ¼è¿ç§»å’Œä¸€è‡´æ€§ä¿æŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é€Ÿåº¦å’Œæ€§èƒ½ä¸Šå‡ä¼˜äºä»¥å¾€çš„æ–¹æ³•ï¼Œæ”¯æŒäº†å…¶ç›®æ ‡ã€‚
7. Methods:

    - (1): è¯¥æ–¹æ³•åŸºäº3Dé«˜æ–¯æ•£å¸ƒï¼ˆ3DGSï¼‰åœºæ™¯è¡¨ç¤ºï¼Œç»“åˆäº†æ‰©æ•£æ¨¡å‹ï¼ˆInstantStyleï¼‰è¿›è¡Œ2Då›¾åƒé£æ ¼è¿ç§»ï¼Œå¹¶æ”¹è¿›äº†InstructNeRF2NeRFä¸­çš„è¿­ä»£æ•°æ®é›†æ›´æ–°ç­–ç•¥ã€‚

    - (2): åˆ©ç”¨Nearest Neighbor Feature Matching (NNFM) æŸå¤±å‡½æ•°ï¼Œå°†2Dé£æ ¼å›¾åƒä¸­çš„å¤æ‚é«˜é¢‘è§†è§‰ç»†èŠ‚ä¼ é€’åˆ°3Dåœºæ™¯ä¸­ï¼Œä»¥æ›´å¥½åœ°ä¿æŒå±€éƒ¨çº¹ç†ç»†èŠ‚ã€‚

    - (3): é€šè¿‡è¿­ä»£æ›´æ–°è®­ç»ƒæ•°æ®é›†å›¾åƒï¼Œåˆ©ç”¨å›¾åƒæ¡ä»¶æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆæ–°çš„2Då›¾åƒï¼Œå¹¶å°†å…¶é£æ ¼è¿ç§»æ•ˆæœåé¦ˆåˆ°3DGSåœºæ™¯ä¸­ã€‚

    - (4): åº”ç”¨è¾¹ç¼˜æ£€æµ‹å›¾ä»¥ä¿æŒåœºæ™¯çš„åŸºæœ¬ç»“æ„ï¼Œç¡®ä¿åŸå§‹ç‰©ä½“çš„å½¢çŠ¶å’Œç»“æ„åœ¨ä¼ é€’åˆ°GSåœºæ™¯æ—¶ä¸ä¼šå‘ç”Ÿæ˜¾è‘—å˜åŒ–ã€‚

    - (5): ä½¿ç”¨L1å’ŒLPIPSæŸå¤±å‡½æ•°è®­ç»ƒé«˜æ–¯æ•£å¸ƒï¼Œä»¥å¤„ç†ä¸åŒè§†è§’çš„å±€éƒ¨ä¸ä¸€è‡´çº¹ç†ã€‚

    - (6): å®æ–½ç»†èŠ‚åŒ…æ‹¬ä½¿ç”¨gsplatåº“ä½œä¸ºåº•å±‚æ¨¡å‹å’Œå¯è§†åŒ–å·¥å…·ï¼Œä»¥åŠInstantStyleä½œä¸ºæ‰©æ•£æ¨¡å‹ã€‚é€šè¿‡è°ƒæ•´æ§åˆ¶ç½‘ç»œçš„æ¡ä»¶ç¼©æ”¾å’Œæ–‡æœ¬ã€å›¾åƒè°ƒæ•´çš„å¼•å¯¼æƒé‡æ¥æ§åˆ¶æ‰©æ•£æ¨¡å‹çš„æ›´æ–°å¼ºåº¦ã€‚

    - (7): è®­ç»ƒè¿‡ç¨‹æ¶‰åŠæœ€å¤š1kæ¬¡è¿­ä»£ï¼Œåœ¨A100 GPUä¸Šä»…éœ€20åˆ†é’Ÿå³å¯å®Œæˆåœºæ™¯çš„é£æ ¼è¿ç§»ç¼–è¾‘ï¼Œå…¶ä¸­å‰15åˆ†é’Ÿç”¨äºæ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒï¼Œå5åˆ†é’Ÿå°†è¿™äº›å›¾åƒåå‘ä¼ æ’­åˆ°æ•´ä¸ªåœºæ™¯ä¸­ã€‚


8. Conclusion:

                    - (1):è¯¥å·¥ä½œçš„æ˜¾è‘—æ„ä¹‰åœ¨äºï¼Œå®ƒæå‡ºäº†ä¸€ç§åŸºäº3Dé«˜æ–¯æ•£å¸ƒçš„å¿«é€Ÿè‰ºæœ¯é£æ ¼è¿ç§»æ–¹æ³•ï¼Œä¸º3Dåœºæ™¯å’Œæ¨¡å‹çš„ç¼–è¾‘æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”é«˜è´¨é‡çš„è§£å†³æ–¹æ¡ˆã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æå‡è™šæ‹Ÿç°å®ã€æœºå™¨äººæ¨¡æ‹Ÿå’Œè‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸçš„åº”ç”¨æ•ˆæœã€‚

                    - (2):Innovation point: InstantStyleGaussianæ–¹æ³•åœ¨3Dé£æ ¼è¿ç§»æ–¹é¢æå‡ºäº†åˆ›æ–°æ€§çš„3Dé«˜æ–¯æ•£å¸ƒåœºæ™¯è¡¨ç¤ºå’Œæ”¹è¿›çš„è¿­ä»£æ•°æ®é›†æ›´æ–°ç­–ç•¥ï¼Œæœ‰æ•ˆæé«˜äº†é£æ ¼è¿ç§»çš„é€Ÿåº¦å’Œè´¨é‡ï¼›Performance: å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨é€Ÿåº¦å’Œæ€§èƒ½ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†å¿«é€Ÿçš„é£æ ¼è¿ç§»å’Œä¸€è‡´æ€§ä¿æŒï¼›Workload: è¯¥æ–¹æ³•åœ¨A100 GPUä¸Šä»…éœ€20åˆ†é’Ÿå³å¯å®Œæˆåœºæ™¯çš„é£æ ¼è¿ç§»ç¼–è¾‘ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—è´Ÿè½½ã€‚




<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f9fedaa9225260030de0fe83c424b149.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4159b0eba641f3a329ed43b6ec03d3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c52e009fe3594898bd9bf1048600d7bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42d5d2c3b7457fabaeda63213d4e2444.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-651ddd779afa150611aa6acb63053ae1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e9fad5c512abc12a5b925eb993be8052.jpg" align="middle">
</details>




## Diffusion Feedback Helps CLIP See Better

**Authors:Wenxuan Wang, Quan Sun, Fan Zhang, Yepeng Tang, Jing Liu, Xinlong Wang**

Contrastive Language-Image Pre-training (CLIP), which excels at abstracting open-world representations across domains and modalities, has become a foundation for a variety of vision and multimodal tasks. However, recent studies reveal that CLIP has severe visual shortcomings, such as which can hardly distinguish orientation, quantity, color, structure, etc. These visual shortcomings also limit the perception capabilities of multimodal large language models (MLLMs) built on CLIP. The main reason could be that the image-text pairs used to train CLIP are inherently biased, due to the lack of the distinctiveness of the text and the diversity of images. In this work, we present a simple post-training approach for CLIP models, which largely overcomes its visual shortcomings via a self-supervised diffusion process. We introduce DIVA, which uses the DIffusion model as a Visual Assistant for CLIP. Specifically, DIVA leverages generative feedback from text-to-image diffusion models to optimize CLIP representations, with only images (without corresponding text). We demonstrate that DIVA improves CLIP's performance on the challenging MMVP-VLM benchmark which assesses fine-grained visual abilities to a large extent (e.g., 3-7%), and enhances the performance of MLLMs and vision models on multimodal understanding and segmentation tasks. Extensive evaluation on 29 image classification and retrieval benchmarks confirms that our framework preserves CLIP's strong zero-shot capabilities. The code is available at https://github.com/baaivision/DIVA. 

[PDF](http://arxiv.org/abs/2407.20171v4) 

**Summary**
CLIPæ¨¡å‹è§†è§‰ç¼ºé™·é€šè¿‡DIVAæ‰©æ•£æ¨¡å‹ä¼˜åŒ–ï¼Œæå‡è§†è§‰èƒ½åŠ›åŠå¤šæ¨¡æ€ç†è§£ã€‚

**Key Takeaways**
- CLIPæ¨¡å‹åœ¨è§†è§‰ä¸Šå­˜åœ¨ç¼ºé™·ï¼Œå¦‚éš¾ä»¥åŒºåˆ†æ–¹å‘ã€æ•°é‡ã€é¢œè‰²å’Œç»“æ„ã€‚
- CLIPçš„è§†è§‰ç¼ºé™·é™åˆ¶äº†åŸºäºCLIPçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚
- è®­ç»ƒCLIPçš„å›¾åƒ-æ–‡æœ¬å¯¹å­˜åœ¨å†…åœ¨åå·®ï¼Œå¯¼è‡´è§†è§‰èƒ½åŠ›å—é™ã€‚
- æå‡ºDIVAæ¨¡å‹ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹ä¼˜åŒ–CLIPè¡¨ç¤ºã€‚
- DIVAé€šè¿‡å›¾åƒä¼˜åŒ–CLIPï¼Œæ— éœ€å¯¹åº”æ–‡æœ¬ï¼Œæå‡æ€§èƒ½ã€‚
- DIVAåœ¨MMVP-VLMåŸºå‡†æµ‹è¯•ä¸­æé«˜äº†CLIPçš„ç»†ç²’åº¦è§†è§‰èƒ½åŠ›ã€‚
- DIVAå¢å¼ºMLLMså’Œè§†è§‰æ¨¡å‹çš„å¤šæ¨¡æ€ç†è§£å’Œåˆ†å‰²ä»»åŠ¡æ€§èƒ½ã€‚
- æ¡†æ¶ä¿æŒCLIPçš„å¼ºé›¶æ ·æœ¬èƒ½åŠ›ã€‚
- ä»£ç å¼€æºäºhttps://github.com/baaivision/DIVAã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**



1. Title: CLIPè§†è§‰è¾…åŠ©ï¼šæ‰©æ•£åé¦ˆå¸®åŠ©CLIPæ›´å¥½åœ°å·¥ä½œ
2. Authors: Wenxuan Wang, Quan Sun, Fan Zhang, Yepeng Tang, Jing Liu, Xinlong Wang
3. Affiliation: ä¸­å›½ç§‘å­¦é™¢è‡ªåŠ¨åŒ–ç ”ç©¶æ‰€
4. Keywords: CLIP, å¤šæ¨¡æ€é¢„è®­ç»ƒ, å›¾åƒ-æ–‡æœ¬å¯¹é½, æ‰©æ•£æ¨¡å‹, è§†è§‰è¾…åŠ©
5. Urls: https://rubics-xuan.github.io/DIVA/, Github: https://github.com/baaivision/DIVA
6. Summary:

   - (1):è¯¥æ–‡ç« çš„ç ”ç©¶èƒŒæ™¯æ˜¯CLIPï¼ˆContrastive Language-Image Pre-trainingï¼‰åœ¨è·¨é¢†åŸŸå’Œæ¨¡æ€æŠ½è±¡è¡¨ç¤ºæ–¹é¢çš„ä¼˜åŠ¿ï¼Œä»¥åŠå…¶è§†è§‰æ„ŸçŸ¥èƒ½åŠ›çš„å±€é™æ€§ã€‚
 
   - (2):è¿‡å»çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨CLIPçš„é¢„è®­ç»ƒå’Œå¾®è°ƒä¸Šï¼Œä½†è¿™äº›æ–¹æ³•ä¾èµ–äºå›¾åƒ-æ–‡æœ¬æ•°æ®å¯¹ï¼Œä¸”æ— æ³•å¤„ç†å›¾åƒæ•°æ®ã€‚æ­¤å¤–ï¼ŒCLIPçš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›å—é™ï¼Œéƒ¨åˆ†åŸå› åœ¨äºè®­ç»ƒæ•°æ®ä¸­çš„å›¾åƒ-æ–‡æœ¬å¯¹å­˜åœ¨å›ºæœ‰åå·®ã€‚è¯¥ç ”ç©¶æ–¹æ³•åŠ¨æœºæ˜ç¡®ï¼Œæ—¨åœ¨å…‹æœCLIPçš„è§†è§‰æ„ŸçŸ¥å±€é™ã€‚
 
   - (3):è¯¥æ–‡ç« æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„åè®­ç»ƒæ–¹æ³•ï¼Œç§°ä¸ºDIVAï¼Œç”¨äºæå‡CLIPçš„è§†è§‰è¡¨ç¤ºèƒ½åŠ›ã€‚DIVAåˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„åé¦ˆæ¥ä¼˜åŒ–CLIPçš„è¡¨ç¤ºï¼Œä»…ä½¿ç”¨å›¾åƒæ•°æ®ï¼ˆæ— éœ€å¯¹åº”æ–‡æœ¬ï¼‰ã€‚
 
   - (4):DIVAåœ¨MMVP-VLMåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†CLIPçš„æ€§èƒ½ï¼ˆä¾‹å¦‚ï¼Œæé«˜äº†3-7%ï¼‰ï¼Œå¹¶åœ¨å¤šæ¨¡æ€ç†è§£å’Œåˆ†å‰²ä»»åŠ¡ä¸­å¢å¼ºäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å’Œè§†è§‰æ¨¡å‹çš„è¡¨ç°ã€‚åœ¨29ä¸ªå›¾åƒåˆ†ç±»å’Œæ£€ç´¢åŸºå‡†æµ‹è¯•ä¸­ï¼ŒDIVAæ¡†æ¶ä¿æŒäº†CLIPçš„å¼ºé›¶æ ·æœ¬èƒ½åŠ›ï¼Œæ”¯æŒäº†ç ”ç©¶ç›®æ ‡ã€‚
7. Methods:

    - (1): é’ˆå¯¹CLIPï¼ˆContrastive Language-Image Pre-trainingï¼‰æ¨¡å‹åœ¨è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ä¸Šçš„å±€é™æ€§ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„åè®­ç»ƒæ–¹æ³•ï¼Œç§°ä¸ºDIVAï¼ˆDiffusion Feedback for CLIP Visual Assistanceï¼‰ã€‚

    - (2): DIVAæ–¹æ³•åˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„åé¦ˆæ¥ä¼˜åŒ–CLIPçš„è§†è§‰è¡¨ç¤ºï¼Œè¿™ä¸€è¿‡ç¨‹ä»…ä½¿ç”¨å›¾åƒæ•°æ®ï¼Œæ— éœ€å¯¹åº”æ–‡æœ¬ã€‚

    - (3): åœ¨DIVAä¸­ï¼Œç ”ç©¶äººå‘˜æ¢ç´¢äº†ä¸åŒç±»å‹çš„æ‰©æ•£æ¨¡å‹å¯¹ç”ŸæˆæŒ‡å¯¼çš„å½±å“ï¼ŒåŒ…æ‹¬DiT (Peebles & Xie, 2023) å’Œç¨³å®šæ‰©æ•£ï¼ˆSDï¼‰ç³»åˆ—ï¼ˆRombach et al., 2022aï¼‰ï¼Œå¦‚1-4, 2-1-base, xl-base-1.0ç­‰ã€‚

    - (4): é€šè¿‡å®éªŒï¼Œæ–‡ç« å‘ç°å°†SD-2-1-baseæ¨¡å‹é›†æˆåˆ°DIVAä¸­å¯ä»¥è·å¾—æœ€å¤§çš„æ€§èƒ½æå‡ï¼ˆå³æå‡6.6%ï¼‰ï¼ŒåŒæ—¶è§‚å¯Ÿåˆ°å°†DiT-XL/2ä½œä¸ºç”ŸæˆæŒ‡å¯¼å¯ä»¥å¢å¼ºåŸå§‹CLIPæ¨¡å‹åœ¨æ•æ‰è§†è§‰ç»†èŠ‚æ–¹é¢çš„èƒ½åŠ›ã€‚

    - (5): å¯¹äºåŒ…å«çš„SDç³»åˆ—ï¼Œå®éªŒç»“æœä¹Ÿè¡¨æ˜DIVAå¯¹SDæ¨¡å‹ç‰ˆæœ¬ä¸æ•æ„Ÿï¼Œå¯ä»¥åœ¨æ¡†æ¶å†…ä¸€è‡´åœ°ä¼˜åŒ–CLIPçš„ç‰¹å¾è¡¨ç¤ºã€‚


8. Conclusion:

                    - (1):è¯¥ç ”ç©¶å·¥ä½œçš„æ„ä¹‰åœ¨äºï¼Œé¦–æ¬¡æ¢ç´¢äº†åˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„åé¦ˆæ¥ç›´æ¥ä¼˜åŒ–CLIPæ¨¡å‹çš„è¡¨ç¤ºï¼Œæœ‰æ•ˆæå‡äº†CLIPæ¨¡å‹åœ¨è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ä¸Šçš„è¡¨ç°ã€‚

                    - (2):Innovation point: åˆ›æ–°ç‚¹ï¼šæå‡ºäº†åŸºäºæ‰©æ•£æ¨¡å‹çš„CLIPè§†è§‰è¾…åŠ©æ–¹æ³•ï¼ˆDIVAï¼‰ï¼Œé€šè¿‡è‡ªç›‘ç£æ¡†æ¶ä¼˜åŒ–CLIPæ¨¡å‹ï¼Œæ— éœ€é¢å¤–æ’ä»¶å³å¯æ˜¾è‘—æå‡æ€§èƒ½ï¼›Performance: æ€§èƒ½ï¼šåœ¨MMVP-VLMåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDIVAæ˜¾è‘—æå‡äº†CLIPçš„æ€§èƒ½ï¼ˆä¾‹å¦‚ï¼Œæé«˜äº†3-7%ï¼‰ï¼Œå¹¶åœ¨å¤šæ¨¡æ€ç†è§£å’Œåˆ†å‰²ä»»åŠ¡ä¸­å¢å¼ºäº†MLLMså’Œè§†è§‰æ¨¡å‹çš„è¡¨ç°ï¼›Workload: å·¥ä½œé‡ï¼šDIVAæ¡†æ¶ç®€å•ï¼Œæ˜“äºå®ç°ï¼Œå¯¹æ‰©æ•£æ¨¡å‹ç‰ˆæœ¬ä¸æ•æ„Ÿï¼Œå¯é€‚åº”ä¸åŒè§„æ¨¡çš„æ•°æ®å’Œæ¨¡å‹ã€‚




<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cb5e9e180a00e460179ae990c404a9a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3bd68dbbf5a12a666387be59a8f54a18.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a9c8a6d46da721c9808a13c7ed436c90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0a777c754cc038dbe2638dc95475da6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70896c70bed6e8bf460f59557c3bb12c.jpg" align="middle">
</details>




## DiffX: Guide Your Layout to Cross-Modal Generative Modeling

**Authors:Zeyu Wang, Jingyu Lin, Yifei Qian, Yi Huang, Shicen Tian, Bosong Chai, Juncan Deng, Qu Yang, Lan Du, Cunjian Chen, Yufei Guo, Kejie Huang**

Diffusion models have made significant strides in language-driven and layout-driven image generation. However, most diffusion models are limited to visible RGB image generation. In fact, human perception of the world is enriched by diverse viewpoints, such as chromatic contrast, thermal illumination, and depth information. In this paper, we introduce a novel diffusion model for general layout-guided cross-modal generation, called DiffX. Notably, our DiffX presents a simple yet effective cross-modal generative modeling pipeline, which conducts diffusion and denoising processes in the modality-shared latent space. Moreover, we introduce the Joint-Modality Embedder (JME) to enhance the interaction between layout and text conditions by incorporating a gated attention mechanism. To facilitate the user-instructed training, we construct the cross-modal image datasets with detailed text captions by the Large-Multimodal Model (LMM) and our human-in-the-loop refinement. Through extensive experiments, our DiffX demonstrates robustness in cross-modal ''RGB+X'' image generation on FLIR, MFNet, and COME15K datasets, guided by various layout conditions. It also shows the potential for the adaptive generation of ''RGB+X+Y(+Z)'' images or more diverse modalities on COME15K and MCXFace datasets. Our code and constructed cross-modal image datasets are available at https://github.com/zeyuwang-zju/DiffX. 

[PDF](http://arxiv.org/abs/2407.15488v4) 

**Summary**
æå‡ºDiffXæ¨¡å‹ï¼Œå®ç°è·¨æ¨¡æ€å¸ƒå±€å¼•å¯¼å›¾åƒç”Ÿæˆï¼Œæ‹“å±•äººç±»æ„ŸçŸ¥è§†è§’ã€‚

**Key Takeaways**
1. Diffusionæ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—è¿›å±•ï¼Œä½†å¤šé™äºRGBå›¾åƒã€‚
2. DiffXæ¨¡å‹é€‚ç”¨äºé€šç”¨å¸ƒå±€å¼•å¯¼çš„è·¨æ¨¡æ€å›¾åƒç”Ÿæˆã€‚
3. DiffXåœ¨æ¨¡æ€å…±äº«çš„æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ‰©æ•£å’Œå»å™ªå¤„ç†ã€‚
4. å¼•å…¥Joint-Modality Embedder (JME)å¢å¼ºå¸ƒå±€ä¸æ–‡æœ¬æ¡ä»¶çš„äº¤äº’ã€‚
5. åˆ©ç”¨LMMå’Œäººå·¥ä¼˜åŒ–æ„å»ºè¯¦ç»†æ–‡æœ¬æè¿°çš„è·¨æ¨¡æ€å›¾åƒæ•°æ®é›†ã€‚
6. DiffXåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå±•ç¤ºäº†åœ¨RGB+Xå›¾åƒç”Ÿæˆçš„é²æ£’æ€§ã€‚
7. DiffXæœ‰æ½œåŠ›åœ¨COME15Kå’ŒMCXFaceæ•°æ®é›†ä¸Šç”Ÿæˆæ›´å¤šæ¨¡æ€çš„å›¾åƒã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**



1. Title: DiffX: Guide Your Layout to Cross-Modal Generative Modeling
                 (ä¸­æ–‡ç¿»è¯‘ï¼šDiffXï¼šå¼•å¯¼å¸ƒå±€å®ç°è·¨æ¨¡æ€ç”Ÿæˆå»ºæ¨¡)

2. Authors: Zeyu Wang, Jingyu Lin, Yifei Qian, Yi Huang, Shicen Tian, Bosong Chai, Juncan Deng, Qu Yang, Lan Du, Cunjian Chen, Yufei Guo, Kejie Huang

3. Affiliation: xxx
                 (æµ™æ±Ÿå¤§å­¦)

4. Keywords: Diffusion models, Cross-modal generation, Layout guidance, Generative models, Image generation

5. Urls: https://arxiv.org/abs/2407.15488v4 or https://github.com/zeyuwang-zju/DiffX

6. Summary:

                    - (1):è¯¥æ–‡ç« çš„ç ”ç©¶èƒŒæ™¯æ˜¯äººç±»æ„ŸçŸ¥ä¸–ç•Œçš„ä¸°å¯Œå¤šæ ·æ€§ï¼ŒåŒ…æ‹¬å¯è§å…‰ä»¥å¤–çš„å¤šç§æ¨¡æ€ï¼Œå¦‚çƒ­æˆåƒå’Œæ·±åº¦ä¿¡æ¯ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è·¨æ¨¡æ€ç”Ÿæˆæ¨¡å‹ä¸»è¦å±€é™äºå¯è§å…‰RGBå›¾åƒç”Ÿæˆï¼Œå¯¼è‡´è·¨æ¨¡æ€æ•°æ®å¢å¼ºå›°éš¾ã€‚

                    - (2)ï¼šè¿‡å»çš„è·¨æ¨¡æ€ç”Ÿæˆæ–¹æ³•ä¸»è¦åŸºäºæ·±åº¦ç”Ÿæˆæ¨¡å‹ï¼Œå¦‚å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ï¼Œä»¥åŠå¸ƒå±€åˆ°å›¾åƒæ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œä¾‹å¦‚åªèƒ½åˆ†åˆ«ç”ŸæˆRGBå’ŒXå›¾åƒï¼Œå¯¼è‡´å›¾åƒå¯¹é”™ä½å’Œä¸ä¸€è‡´ã€‚

                    - (3)ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDiffXçš„æ–°å‹æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºè·¨æ¨¡æ€å›¾åƒç”Ÿæˆã€‚DiffXæ¨¡å‹åœ¨æ¨¡æ€å…±äº«çš„æ½œåœ¨ç©ºé—´ä¸­æ‰§è¡Œæ‰©æ•£å’Œå»å™ªè¿‡ç¨‹ï¼Œå¹¶å¼•å…¥äº†è”åˆæ¨¡æ€åµŒå…¥å™¨ï¼ˆJMEï¼‰æ¥å¢å¼ºå¸ƒå±€å’Œæ–‡æœ¬æ¡ä»¶ä¹‹é—´çš„äº¤äº’ã€‚

                    - (4)ï¼šDiffXåœ¨FLIRã€MFNetå’ŒCOME15Kæ•°æ®é›†ä¸Šå±•ç¤ºäº†åœ¨è·¨æ¨¡æ€â€œRGB+Xâ€å›¾åƒç”Ÿæˆæ–¹é¢çš„é²æ£’æ€§ï¼Œå¹¶é€šè¿‡å„ç§å¸ƒå±€æ¡ä»¶è¿›è¡Œå¼•å¯¼ã€‚åœ¨COME15Kå’ŒMCXFaceæ•°æ®é›†ä¸Šï¼ŒDiffXè¿˜å±•ç¤ºäº†è‡ªé€‚åº”ç”Ÿæˆâ€œRGB+X+Y(+Z)â€å›¾åƒæˆ–æ›´å¤šæ¨¡æ€çš„æ½œåŠ›ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†è¯¥æ¨¡å‹çš„ç›®æ ‡ã€‚


8. Conclusion:

    - (1):è¿™ç¯‡å·¥ä½œçš„æ„ä¹‰åœ¨äºï¼ŒDiffXæ¨¡å‹é€šè¿‡å¼•å…¥æ–°é¢–çš„æ‰©æ•£æ¨¡å‹ç»“æ„ï¼Œåœ¨è·¨æ¨¡æ€å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå°¤å…¶å¯¹äºâ€œRGB+Xâ€ä»¥åŠæ›´å¤šæ¨¡æ€çš„å›¾åƒç”Ÿæˆï¼Œä¸ºè·¨æ¨¡æ€æ•°æ®å¢å¼ºå’Œå›¾åƒç”Ÿæˆä»»åŠ¡æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚

    - (2):Innovation point: DiffXæ¨¡å‹åœ¨è·¨æ¨¡æ€ç”Ÿæˆå»ºæ¨¡ä¸­æå‡ºäº†æ–°çš„æ‰©æ•£æµç¨‹ï¼Œå®ç°äº†å¤šæ¨¡æ€å…±äº«æ½œåœ¨ç©ºé—´ä¸­çš„ç‹¬ç«‹æ¨¡æ€æ‰©æ•£å’Œå»å™ªï¼Œå…·æœ‰åˆ›æ–°æ€§ï¼›Performance: åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šï¼ŒDiffXåœ¨â€œRGB+Xâ€å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸”å…·æœ‰ç”Ÿæˆæ›´å¤šæ¨¡æ€å›¾åƒçš„æ½œåŠ›ï¼›Workload: æ¨¡å‹çš„è®­ç»ƒå’Œè¿è¡Œç›¸å¯¹å¤æ‚ï¼Œéœ€è¦è¾ƒé«˜çš„è®¡ç®—èµ„æºã€‚




<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7f526f28c641ec7c1c62f8b57dd59db9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7c0b3dd4d585e5d43aaa56d901ff3a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-858f2481cda0694f7f0dfa694f5b677e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-05b13fd052c5c2e4a0a54f98c99b06cf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-822dc2bc7d336c2d1c95e1494c86577c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06be54a30b51ba2577c3113e790f7c4d.jpg" align="middle">
</details>




