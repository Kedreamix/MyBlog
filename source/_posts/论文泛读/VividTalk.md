# VividTalk: One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior



Paper   : [https://arxiv.org/pdf/2312.01841.pdf](https://arxiv.org/pdf/2312.01841.pdf)

Project : [https://humanaigc.github.io/vivid-talk/](https://humanaigc.github.io/vivid-talk/)

Video   : [https://www.youtube.com/watch?v=lJVzt7JCe_4](https://www.youtube.com/watch?v=lJVzt7JCe_4)

Code    : [https://github.com/HumanAIGC/VividTalk](https://github.com/HumanAIGC/VividTalk)  (Maybe Comming Soon)



**摘要**

创新的两阶段框架 VividTalk 可生成高质量视觉效果的说话人头部视频，包括唇形同步、丰富的面部表情、自然的头部姿势等。

（1）：音频驱动的说话头生成已经引起广泛关注，在唇形同步、面部表情、头部姿势生成和视频质量方面取得了进展。然而，由于音频和动作之间的一对多映射，还没有模型能够在所有这些指标上达到最优。
（2）：以往的方法通常使用混合形状或顶点偏移来表示面部表情，但这些方法在捕捉精细的表情细节方面存在局限性。此外，头部姿势的生成通常是通过直接从音频中学习来实现的，这可能会导致不合理和不连续的结果。
（3）：本文提出了一种名为 VividTalk 的两阶段通用框架，支持生成具有所有上述属性的高视觉质量说话头视频。在第一阶段，通过学习非刚性表情运动和刚性头部运动将音频映射到网格。对于表情运动，采用混合形状和顶点作为中间表示，以最大限度地提高模型的表示能力。对于自然头部运动，提出了一种新颖的可学习头部姿势码本，并采用两阶段训练机制。在第二阶段，提出了一种双分支运动-VAE 和生成器，将网格转换为密集运动并逐帧合成高质量视频。
（4）：广泛的实验表明，所提出的 VividTalk 可以生成具有唇形同步和逼真头部姿势的高视觉质量说话头视频，并且在客观和主观比较中优于以往的最新作品。

**要点**

- VividTalk 采用双阶段通用框架，可以生成高质量视觉效果的说话人头部视频。
- VividTalk 在第一阶段通过学习非刚性表情运动和刚性头部运动，将音频映射到网格。
- VividTalk 在第二阶段使用双分支运动-VAE 和生成器将网格转换为密集运动并逐帧合成高质量视频。
- 广泛的实验表明，与目前最先进的作品相比，VividTalk 可以生成高质量视觉效果的说话人头部视频，并将唇形同步和逼真的增强效果提高很大幅度。



**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>标题：生动语聊：高保真音视频生成框架（VividTalk: A High-Fidelity Audio-Driven Talking Head Generation Framework）</li>
<li>作者：Yuhang Jiang, Mingyu Ding, Junhui Hou, Yanan Sun, Lu Sheng, Zhiwei Xiong, Hang Zhou</li>
<li>单位：无</li>
<li>关键词：音频驱动、说话头生成、面部表情、头部姿势、视频合成</li>
<li>链接：https://arxiv.org/abs/2312.01841, Github：无</li>
<li>
<p>摘要：
（1）：音频驱动的说话头生成已经引起广泛关注，在唇形同步、面部表情、头部姿势生成和视频质量方面取得了进展。然而，由于音频和动作之间的一对多映射，还没有模型能够在所有这些指标上达到最优。
（2）：以往的方法通常使用混合形状或顶点偏移来表示面部表情，但这些方法在捕捉精细的表情细节方面存在局限性。此外，头部姿势的生成通常是通过直接从音频中学习来实现的，这可能会导致不合理和不连续的结果。
（3）：本文提出了一种名为 VividTalk 的两阶段通用框架，支持生成具有所有上述属性的高视觉质量说话头视频。在第一阶段，通过学习非刚性表情运动和刚性头部运动将音频映射到网格。对于表情运动，采用混合形状和顶点作为中间表示，以最大限度地提高模型的表示能力。对于自然头部运动，提出了一种新颖的可学习头部姿势码本，并采用两阶段训练机制。在第二阶段，提出了一种双分支运动-VAE 和生成器，将网格转换为密集运动并逐帧合成高质量视频。
（4）：广泛的实验表明，所提出的 VividTalk 可以生成具有唇形同步和逼真头部姿势的高视觉质量说话头视频，并且在客观和主观比较中优于以往的最新作品。</p>
</li>
<li>
<p>方法：
（1）：VividTalk 框架分为两个阶段：网格生成阶段和视频合成阶段。在网格生成阶段，音频首先被映射到网格，网格由混合形状和顶点偏移表示。混合形状用于捕捉精细的表情细节，而顶点偏移用于捕捉刚性头部运动。在视频合成阶段，网格被转换为密集运动，然后逐帧合成高质量视频。
（2）：在网格生成阶段，音频首先被映射到一个中间表示，该中间表示由混合形状和顶点偏移组成。混合形状用于捕捉精细的表情细节，而顶点偏移用于捕捉刚性头部运动。然后，中间表示被映射到网格。
（3）：在视频合成阶段，网格被转换为密集运动。密集运动然后被用于逐帧合成高质量视频。视频合成器是一个双分支网络，由一个运动-VAE 和一个生成器组成。运动-VAE 用于生成密集运动，而生成器用于合成视频。</p>
</li>
<li>
<p>结论：
（1）：本工作首次提出了一种支持生成具有丰富面部表情和自然头部姿势的高质量说话头视频的新颖通用框架 VividTalk。对于非刚性表情运动，混合形状和顶点都被映射为中间表示以最大化模型的表示能力，并设计了一个精心构建的多分支生成器来分别对全局和局部面部运动进行建模。至于刚性头部运动，提出了一种新颖的可学习头部姿势码本和一种两阶段训练机制来合成自然结果。得益于双分支运动-VAE 和生成器，驱动的网格可以很好地转换为密集运动，并用于合成最终视频。实验表明，我们的方法优于以往最先进的方法，并在数字人创建、视频会议等许多应用中开辟了新途径。
（2）：创新点：</p>
</li>
<li>提出了一种新颖的通用框架 VividTalk，支持生成具有丰富面部表情和自然头部姿势的高质量说话头视频。</li>
<li>对于非刚性表情运动，混合形状和顶点都被映射为中间表示以最大化模型的表示能力，并设计了一个精心构建的多分支生成器来分别对全局和局部面部运动进行建模。</li>
<li>对于刚性头部运动，提出了一种新颖的可学习头部姿势码本和一种两阶段训练机制来合成自然结果。</li>
<li>得益于双分支运动-VAE 和生成器，驱动的网格可以很好地转换为密集运动，并用于合成最终视频。
性能：</li>
<li>在客观和主观比较中，VividTalk 优于以往最先进的作品。</li>
<li>VividTalk 可以生成具有唇形同步和逼真头部姿势的高视觉质量说话头视频。
工作量：</li>
<li>VividTalk 的实现相对复杂，需要大量的数据和计算资源。</li>
<li>VividTalk 的训练过程可能需要数天或数周的时间。</li>
</ol>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b072ca131954e5aa54fae54f90858dae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ef1751be1b0bb02f7a73562aad64e5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18bcd1380728d32e1277fd17982288c6.jpg" align="middle">
</details>


