
---
title: 元宇宙/虚拟人
date: 2024-10-07 19:15:31
author: Kedreamix
cover: 
categories: Paper
tags:
    - 元宇宙/虚拟人
description: 元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2024-10-07  EgoAvatar Egocentric View-Driven and Photorealistic Full-body Avatars  
keywords: 元宇宙/虚拟人
toc:
toc_number: false
toc_style_simple: true
copyright:
copyright_author:
copyright_author_href:
copyright_url:
copyright_info:
mathjax: true
katex:
aplayer:
highlight_shrink:
aside:
---

>⚠️ 以下所有内容总结都来自于 Google的大语言模型[Gemini-Pro](https://ai.google.dev/)的能力，如有错误，仅供参考，谨慎使用
>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！
>💗 如果您觉得我们的项目对您有帮助 [ChatPaperFree](https://github.com/Kedreamix/ChatPaperFree) ，还请您给我们一些鼓励！⭐️ [HuggingFace免费体验](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)

# 2024-10-07 更新


## EgoAvatar: Egocentric View-Driven and Photorealistic Full-body Avatars

**Authors:Jianchun Chen, Jian Wang, Yinda Zhang, Rohit Pandey, Thabo Beeler, Marc Habermann, Christian Theobalt**

Immersive VR telepresence ideally means being able to interact and communicate with digital avatars that are indistinguishable from and precisely reflect the behaviour of their real counterparts. The core technical challenge is two fold: Creating a digital double that faithfully reflects the real human and tracking the real human solely from egocentric sensing devices that are lightweight and have a low energy consumption, e.g. a single RGB camera. Up to date, no unified solution to this problem exists as recent works solely focus on egocentric motion capture, only model the head, or build avatars from multi-view captures. In this work, we, for the first time in literature, propose a person-specific egocentric telepresence approach, which jointly models the photoreal digital avatar while also driving it from a single egocentric video. We first present a character model that is animatible, i.e. can be solely driven by skeletal motion, while being capable of modeling geometry and appearance. Then, we introduce a personalized egocentric motion capture component, which recovers full-body motion from an egocentric video. Finally, we apply the recovered pose to our character model and perform a test-time mesh refinement such that the geometry faithfully projects onto the egocentric view. To validate our design choices, we propose a new and challenging benchmark, which provides paired egocentric and dense multi-view videos of real humans performing various motions. Our experiments demonstrate a clear step towards egocentric and photoreal telepresence as our method outperforms baselines as well as competing methods. For more details, code, and data, we refer to our project page. 

[PDF](http://arxiv.org/abs/2410.01835v1) 

**Summary**
该研究首次提出一种个性化自视角远程呈现方法，通过单一自视角视频同时建模和驱动逼真的数字虚拟人。

**Key Takeaways**
1. 研究聚焦于创建与真人行为一致的数字孪生虚拟人。
2. 技术挑战包括创建精确的数字双胞胎和轻量级低能耗的跟踪设备。
3. 目前缺乏统一解决方案，现有研究主要关注头部捕捉或多视角捕捉。
4. 首次提出基于个性化自视角的远程呈现方法。
5. 提出可由骨骼动作驱动的动画模型，同时建模几何和外观。
6. 引入个性化自视角动作捕捉组件，从单视角视频恢复全身运动。
7. 通过测试时网格细化，确保几何形状忠实映射到自视角视图。
8. 提出新的基准，提供配对的自视角和密集多视角视频进行验证。
9. 方法优于基线及竞争方法，向自视角和逼真远程呈现迈进。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**


7. 方法论：

(1) 研究提出了一个基于自我视角的视频驱动虚拟形象生成方法。该方法旨在通过自我视角的视频输入来驱动虚拟角色的动作和表情。

(2) 方法首先进行姿态预测的个人化调整（Personalization of Pose Prediction）。通过对特定个体的数据进行微调，提高测试时的准确性。

(3) 接着，研究引入了IKSolver中的正则化项EReg。该正则化项使用平均运动¯𝜽作为简单的运动先验，有效提高了运动跟踪的准确性。

(4) 为了处理复杂的服装变形，研究引入了MotionDeformer和EgoDeformer两个模块。这两个模块能够预测合理的服装动画结果，甚至在具有挑战性的身体移动下也能保持效果。

(5) 研究还进行了鲁棒性测试，验证了该方法在不同照明条件下的有效性。在户外场景中，尽管照明条件与训练数据差异显著，但估计的姿态和渲染质量仍然表现良好。

以上即为该研究的核心方法论思路。
8. Conclusion:

(1) 工作意义：该研究提出了一种基于自我视角的视频驱动虚拟形象生成方法，具有重要的应用价值。该方法能够实时生成逼真的全身虚拟形象，为远程沉浸体验、虚拟现实和增强现实等领域的应用提供了强有力的支持，如在线教学、电影制作和游戏等。

(2) 优缺点分析：

     - 创新点：该研究在创新点方面表现出色。它引入了一种个性化的姿态预测调整方法，提高了测试准确性。此外，研究还引入了正则化项EReg、MotionDeformer和EgoDeformer等模块，有效提高了运动跟踪的准确性和处理复杂服装变形的能力。
     
     - 性能：该文章在性能方面表现良好。研究验证了该方法在不同照明条件下的有效性，并且在户外场景中，即使照明条件与训练数据差异显著，估计的姿态和渲染质量仍然表现良好。
     
     - 工作量：从文章描述来看，该研究工作量大，涉及多个模块的设计和实现，以及大量的实验验证和性能测试。

以上是对该文章的总结和分析，希望对您有所帮助。


<details>
  <summary>点此查看论文截图</summary>
<img src="http://article.biliimg.com/bfs/new_dyn/6654ccfa71018884181f857eea6a0629241286257.jpg" align="middle">
<img src="http://article.biliimg.com/bfs/new_dyn/2aca4cd5df0d0b4b13be2e77ac909391241286257.jpg" align="middle">
<img src="http://article.biliimg.com/bfs/new_dyn/3c6f0e51cfd9c66af35abd86a7aa2fba241286257.jpg" align="middle">
</details>




## Towards Native Generative Model for 3D Head Avatar

**Authors:Yiyu Zhuang, Yuxiao He, Jiawei Zhang, Yanwen Wang, Jiahe Zhu, Yao Yao, Siyu Zhu, Xun Cao, Hao Zhu**

Creating 3D head avatars is a significant yet challenging task for many applicated scenarios. Previous studies have set out to learn 3D human head generative models using massive 2D image data. Although these models are highly generalizable for human appearance, their result models are not 360$^\circ$-renderable, and the predicted 3D geometry is unreliable. Therefore, such results cannot be used in VR, game modeling, and other scenarios that require 360$^\circ$-renderable 3D head models. An intuitive idea is that 3D head models with limited amount but high 3D accuracy are more reliable training data for a high-quality 3D generative model. In this vein, we delve into how to learn a native generative model for 360$^\circ$ full head from a limited 3D head dataset. Specifically, three major problems are studied: 1) how to effectively utilize various representations for generating the 360$^\circ$-renderable human head; 2) how to disentangle the appearance, shape, and motion of human faces to generate a 3D head model that can be edited by appearance and driven by motion; 3) and how to extend the generalization capability of the generative model to support downstream tasks. Comprehensive experiments are conducted to verify the effectiveness of the proposed model. We hope the proposed models and artist-designed dataset can inspire future research on learning native generative 3D head models from limited 3D datasets. 

[PDF](http://arxiv.org/abs/2410.01226v1) 

**Summary**
3D头像生成模型研究：从有限3D数据集学习360度全头像生成。

**Key Takeaways**
1. 2D图像数据生成的3D头像模型难以360度渲染。
2. 高精度3D模型是高质量生成模型更可靠的训练数据。
3. 研究如何从有限3D数据集学习360度全头像生成模型。
4. 解决如何有效生成360度可渲染的人头问题。
5. 如何分离人脸的外观、形状和运动，以编辑3D头像模型。
6. 如何扩展生成模型的泛化能力以支持下游任务。
7. 提出的模型和艺术家设计的数据集有望激发未来研究。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

1. Title: 面向有限数据集的三维头部生成模型研究（Towards Native Generative Model for 3D Head Avatar）

2. Authors: Yiyu Zhuang, Yuxiao He, Jiawei Zhang, Yanwen Wang, Jiahe Zhu, Yao Yao, Siyu Zhu, Xun Cao, Hao Zhu等。

3. Affiliation: 南京大学教授（Professor from Nanjing University）。

4. Keywords: 三维头部模型，生成模型，图像拟合，文本编辑，面部动画等（3D head model, generative model, image-based fitting, text-based editing, facial animation）。

5. Urls: 文章链接（具体链接需根据实际情况填写），GitHub代码链接（如果有的话，否则填写GitHub:None）。

6. Summary: 

    - (1) 研究背景：本文研究了在有限的三维头部数据集上学习原生生成模型的问题。随着计算机视觉和计算机图形学的发展，创建三维头部模型在许多领域都有广泛应用，如电影制作、数字化身等。然而，传统的三维头部建模方法成本高且需要大量手动处理，因此寻求一种经济有效的建模方法成为了一个重要的研究方向。本文提出了一种面向三维头部模型的原生生成模型学习方法。
    
    - (2) 过去的方法及问题：以往的方法主要分为基于二维图像的方法和基于三维数据的方法。虽然基于二维图像的方法具有良好的泛化能力，但它们生成的模型无法做到全方位的渲染，预测的三维几何结构也不可靠。因此，这些方法无法应用于需要全方位渲染的三维头部模型的场景，如虚拟现实、游戏建模等。针对这一问题，本文提出了一种新的方法来解决在有限的三维头部数据集上学习高质量的三维生成模型的问题。
    
    - (3) 研究方法：本文提出了一个面向全方位的三维头部模型的生成模型学习方法。主要研究了三个关键问题：一是如何利用各种表示法生成全方位可渲染的三维头部；二是如何解耦人脸的外观、形状和运动以生成能够被编辑和驱动的模型；三是如何扩展生成模型的泛化能力以支持下游任务。本文设计了一种新型的模型结构和方法来解决这些问题。
    
    - (4) 任务与性能：本文的实验验证了所提出模型的有效性。所提出的方法和艺术家设计的数据集为从有限的三维数据集学习原生生成三维头部模型的研究提供了灵感。实验结果表明，该方法能够在有限的训练数据下生成具有真实感和全方位渲染能力的三维头部模型，同时具有良好的泛化性能和应用效果。实验结果支持文章的目标，即通过引入一种新的三维头部生成模型方法，实现更高效和经济的人脸建模。
7. 方法论概述：

本文提出了一种面向三维头部模型的原生生成模型学习方法，主要包括以下几个步骤：

    - (1) 研究背景与问题定义：文章首先介绍了研究背景，指出了传统三维头部建模方法的高成本和大手动处理需求，从而提出研究问题，即在有限的三维头部数据集上学习高质量的三维生成模型。
    
    - (2) 数据集与模型构建：文章使用了特定数据集进行研究，并设计了一种新型的模型结构来解决所提出的问题。该模型结构考虑了如何利用各种表示法生成全方位可渲染的三维头部、如何解耦人脸的外观、形状和运动以生成能够被编辑和驱动的模型以及如何扩展生成模型的泛化能力以支持下游任务等关键问题。
    
    - (3) 实验设计与实现：文章通过实验验证了所提出模型的有效性。实验中，采用了多种评估指标（如PSNR、SSIM、LPIPS等）来定量评估生成结果的质量。同时，文章还介绍了模型的拟合方法，包括基于混合方法、基于点的方法和优化过程等。这些方法考虑了如何适应不同数据集、如何处理目标图像与合成数据之间的差异等问题。
    
    - (4) 动画与评估：生成的或拟合的头部可以通过标准52面部blendshapes进行直接动画处理。文章还通过定量评估结果表（如表格IV和V）展示了不同方法的性能差异。这些评估结果证明了所提出方法在生成具有真实感和全方位渲染能力的三维头部模型方面的有效性。

整体而言，本文提出了一种面向有限数据集的三维头部生成模型学习方法，通过设计新型模型结构和实验方法，实现了高效和经济的人脸建模。
8. Conclusion:

- (1) 这项工作的意义在于提出了一种面向有限数据集的三维头部生成模型学习方法，解决了传统三维头部建模方法成本高、需要大量手动处理的问题，为电影制作、数字化身等领域提供了一种高效且经济的建模方法。
- (2) 创新点：本文的创新点在于提出了一种新型的三维头部生成模型学习方法，解决了在有限的三维头部数据集上学习高质量的三维生成模型的问题，并设计了针对全方位三维头部模型的生成模型学习方法，解决了如何利用各种表示法生成全方位可渲染的三维头部、如何解耦人脸的外观、形状和运动以及如何扩展生成模型的泛化能力等关键问题。
- 性能：该文章所提出的方法和设计师的数据集实验验证了模型的有效性，能够生成具有真实感和全方位渲染能力的三维头部模型，并具有良好的泛化性能和应用效果。
- 工作量：文章进行了大量的实验和模型设计，包括数据集的构建、模型结构的设计、实验方法的探索等，工作量较大。

总体来说，这篇文章提出了一种面向有限数据集的三维头部生成模型学习方法，通过设计新型模型结构和实验方法，实现了高效和经济的人脸建模，对于相关领域的研究和应用具有一定的推动作用。


<details>
  <summary>点此查看论文截图</summary>
<img src="http://article.biliimg.com/bfs/new_dyn/2b71abe1df110df40ccd43476cc4a065241286257.jpg" align="middle">
<img src="http://article.biliimg.com/bfs/new_dyn/3ac3c264593382fa80ebf9db5cf1ec99241286257.jpg" align="middle">
<img src="http://article.biliimg.com/bfs/new_dyn/f8bde6c79530a404dbb2eaaa2de76cea241286257.jpg" align="middle">
</details>




## Subjective and Objective Quality Assessment of Rendered Human Avatar   Videos in Virtual Reality

**Authors:Yu-Chih Chen, Avinab Saha, Alexandre Chapiro, Christian Häne, Jean-Charles Bazin, Bo Qiu, Stefano Zanetti, Ioannis Katsavounidis, Alan C. Bovik**

We study the visual quality judgments of human subjects on digital human avatars (sometimes referred to as "holograms" in the parlance of virtual reality [VR] and augmented reality [AR] systems) that have been subjected to distortions. We also study the ability of video quality models to predict human judgments. As streaming human avatar videos in VR or AR become increasingly common, the need for more advanced human avatar video compression protocols will be required to address the tradeoffs between faithfully transmitting high-quality visual representations while adjusting to changeable bandwidth scenarios. During transmission over the internet, the perceived quality of compressed human avatar videos can be severely impaired by visual artifacts. To optimize trade-offs between perceptual quality and data volume in practical workflows, video quality assessment (VQA) models are essential tools. However, there are very few VQA algorithms developed specifically to analyze human body avatar videos, due, at least in part, to the dearth of appropriate and comprehensive datasets of adequate size. Towards filling this gap, we introduce the LIVE-Meta Rendered Human Avatar VQA Database, which contains 720 human avatar videos processed using 20 different combinations of encoding parameters, labeled by corresponding human perceptual quality judgments that were collected in six degrees of freedom VR headsets. To demonstrate the usefulness of this new and unique video resource, we use it to study and compare the performances of a variety of state-of-the-art Full Reference and No Reference video quality prediction models, including a new model called HoloQA. As a service to the research community, we publicly releases the metadata of the new database at https://live.ece.utexas.edu/research/LIVE-Meta-rendered-human-avatar/index.html. 

[PDF](http://arxiv.org/abs/2408.07041v2) Accepted to IEEE Transactions on Image Processing, 2024

**Summary**
研究人类对扭曲后的数字人偶视觉质量判断，并评估视频质量模型预测人类判断的能力。

**Key Takeaways**
1. 研究对象为VR/AR系统中的数字人偶视觉质量判断。
2. 评估视频质量模型对人类判断的预测能力。
3. 需要更先进的视频压缩协议来平衡高视觉质量和可变带宽。
4. 压缩视频的传输过程中，视觉质量可能受到严重损害。
5. 视频质量评估模型对优化感知质量和数据量至关重要。
6. 缺乏针对人体avatar视频的VQA算法和综合数据集。
7. 引入LIVE-Meta Rendered Human Avatar VQA数据库以填补这一空白。
8. 使用数据库研究并比较了多种视频质量预测模型。
9. 公开发布数据库的元数据，以服务研究社区。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**


8. Conclusion:

(1) 此作品的意义在于xxx（根据文章内容填写具体的意义，如探讨某一文学主题、反映某一社会现象等）。

(2) Innovation point: 本文在创新点方面的表现可概括为xxx（如采用新的文学手法、提出独特的观点等）。然而，也存在一些创新点不够突出或者缺乏新颖性的问题。
Performance: 在性能表现方面，本文展现了xxx（如深入的人物刻画、紧凑的情节安排等）。但同时，可能存在某些方面如语言表达、情节逻辑等方面的不够完美。
Workload: 文章在工作量方面表现出较大的投入，涵盖了xxx方面的内容（如广泛的主题、大量的细节描写等）。然而，也可能存在内容过于繁琐或冗余的情况。

请注意，以上回答中的"xxx"需要根据实际文章内容填写。在总结时，尽量保持客观、中立的立场，避免主观偏见。同时，确保使用学术、简洁的语言表达。


<details>
  <summary>点此查看论文截图</summary>
<img src="http://article.biliimg.com/bfs/new_dyn/52882f3388cc5983b5b6e4c5613ac33f241286257.jpg" align="middle">
<img src="http://article.biliimg.com/bfs/new_dyn/0045bc7437a20cb334c47ffc6e46939b241286257.jpg" align="middle">
<img src="http://article.biliimg.com/bfs/new_dyn/9a5406b6a61a01a0f0a1a56a381e00e6241286257.jpg" align="middle">
<img src="http://article.biliimg.com/bfs/new_dyn/feabb392da3830f75ced1d44fe2521e2241286257.jpg" align="middle">
<img src="http://article.biliimg.com/bfs/new_dyn/b9116cf345385d220f1745bd747f7fcc241286257.jpg" align="middle">
</details>




