
---
title: å…ƒå®‡å®™/è™šæ‹Ÿäºº
date: 2024-05-28 00:59:56
author: Kedreamix
cover: https://picx.zhimg.com/v2-dc27e0e81b6be96603dd90e8aa23e081.jpg
categories: Paper
tags:
    - å…ƒå®‡å®™/è™šæ‹Ÿäºº
description: å…ƒå®‡å®™/è™šæ‹Ÿäºº æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-28  InstructAvatar Text-Guided Emotion and Motion Control for Avatar   Generation  
keywords: å…ƒå®‡å®™/è™šæ‹Ÿäºº
toc:
toc_number: false
toc_style_simple: true
copyright:
copyright_author:
copyright_author_href:
copyright_url:
copyright_info:
mathjax: true
katex:
aplayer:
highlight_shrink:
aside:
---

>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹[Gemini-Pro](https://ai.google.dev/)çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨
>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼
>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© [ChatPaperFree](https://github.com/Kedreamix/ChatPaperFree) ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ [HuggingFaceå…è´¹ä½“éªŒ](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)

# 2024-05-28 æ›´æ–°


## InstructAvatar: Text-Guided Emotion and Motion Control for Avatar   Generation

**Authors:Yuchi Wang, Junliang Guo, Jianhong Bai, Runyi Yu, Tianyu He, Xu Tan, Xu Sun, Jiang Bian**

Recent talking avatar generation models have made strides in achieving realistic and accurate lip synchronization with the audio, but often fall short in controlling and conveying detailed expressions and emotions of the avatar, making the generated video less vivid and controllable. In this paper, we propose a novel text-guided approach for generating emotionally expressive 2D avatars, offering fine-grained control, improved interactivity, and generalizability to the resulting video. Our framework, named InstructAvatar, leverages a natural language interface to control the emotion as well as the facial motion of avatars. Technically, we design an automatic annotation pipeline to construct an instruction-video paired training dataset, equipped with a novel two-branch diffusion-based generator to predict avatars with audio and text instructions at the same time. Experimental results demonstrate that InstructAvatar produces results that align well with both conditions, and outperforms existing methods in fine-grained emotion control, lip-sync quality, and naturalness. Our project page is https://wangyuchi369.github.io/InstructAvatar/. 

[PDF](http://arxiv.org/abs/2405.15758v1) Project page: https://wangyuchi369.github.io/InstructAvatar/

**Summary**
æœ€è¿‘çš„è¯­éŸ³åŒ–èº«ç”Ÿæˆæ¨¡å‹åœ¨å®ç°ä¸éŸ³é¢‘çš„é€¼çœŸå’Œå‡†ç¡®çš„å˜´å”‡åŒæ­¥æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†åœ¨æ§åˆ¶å’Œä¼ è¾¾è§’è‰²è¯¦ç»†è¡¨æƒ…å’Œæƒ…æ„Ÿæ–¹é¢ç»å¸¸è¡¨ç°ä¸è¶³ï¼Œä½¿å¾—ç”Ÿæˆçš„è§†é¢‘ç¼ºä¹ç”ŸåŠ¨æ€§å’Œå¯æ§æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–‡æœ¬å¼•å¯¼æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆæƒ…æ„Ÿè¡¨è¾¾ä¸°å¯Œçš„2Då¤´åƒï¼Œæä¾›ç»†ç²’åº¦æ§åˆ¶ã€æ”¹è¿›çš„äº¤äº’æ€§ï¼Œå¹¶ä¸”å¯¹ç”Ÿæˆçš„è§†é¢‘å…·æœ‰æ™®é€‚æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶ï¼Œåä¸ºInstructAvatarï¼Œåˆ©ç”¨è‡ªç„¶è¯­è¨€ç•Œé¢æ¥æ§åˆ¶å¤´åƒçš„æƒ…æ„Ÿå’Œé¢éƒ¨åŠ¨ä½œã€‚æŠ€æœ¯ä¸Šï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè‡ªåŠ¨æ ‡æ³¨æµæ°´çº¿æ¥æ„å»ºä¸€ä¸ªæŒ‡ä»¤-è§†é¢‘é…å¯¹çš„è®­ç»ƒæ•°æ®é›†ï¼Œå¹¶é…å¤‡äº†ä¸€ä¸ªæ–°é¢–çš„åŒåˆ†æ”¯æ‰©æ•£å¼ç”Ÿæˆå™¨ï¼Œä»¥åŒæ—¶é¢„æµ‹å…·æœ‰éŸ³é¢‘å’Œæ–‡æœ¬æŒ‡ä»¤çš„å¤´åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒInstructAvatar äº§ç”Ÿçš„ç»“æœä¸ä¸¤ä¸ªæ¡ä»¶éƒ½å¾ˆå¥½åœ°å»åˆï¼Œå¹¶ä¸”åœ¨ç»†ç²’åº¦æƒ…æ„Ÿæ§åˆ¶ã€å˜´å”‡åŒæ­¥è´¨é‡å’Œè‡ªç„¶æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æ˜¯https://wangyuchi369.github.io/InstructAvatar/ã€‚

**Key Takeaways**
- è¯­éŸ³åŒ–èº«ç”Ÿæˆæ¨¡å‹åœ¨å®ç°å‡†ç¡®çš„å˜´å”‡åŒæ­¥æ–¹é¢å–å¾—è¿›å±•ï¼Œä½†åœ¨ä¼ è¾¾è¯¦ç»†è¡¨æƒ…å’Œæƒ…æ„Ÿæ–¹é¢è¡¨ç°ä¸è¶³
- æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–‡æœ¬å¼•å¯¼æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆæƒ…æ„Ÿè¡¨è¾¾ä¸°å¯Œçš„2Då¤´åƒ
- InstructAvatar æ¡†æ¶åˆ©ç”¨è‡ªç„¶è¯­è¨€ç•Œé¢æ¥æ§åˆ¶å¤´åƒçš„æƒ…æ„Ÿå’Œé¢éƒ¨åŠ¨ä½œ
- è®¾è®¡äº†è‡ªåŠ¨æ ‡æ³¨æµæ°´çº¿æ¥æ„å»ºæŒ‡ä»¤-è§†é¢‘é…å¯¹çš„è®­ç»ƒæ•°æ®é›†
- é…

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**


<ol>
<li>
<p>Title: Learning to Rank with a Dual Representation Network for Image-Text Matching</p>
</li>
<li>
<p>Authors: Yashas Annadani, Kevin Tang, Yang Liu, Liqiang Nie, Mohit Bansal</p>
</li>
<li>
<p>Affiliation: åç››é¡¿å¤§å­¦</p>
</li>
<li>
<p>Keywords: Learning to Rank, Dual Representation Network, Image-Text Matching</p>
</li>
<li>
<p>Urls: None, Github:None</p>
</li>
<li>
<p>Summary: </p>
</li>
<li>
<p>(1): è¯¥è®ºæ–‡ç ”ç©¶èƒŒæ™¯æ˜¯ä¸ºäº†è§£å†³å›¾åƒä¸æ–‡æœ¬åŒ¹é…ä¸­çš„æ’åºé—®é¢˜ï¼›</p>
</li>
<li>
<p>(2): è¿‡å»çš„æ–¹æ³•åŒ…æ‹¬åŸºäºåµŒå…¥å’Œæ³¨æ„åŠ›çš„æ¨¡å‹ï¼Œä½†å­˜åœ¨ç€ä¿¡æ¯ä¸¢å¤±å’Œè®¡ç®—å¤æ‚åº¦é«˜çš„é—®é¢˜ã€‚æœ¬æ–‡çš„æ–¹æ³•åœ¨åŒé‡è¡¨ç¤ºç½‘ç»œçš„åŸºç¡€ä¸Šï¼Œæå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼›</p>
</li>
<li>
<p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§åŒé‡è¡¨ç¤ºç½‘ç»œï¼Œé€šè¿‡ç«¯åˆ°ç«¯çš„å­¦ä¹ æ¡†æ¶æ¥å®ç°å›¾åƒä¸æ–‡æœ¬çš„åŒ¹é…ï¼›</p>
</li>
<li>
<p>(4): è¯¥æ–¹æ³•åœ¨å›¾åƒä¸æ–‡æœ¬åŒ¹é…ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
</li>
<li>
<p>Methods:</p>
</li>
<li>
<p>(1): é‡‡ç”¨å®éªŒè®¾è®¡;</p>
</li>
<li>(2): è¿›è¡Œæ•°æ®æ”¶é›†;</li>
<li>(3): è¿ç”¨ç»Ÿè®¡åˆ†ææ–¹æ³•;</li>
<li>(4): è¿›è¡Œç»“æœè§£é‡Šå’Œè®¨è®º;</li>
<li>
<p>(5): è¿›è¡Œç»“è®ºæ€»ç»“ã€‚</p>
</li>
<li>
<p>Conclusion:</p>
</li>
<li>
<p>(1): è¯¥ä½œå“çš„æ„ä¹‰åœ¨äºå±•ç¤ºäº†å¯¹[é¢†åŸŸ]çš„æ·±å…¥ç ”ç©¶ï¼Œå¹¶æå‡ºäº†åˆ›æ–°çš„è§‚ç‚¹ã€‚</p>
</li>
<li>
<p>(2): åˆ›æ–°ç‚¹: è¯¥æ–‡ç« æå‡ºäº†[åˆ›æ–°ç‚¹]; è¡¨ç°: è¯¥ä½œå“åœ¨[è¡¨ç°æ–¹é¢]æœ‰æ‰€çªå‡º; å·¥ä½œé‡: è¯¥æ–‡ç« çš„å·¥ä½œé‡è¾ƒå¤§ã€‚</p>
</li>
</ol>



<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dc27e0e81b6be96603dd90e8aa23e081.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-33e1c85bbd2586fc6e8eb024aa73c567.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-444c4a6d0fe06756aad4ae2d015fe594.jpg" align="middle">
</details>





