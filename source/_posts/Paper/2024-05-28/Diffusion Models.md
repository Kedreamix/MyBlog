
---
title: Diffusion Models
date: 2024-05-28 01:19:08
author: Kedreamix
cover: https://pic1.zhimg.com/v2-36a0effe69414b2ffa084f4cd6a69d06.jpg
categories: Paper
tags:
    - Diffusion Models
description: Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-28  DiffCalib Reformulating Monocular Camera Calibration as Diffusion-Based   Dense Incident Map Generation  
keywords: Diffusion Models
toc:
toc_number: false
toc_style_simple: true
copyright:
copyright_author:
copyright_author_href:
copyright_url:
copyright_info:
mathjax: true
katex:
aplayer:
highlight_shrink:
aside:
---

>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹[Gemini-Pro](https://ai.google.dev/)çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨
>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼
>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© [ChatPaperFree](https://github.com/Kedreamix/ChatPaperFree) ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ [HuggingFaceå…è´¹ä½“éªŒ](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)

# 2024-05-28 æ›´æ–°


## DiffCalib: Reformulating Monocular Camera Calibration as Diffusion-Based   Dense Incident Map Generation

**Authors:Xiankang He, Guangkai Xu, Bo Zhang, Hao Chen, Ying Cui, Dongyan Guo**

Monocular camera calibration is a key precondition for numerous 3D vision applications. Despite considerable advancements, existing methods often hinge on specific assumptions and struggle to generalize across varied real-world scenarios, and the performance is limited by insufficient training data. Recently, diffusion models trained on expansive datasets have been confirmed to maintain the capability to generate diverse, high-quality images. This success suggests a strong potential of the models to effectively understand varied visual information. In this work, we leverage the comprehensive visual knowledge embedded in pre-trained diffusion models to enable more robust and accurate monocular camera intrinsic estimation. Specifically, we reformulate the problem of estimating the four degrees of freedom (4-DoF) of camera intrinsic parameters as a dense incident map generation task. The map details the angle of incidence for each pixel in the RGB image, and its format aligns well with the paradigm of diffusion models. The camera intrinsic then can be derived from the incident map with a simple non-learning RANSAC algorithm during inference. Moreover, to further enhance the performance, we jointly estimate a depth map to provide extra geometric information for the incident map estimation. Extensive experiments on multiple testing datasets demonstrate that our model achieves state-of-the-art performance, gaining up to a 40% reduction in prediction errors. Besides, the experiments also show that the precise camera intrinsic and depth maps estimated by our pipeline can greatly benefit practical applications such as 3D reconstruction from a single in-the-wild image. 

[PDF](http://arxiv.org/abs/2405.15619v1) 

**Summary**
å•ç›®ç›¸æœºæ ¡å‡†æ˜¯ä¼—å¤š3Dè§†è§‰åº”ç”¨çš„å…³é”®å…ˆå†³æ¡ä»¶ã€‚æœ€è¿‘ï¼ŒåŸºäºå¤§è§„æ¨¡æ•°æ®é›†è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¢«è¯å®èƒ½å¤Ÿç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„å›¾åƒï¼Œä¸ºå•ç›®ç›¸æœºå†…åœ¨ä¼°è®¡æä¾›æ›´å¼ºå¤§å’Œå‡†ç¡®çš„æ”¯æŒã€‚

**Key Takeaways**
- å•ç›®ç›¸æœºæ ¡å‡†å¯¹äºå¤šç§3Dè§†è§‰åº”ç”¨è‡³å…³é‡è¦
- æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„å›¾åƒ
- é€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹ä¸­çš„è§†è§‰çŸ¥è¯†ï¼Œèƒ½å¤Ÿå®ç°æ›´ç¨³å¥å’Œå‡†ç¡®çš„å•ç›®ç›¸æœºå†…åœ¨ä¼°è®¡
- é€šè¿‡å°†ä¼°è®¡ç›¸æœºå†…åœ¨å‚æ•°çš„é—®é¢˜é‡æ–°æ„å»ºä¸ºå¯†é›†å…¥å°„å›¾ç”Ÿæˆä»»åŠ¡ï¼Œèƒ½å¤Ÿå®ç°æ›´ç®€å•çš„æ¨æ–­è¿‡ç¨‹
- è”åˆä¼°è®¡æ·±åº¦å›¾èƒ½å¤Ÿè¿›ä¸€æ­¥æå‡æ€§èƒ½
- å®éªŒè¯æ˜è¯¥æ¨¡å‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œé¢„æµ‹è¯¯å·®é™ä½äº†40%
- ç²¾ç¡®çš„ç›¸æœºå†…åœ¨å’Œæ·±åº¦å›¾èƒ½å¤Ÿæå¤§åœ°ä¿ƒè¿›ä»å•å¼ é‡å¤–å›¾åƒè¿›è¡Œçš„3Dé‡å»ºç­‰å®é™…åº”ç”¨

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**


<ol>
<li>
<p>Title: DiffCalib: Reformulating Monocular Camera Calibration as Diffusion-Based Dense Incident Map Generation (DiffCalibï¼šå°†å•ç›®æ‘„åƒæœºæ ¡å‡†é‡æ„ä¸ºåŸºäºæ‰©æ•£çš„å¯†é›†å…¥å°„å›¾ç”Ÿæˆ)</p>
</li>
<li>
<p>Authors: Xiankang He, Guangkai Xu, Bo Zhang, Hao Chen, Ying Cui, Dongyan Guo</p>
</li>
<li>
<p>Affiliation: æµ™æ±Ÿå·¥ä¸šå¤§å­¦</p>
</li>
<li>
<p>Keywords: monocular camera calibration, diffusion models, incident map generation</p>
</li>
<li>
<p>Urls: arXiv:2405.15619v1, Github:None</p>
</li>
<li>
<p>Summary:</p>
</li>
<li>
<p>(1):æœ¬æ–‡çš„ç ”ç©¶èƒŒæ™¯æ˜¯å•ç›®æ‘„åƒæœºæ ¡å‡†ï¼Œè¿™æ˜¯è®¸å¤šä¸‰ç»´è§†è§‰åº”ç”¨çš„å…³é”®å‰ææ¡ä»¶ã€‚</p>
</li>
<li>
<p>(2):è¿‡å»çš„æ–¹æ³•å­˜åœ¨ä¸€äº›å‡è®¾å’Œé™åˆ¶ï¼Œæ— æ³•åœ¨ä¸åŒçš„çœŸå®ä¸–ç•Œåœºæ™¯ä¸­æ³›åŒ–ï¼Œå¹¶ä¸”å—é™äºè®­ç»ƒæ•°æ®çš„ä¸è¶³ã€‚æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢å–å¾—äº†æˆåŠŸï¼Œè¿™å¯å‘äº†æˆ‘ä»¬ä½¿ç”¨æ‰©æ•£æ¨¡å‹æ¥å®ç°æ›´é²æ£’å’Œå‡†ç¡®çš„å•ç›®æ‘„åƒæœºæ ¡å‡†ã€‚</p>
</li>
<li>
<p>(3):æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•æ˜¯å°†å•ç›®æ‘„åƒæœºæ ¡å‡†é—®é¢˜é‡æ„ä¸ºåŸºäºæ‰©æ•£çš„å¯†é›†å…¥å°„å›¾ç”Ÿæˆä»»åŠ¡ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆå…¥å°„å›¾ï¼Œç„¶åä½¿ç”¨RANSACç®—æ³•æ¨æ–­æ‘„åƒæœºå‚ã€‚</p>
</li>
<li>
<p>(4):æœ¬æ–‡çš„æ–¹æ³•åœ¨å•ç›®æ‘„åƒæœºæ ¡å‡†ä»»åŠ¡ä¸Šå–å¾—äº†å¾ˆå¥½çš„æ€§èƒ½ï¼Œè¯æ˜äº†æ‰©æ•£æ¨¡å‹åœ¨ç†è§£è§†è§‰ä¿¡æ¯æ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶ä¸”å¯ä»¥ç”¨äºåœ¨é‡ä¸‰ç»´é‡å»ºä»»åŠ¡ä¸­ã€‚</p>
</li>
<li>
<p>æ–¹æ³•ï¼š</p>
</li>
<li>
<p>(1)ï¼šå°†å•ç›®æ‘„åƒæœºæ ¡å‡†é—®é¢˜é‡æ„ä¸ºåŸºäºæ‰©æ•£çš„å¯†é›†å…¥å°„å›¾ç”Ÿæˆä»»åŠ¡ï¼Œä»¥ä¾¿èƒ½å¤Ÿåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆå…¥å°„å›¾ã€‚</p>
</li>
<li>
<p>(2)ï¼šä½¿ç”¨Stable Diffusion v2.1æ¨¡å‹å¯¹å…¥å°„å›¾è¿›è¡Œç¼–ç å’Œè§£ç ï¼Œç”Ÿæˆå™ªå£°åçš„å…¥å°„å›¾latent codesï¼Œå¹¶è®­ç»ƒU-Netæ¨¡å‹æ¥é¢„æµ‹å™ªå£°ã€‚</p>
</li>
<li>
<p>(3)ï¼šå°†æ·±åº¦å›¾å’Œå…¥å°„å›¾è”åˆå­¦ä¹ ï¼Œä»¥æé«˜å…¥å°„å›¾ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</p>
</li>
<li>
<p>(4)ï¼šä½¿ç”¨RANSACç®—æ³•ä»ç”Ÿæˆçš„å…¥å°„å›¾ä¸­æ¢å¤æ‘„åƒæœºçš„å†…å‚æ•°çŸ©é˜µKã€‚</p>
</li>
<li>
<p>(5)ï¼šä½¿ç”¨ensembleæ–¹æ³•æ¥æé«˜å…¥å°„å›¾ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚</p>
</li>
<li>
<p>(6)ï¼šä½¿ç”¨æ¢å¤çš„æ‘„åƒæœºå†…å‚æ•°çŸ©é˜µKæ¥è¿›è¡Œå•ç›®æ‘„åƒæœºæ ¡å‡†ã€‚</p>
</li>
<li>
<p>Conclusion: </p>
</li>
<li>
<p>(1): è¿™ç¯‡æ–‡ç« çš„æ„ä¹‰åœ¨äºæå‡ºäº†å¯¹äº[é¢†åŸŸ]çš„æ–°æ€è·¯ï¼Œä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶å’Œå‘å±•å¸¦æ¥äº†æ–°çš„å¯å‘å’Œæ–¹å‘ï¼›</p>
</li>
<li>(2): Innovation point: è¯¥æ–‡ç« çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ç§å…¨æ–°çš„[åˆ›æ–°ç‚¹]ï¼Œçªç ´äº†ä¼ ç»Ÿçš„[åˆ›æ–°ç‚¹]æ–¹å¼ï¼› Performance: è¯¥æ–‡ç« åœ¨å®éªŒè¡¨ç°æ–¹é¢å±•ç°å‡ºäº†è¾ƒé«˜çš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ï¼Œä½†ä»æœ‰å¾…è¿›ä¸€æ­¥æå‡ï¼› Workload: è¯¥æ–‡ç« çš„å·¥ä½œé‡è¾ƒå¤§ï¼Œéœ€è¦æ›´å¤šçš„å®éªŒæ•°æ®å’Œåˆ†ææ¥æ”¯æ’‘å…¶ç»“è®ºã€‚</li>
</ol>



<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-02a306a749ab4f7167af1ae9e9bd38f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3354b1c0f182b11d7a2fe0d1f53745ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3bcd389775a3247ad6697fadd1fd9cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a6244aa42d8f424a5319ca260b17f35.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36a0effe69414b2ffa084f4cd6a69d06.jpg" align="middle">
</details>




## Defensive Unlearning with Adversarial Training for Robust Concept   Erasure in Diffusion Models

**Authors:Yimeng Zhang, Xin Chen, Jinghan Jia, Yihua Zhang, Chongyu Fan, Jiancheng Liu, Mingyi Hong, Ke Ding, Sijia Liu**

Diffusion models (DMs) have achieved remarkable success in text-to-image generation, but they also pose safety risks, such as the potential generation of harmful content and copyright violations. The techniques of machine unlearning, also known as concept erasing, have been developed to address these risks. However, these techniques remain vulnerable to adversarial prompt attacks, which can prompt DMs post-unlearning to regenerate undesired images containing concepts (such as nudity) meant to be erased. This work aims to enhance the robustness of concept erasing by integrating the principle of adversarial training (AT) into machine unlearning, resulting in the robust unlearning framework referred to as AdvUnlearn. However, achieving this effectively and efficiently is highly nontrivial. First, we find that a straightforward implementation of AT compromises DMs' image generation quality post-unlearning. To address this, we develop a utility-retaining regularization on an additional retain set, optimizing the trade-off between concept erasure robustness and model utility in AdvUnlearn. Moreover, we identify the text encoder as a more suitable module for robustification compared to UNet, ensuring unlearning effectiveness. And the acquired text encoder can serve as a plug-and-play robust unlearner for various DM types. Empirically, we perform extensive experiments to demonstrate the robustness advantage of AdvUnlearn across various DM unlearning scenarios, including the erasure of nudity, objects, and style concepts. In addition to robustness, AdvUnlearn also achieves a balanced tradeoff with model utility. To our knowledge, this is the first work to systematically explore robust DM unlearning through AT, setting it apart from existing methods that overlook robustness in concept erasing. Codes are available at: https://github.com/OPTML-Group/AdvUnlearn 

[PDF](http://arxiv.org/abs/2405.15234v1) Codes are available at https://github.com/OPTML-Group/AdvUnlearn

**Summary**
åŸºäºå¯¹æŠ—è®­ç»ƒå¢å¼ºæœºå™¨unlearningï¼Œæå‡ºAdvUnlearnæ¡†æ¶ï¼Œä»¥æé«˜æ¦‚å¿µæ“¦é™¤çš„é²æ£’æ€§ã€‚

**Key Takeaways**
â€¢  Diffusionæ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†ä¹Ÿå­˜åœ¨å®‰å…¨é£é™©ï¼Œå¦‚ç”Ÿæˆæœ‰å®³å†…å®¹å’Œç‰ˆæƒè¿è§„ã€‚
â€¢  æœºå™¨unlearningæŠ€æœ¯å¯ä»¥è§£å†³è¿™äº›é£é™©ï¼Œä½†æ˜“å—åˆ°å¯¹æŠ—promptæ”»å‡»ã€‚
â€¢  æœ¬å·¥ä½œæå‡ºAdvUnlearnæ¡†æ¶ï¼Œé€šè¿‡å°†å¯¹æŠ—è®­ç»ƒåŸåˆ™é›†æˆåˆ°æœºå™¨unlearningä¸­ï¼Œä»¥æé«˜æ¦‚å¿µæ“¦é™¤çš„é²æ£’æ€§ã€‚
â€¢ AdvUnlearnæ¡†æ¶ä½¿ç”¨utility-retaining regularizationæ¥å¹³è¡¡æ¦‚å¿µæ“¦é™¤é²æ£’æ€§å’Œæ¨¡å‹å®ç”¨æ€§ã€‚
â€¢  æ–‡æœ¬ç¼–ç å™¨æ˜¯å®ç°æœºå™¨unlearningçš„æ›´é€‚åˆæ¨¡å—ã€‚
â€¢  AdvUnlearnæ¡†æ¶å¯ä»¥åœ¨å„ç§Diffusionæ¨¡å‹unlearningåœºæ™¯ä¸‹å®ç°é²æ£’çš„æ¦‚å¿µæ“¦é™¤ã€‚
â€¢  æœ¬å·¥ä½œæ˜¯é¦–æ¬¡ç³»ç»Ÿåœ°æ¢ç´¢é€šè¿‡å¯¹æŠ—è®­ç»ƒå®ç°é²æ£’çš„Diffusionæ¨¡å‹unlearningã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**


<ol>
<li>
<p>Title: AdvUnlearn: Robust Unlearning for Diffusion Models (Diffusionæ¨¡å‹çš„é²æ£’unlearning)</p>
</li>
<li>
<p>Authors: (no authors listed)</p>
</li>
<li>
<p>Affiliation: æ— </p>
</li>
<li>
<p>Keywords: Diffusion Models, Machine Unlearning, Adversarial Training, Text-to-Image Generation</p>
</li>
<li>
<p>Urls: https://github.com/OPTML-Group/AdvUnlearn</p>
</li>
<li>
<p>Summary:</p>
<ul>
<li>
<p>(1):éšç€Diffusionæ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­çš„æˆåŠŸï¼Œå®ƒä»¬ä¹Ÿå¸¦æ¥äº†å®‰å…¨é£é™©ï¼Œå¦‚ç”Ÿæˆæœ‰å®³å†…å®¹å’Œç‰ˆæƒè¿åã€‚ä¸ºè§£å†³è¿™äº›é£é™©ï¼Œæœºå™¨unlearningæŠ€æœ¯è¢«å¼€å‘å‡ºæ¥ï¼Œä½†æ˜¯è¿™äº›æŠ€æœ¯ä»æ˜“å—å¯¹æŠ—æ€§promptæ”»å‡»çš„å½±å“ã€‚</p>
</li>
<li>
<p>(2):è¿‡å»çš„æ–¹æ³•ï¼Œå¦‚ScissorHandså’ŒEraseDiffï¼Œè™½ç„¶å¯ä»¥å®ç°é«˜çš„unlearning robustnessï¼Œä½†æ˜¯å®ƒä»¬å›¾åƒç”Ÿæˆè´¨é‡ä¸‹é™æ˜æ˜¾ã€‚è¿™äº›æ–¹æ³•çš„motivationä¸è¶³ï¼Œæ— æ³•è§£å†³æœºå™¨unlearningä¸­çš„å®‰å…¨é£é™©ã€‚</p>
</li>
<li>
<p>(3):æœ¬æ–‡æå‡ºäº†AdvUnlearnæ¡†æ¶ï¼Œç»“åˆå¯¹æŠ—æ€§è®­ç»ƒæ¥å¢å¼ºæœºå™¨unlearningçš„robustnessã€‚è¯¥æ¡†æ¶ä½¿ç”¨utility-retaining regularizationæ¥å¹³è¡¡æ¦‚å¿µæ“¦é™¤çš„robustnesså’Œæ¨¡å®ç”¨æ€§ï¼Œå¹¶å°†æ–‡æœ¬ç¼–ç å™¨ä½œä¸ºrobustificationçš„æ¨¡å—ã€‚</p>
</li>
<li>
<p>(4):æœ¬æ–‡åœ¨å¤šä¸ªDiffusionæ¨¡å‹unlearningåœºæ™¯ä¸­è¿›è¡Œäº†å®éªŒï¼ŒåŒ…æ‹¬è£¸ä½“ã€å¯¹è±¡å’Œé£æ ¼æ¦‚å¿µçš„æ“¦é™¤ã€‚ç»“æœè¡¨æ˜ï¼ŒAdvUnlearnæ¡†æ¶å¯ä»¥å®ç°robustçš„æœºå™¨unlearningï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å®ç”¨æ€§ã€‚</p>
</li>
<li>æ–¹æ³•ï¼š</li>
</ul>
</li>
<li>
<p>(1):æå‡ºAdvUnlearnæ¡†æ¶ï¼Œç»“åˆå¯¹æŠ—æ€§è®­ç»ƒæ¥å¢å¼ºæœºå™¨unlearningçš„robustnessï¼Œä½¿ç”¨utility-retaining regularizationæ¥å¹³è¡¡æ¦‚å¿µæ“¦é™¤çš„robustnesså’Œæ¨¡å®ç”¨ï¼Œå¹¶å°†æ–‡æœ¬ç¼–ç å™¨ä½œä¸ºrobustificationçš„æ¨¡å—ã€‚</p>
</li>
<li>
<p>(2):ä½¿ç”¨large language model (LLM)ä½œä¸ºjudgeæ¥ç­›é€‰ä¿ç•™promptï¼Œæ’é™¤ä¸ç›®æ ‡æ¦‚å¿µæ“¦é™¤ç›¸å…³çš„promptï¼Œä»è€Œç¡®ä¿å›¾åƒç”Ÿæˆè´¨é‡ä¸å—æŸå®³ã€‚</p>
</li>
<li>
<p>(3):å®šä¹‰utility-retaining regularizationæŸå¤±å‡½æ•°â„“ESDï¼Œpenalizeså›¾åƒç”Ÿæˆè´¨é‡çš„ä¸‹é™ï¼Œä½¿ç”¨å½“å‰Diffusionæ¨¡å‹Î¸ä¸åŸå§‹Î¸oä¸‹çš„ä¿ç•™æ¦‚å¿µËœcæ¥è®¡ç®—ã€‚</p>
</li>
<li>
<p>(4):ä½¿ç”¨fast attack generationæ–¹æ³•æ¥ç®€åŒ–AdvUnlearnçš„lower-levelä¼˜åŒ–ï¼Œä½¿ç”¨fast gradient sign method (FGSM)æ¥è§£å†³quadratic programï¼Œå¹¶ç”Ÿæˆå¯¹æŠ—æ€§promptã€‚</p>
</li>
<li>
<p>(5):å°†AdvUnlearnåº”ç”¨äºä¸åŒçš„Diffusionæ¨¡å‹unlearningåœºæ™¯ï¼ŒåŒ…æ‹¬è£¸ä½“ã€å¯¹è±¡å’Œé£æ ¼æ¦‚å¿µçš„æ“¦é™¤ï¼Œå¹¶è¯„ä¼°å…¶robustnesså’Œå›¾åƒç”Ÿæˆè´¨é‡ã€‚</p>
</li>
<li>
<p>(6):æ¯”è¾ƒAdvUnlearnä¸å…¶æ–¹æ³•ï¼ˆå¦‚ESDå’ŒAT-ESDï¼‰çš„æ€§èƒ½ï¼Œè¯æ˜AdvUnlearnå¯ä»¥å®ç°robustçš„æœºå™¨unlearningï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å®ç”¨æ€§</p>
</li>
<li>
<p>(7):æ¢ç´¢AdvUnlearnçš„æ¨¡å—åŒ–è®¾è®¡ï¼Œè®¨è®ºå°†æ–‡æœ¬ç¼–ç å™¨ä½œä¸ºplug-in unlearnerçš„å¯èƒ½æ€§ï¼Œä»¥æé«˜æœºå™¨unlearningçš„æ•ˆç‡å’Œæ™®é€‚æ€§ã€‚</p>
</li>
<li>
<p>Conclusion:</p>
</li>
<li>
<p>(1):æœ¬æ–‡æå‡ºçš„AdvUnlearnæ¡†æ¶å¯¹Diffusionæ¨¡å‹çš„æœºå™¨unlearningé¢†åŸŸå…·æœ‰é‡è¦æ„ä¹‰ï¼Œå› ä¸ºå®ƒå¯ä»¥å¢å¼ºæœºå™¨unlearningçš„robustnessï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å®ç”¨æ€§ã€‚</p>
</li>
<li>
<p>(2):Innovation point: æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æœºå™¨unlearningæ–¹æ³•ï¼Œç»“åˆå¯¹æŠ—æ€§è®­ç»ƒå’Œutility-retaining regularizationæ¥å¢å¼ºæœºå™¨unlearningçš„robustnessï¼›Performance: AdvUnlearnæ¡†æ¶åœ¨å¤šä¸ªDiffusionæ¨¡å‹unlearningåœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œå®ç°äº†robustçš„æœºå™¨unlearningï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å®ç”¨æ€§ï¼›Workload: æœ¬æ–‡çš„å®éªŒè®¾è®¡å’Œå®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œæ—¶é—´ã€‚</p>
</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-12bc7afe95c87708c06799dd505c46da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3f86497a08db26b9953f1bc30dad1c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ef67ded1db4d01263a65cdacd20797a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-202a39b4f890f5df5c6e0f34c4f7a6a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89575cd27c93753bf34b1aebf5ce8aef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-005e6d2cd8b93a64b356e1bd2dd224c9.jpg" align="middle">
</details>




## DEEM: Diffusion Models Serve as the Eyes of Large Language Models for   Image Perception

**Authors:Run Luo, Yunshui Li, Longze Chen, Wanwei He, Ting-En Lin, Ziqiang Liu, Lei Zhang, Zikai Song, Xiaobo Xia, Tongliang Liu, Min Yang, Binyuan Hui**

The development of large language models (LLMs) has significantly advanced the emergence of large multimodal models (LMMs). While LMMs have achieved tremendous success by promoting the synergy between multimodal comprehension and creation, they often face challenges when confronted with out-of-distribution data. This is primarily due to their reliance on image encoders trained to encode images into task-relevant features, which may lead them to disregard irrelevant details. Delving into the modeling capabilities of diffusion models for images naturally prompts the question: Can diffusion models serve as the eyes of large language models for image perception? In this paper, we propose DEEM, a simple and effective approach that utilizes the generative feedback of diffusion models to align the semantic distributions of the image encoder. This addresses the drawbacks of previous methods that solely relied on image encoders like ViT, thereby enhancing the model's resilience against out-of-distribution samples and reducing visual hallucinations. Importantly, this is achieved without requiring additional training modules and with fewer training parameters. We extensively evaluated DEEM on both our newly constructed RobustVQA benchmark and another well-known benchmark, POPE, for object hallucination. Compared to the state-of-the-art interleaved content generation models, DEEM exhibits enhanced robustness and a superior capacity to alleviate model hallucinations while utilizing fewer trainable parameters, less pre-training data (10%), and a smaller base model size. 

[PDF](http://arxiv.org/abs/2405.15232v1) 25 pages

**Summary**
é€šè¿‡ä½¿ç”¨æ‰©æ•£æ¨¡å‹ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDEEMçš„ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆåé¦ˆæ¥è°ƒæ•´å›¾åƒç¼–ç å™¨çš„è¯­ä¹‰åˆ†å¸ƒï¼Œä»è€Œå¢å¼ºäº†æ¨¡å‹å¯¹äºè¶…å‡ºåˆ†å¸ƒæ•°æ®çš„é²æ£’æ€§ï¼Œå‡å°‘äº†è§†è§‰å¹»è§‰ï¼ŒåŒæ—¶æ— éœ€é¢å¤–çš„è®­ç»ƒæ¨¡å—å’Œæ›´å°‘çš„è®­ç»ƒå‚æ•°ã€‚

**Key Takeaways**
- å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•æ¨åŠ¨äº†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„å‡ºç°ï¼›
- LMMsåœ¨ä¿ƒè¿›å¤šæ¨¡æ€ç†è§£å’Œåˆ›ä½œæ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†åœ¨å¤„ç†è¶…å‡ºåˆ†å¸ƒæ•°æ®æ—¶é¢ä¸´æŒ‘æˆ˜ï¼›
- DEEMåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆåé¦ˆæ¥è°ƒæ•´å›¾åƒç¼–ç å™¨çš„è¯­ä¹‰åˆ†å¸ƒï¼Œè§£å†³äº†ä»¥å¾€ä»…ä¾èµ–äºå›¾åƒç¼–ç å™¨çš„æ–¹æ³•çš„ç¼ºé™·ï¼›
- DEEMåœ¨RobustVQAåŸºå‡†å’ŒPOPEåŸºå‡†ä¸Šå¾—åˆ°äº†å¹¿æ³›è¯„ä¼°ï¼Œè¡¨ç°å‡ºå“è¶Šçš„é²æ£’æ€§å’Œå‡å°‘æ¨¡å‹å¹»è§‰çš„èƒ½åŠ›ï¼›
- DEEMç›¸è¾ƒäºæœ€å…ˆè¿›çš„äº¤æ›¿å†…å®¹ç”Ÿæˆæ¨¡å‹ï¼Œå±•ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ï¼Œå¹¶åˆ©ç”¨æ›´å°‘çš„å¯è®­ç»ƒå‚æ•°ã€æ›´å°‘çš„é¢„è®­ç»ƒæ•°æ®ï¼ˆ10%ï¼‰å’Œæ›´å°çš„åŸºç¡€æ¨¡å‹å°ºå¯¸ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<h2>Paper:1</h2>
<ol>
<li>
<p>Title: DEEMï¼šä½¿ç”¨æ‰©æ•£æ¨¡å‹å¯¹å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„å›¾åƒæ„ŸçŸ¥è¿›è¡Œå¢å¼º (DEEM: Enhancing Image Perception of Large Multimodal Models with Diffusion Models)</p>
</li>
<li>
<p>Authors: (no author names provided)</p>
</li>
<li>
<p>Affiliation: æ—  (no affiliation provided)</p>
</li>
<li>
<p>Keywords: large language models, large multimodal models, diffusion models, image perception, robustness, hallucination</p>
</li>
<li>
<p>Urls: arXiv:2405.15232v1, Github: None</p>
</li>
<li>
<p>Summary:</p>
<ul>
<li>
<p>(1):è¯¥è®ºæ–‡çš„ç ”ç©¶èƒŒæ™¯æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„å‘å±•ï¼Œåè€…é€šè¿‡ç®€å•çš„æ˜ å°„æ¨¡å—å°†LLMsä¸å›¾åƒç¼–ç å™¨è¿æ¥èµ·æ¥ï¼Œå®ç°å¤šæ¨¡æ€ç†è§£ä»»åŠ¡ã€‚</p>
</li>
<li>
<p>(2):è¿‡å»çš„æ–¹æ³•ä¸»è¦ä¾èµ–å›¾åƒç¼–ç å™¨æ¥å°†å›¾åƒç¼–ç ä¸ºä»»åŠ¡ç›¸å…³ç‰¹å¾ï¼Œå¯èƒ½å¿½è§†æ— å…³ç»†èŠ‚ï¼Œä»è€Œå¯¼è‡´æ¨¡å‹å¯¹å¤–åˆ†å¸ƒæ•°æ®çš„robustnesså’Œhallucinationé—®é¢˜ã€‚</p>
</li>
<li>
<p>(3):æœ¬æ–‡æå‡ºçš„æ–¹æ³•æ˜¯DEEMï¼Œå®ƒä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆåé¦ˆæ¥å¯¹é½å›¾åƒç¼–ç å™¨çš„è¯­ä¹‰åˆ†å¸ƒï¼Œæé«˜æ¨¡å‹å¯¹å¤–åˆ†å¸ƒæ•°æ®çš„robustnesså’Œå‡å°‘hallucinationã€‚</p>
</li>
<li>
<p>(4):è¯¥æ–¹æ³•åœ¨RobustVQAå’ŒPOPEä¸¤ä¸ªåŸºå‡†æµ‹è¯•æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜DEEMç›¸æ¯”äºå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹å…·æœ‰æ›´å¥½çš„robustnesså’Œå‡å°‘hallucinationèƒ½åŠ›ï¼ŒåŒæ—¶è¿˜å¯ä»¥åœ¨å¤šæ¨¡æ€ä»»åŠ¡å¦‚è§†è§‰é—®ç­”ã€å›¾åƒå­—å¹•ç”Ÿæˆå’Œæ–‡æœ¬æ¡ä»¶å›¾åƒåˆæˆç­‰æ–¹é¢å–å¾—ç«äº‰æ€§çš„ç»“æœã€‚</p>
</li>
<li>æ–¹æ³•ï¼š</li>
</ul>
</li>
<li>
<p>(1)ï¼šé¦–å…ˆï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºæ–‡æœ¬ç¼–ç å™¨ï¼Œç”Ÿæˆå›¾åƒç›¸å…³çš„æ–‡æœ¬ç‰¹å¾ï¼Œä»¥ä¾¿ä¸å›¾åƒç¼–ç å™¨è¿›è¡Œå¯¹é½ã€‚</p>
</li>
<li>
<p>(2)ï¼šç„¶åï¼Œä½¿ç”¨æ‰©æ•£æ¨¡å‹ï¼ˆDiffusion Modelï¼‰å¯¹å›¾åƒç¼–ç å™¨çš„è¾“å‡ºè¿›è¡Œç”Ÿæˆåé¦ˆï¼Œä»¥è°ƒæ•´å›¾åƒç¼–ç å™¨è¯­ä¹‰åˆ†å¸ƒï¼Œæé«˜æ¨¡å‹å¯¹å¤–åˆ†å¸ƒæ•°æ®çš„robustnessã€‚</p>
</li>
<li>
<p>(3)ï¼šåœ¨ç”Ÿæˆåé¦ˆè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨å¯¹æŠ—è®­ç»ƒï¼ˆAdversarial Trainingï¼‰æ¥é¼“åŠ±å›¾åƒç¼–ç å™¨ç”Ÿæˆæ›´åŠ robustçš„ç‰¹å¾ï¼Œå‡å°‘hallucinationçš„å¯èƒ½æ€§ã€‚</p>
</li>
<li>
<p>(4)ï¼šæ¥ç€ï¼Œå¯¹DEEMæ¨¡å‹è¿›è¡Œå¤šæ¨¡æ€ä»»åŠ¡çš„fine-tuningï¼Œä¾‹å¦‚è§†è§‰é—®ç­”ã€å›¾åƒå­—å¹•ç”Ÿæˆå’Œæ–‡æœ¬æ¡ä»¶å›¾åƒåˆæˆç­‰ï¼Œä»¥æé«˜æ¨¡å‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</p>
</li>
<li>
<p>(5)ï¼šæœ€åï¼Œåœ¨RobustVQAå’ŒPOPEä¸¤ä¸ªåŸºå‡†æµ‹è¯•æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œè¯„ä¼°DEEMæ¨¡å‹çš„robustnesså’Œhallucinationèƒ½åŠ›ï¼Œä¸¦ä¸å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚</p>
</li>
<li>
<p>Conclusion: </p>
</li>
<li>
<p>(1): æœ¬ç ”ç©¶çš„æ„ä¹‰åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼ˆDEEMï¼‰ï¼Œé€šè¿‡ä½¿ç”¨æ‰©æ•£æ¨¡å‹å¯¹å¤§å‹å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œå›¾åƒæ„ŸçŸ¥å¢å¼ºï¼Œæœ‰æ•ˆæé«˜äº†æ¨¡å‹çš„é²æ£’æ€§å’Œå‡å°‘äº†è™šå‡æ„ŸçŸ¥ï¼Œä¸ºå¤šæ¨¡æ€ä»»åŠ¡çš„æ€§èƒ½æå‡æä¾›äº†æ–°çš„æ€è·¯ã€‚</p>
</li>
<li>
<p>(2): åˆ›æ–°ç‚¹ï¼šDEEMæ–¹æ³•åˆ©ç”¨æ‰©æ•£æ¨¡å‹å¯¹å›¾åƒç¼–ç å™¨çš„è¯­ä¹‰åˆ†å¸ƒè¿›è¡Œè°ƒæ•´ï¼Œåœ¨æé«˜æ¨¡å‹é²æ£’æ€§å’Œå‡å°‘è™šå‡æ„ŸçŸ¥æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ã€‚æ€§èƒ½ï¼šDEEMåœ¨RobustVQAå’ŒPOPEä¸¤ä¸ªåŸºå‡†æµ‹è¯•æ•°æ®é›†ä¸Šç›¸æ¯”å½“å‰æœ€å…ˆè¿›æ¨¡å‹å…·æœ‰æ›´å¥½çš„é²æ£’æ€§å’Œå‡å°‘è™šå‡æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šå–å¾—äº†ç«äº‰æ€§çš„ç»“æœã€‚å·¥ä½œé‡ï¼šè®ºæ–‡æ‰€æå‡ºçš„DEEMæ–¹æ³•éœ€è¦è¿›ä¸€æ­¥å®éªŒå’ŒéªŒè¯ï¼Œä»¥ç¡®ä¿å…¶åœ¨ä¸åŒé¢†åŸŸçš„æ³›åŒ–æ€§èƒ½ï¼Œè¿™å¯èƒ½éœ€è¦æ›´å¤šçš„å·¥ä½œé‡æ¥æ”¯æŒã€‚</p>
</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c0b6103bc7ef9889b013616a33153dac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5911a832e2f068efcd4f1c57fb6c0989.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f388f04ad9850dd89191f6903b1cf64.jpg" align="middle">
</details>




## NIVeL: Neural Implicit Vector Layers for Text-to-Vector Generation

**Authors:Vikas Thamizharasan, Difan Liu, Matthew Fisher, Nanxuan Zhao, Evangelos Kalogerakis, Michal Lukac**

The success of denoising diffusion models in representing rich data distributions over 2D raster images has prompted research on extending them to other data representations, such as vector graphics. Unfortunately due to their variable structure and scarcity of vector training data, directly applying diffusion models on this domain remains a challenging problem. Using workarounds like optimization via Score Distillation Sampling (SDS) is also fraught with difficulty, as vector representations are non trivial to directly optimize and tend to result in implausible geometries such as redundant or self-intersecting shapes. NIVeL addresses these challenges by reinterpreting the problem on an alternative, intermediate domain which preserves the desirable properties of vector graphics -- mainly sparsity of representation and resolution-independence. This alternative domain is based on neural implicit fields expressed in a set of decomposable, editable layers. Based on our experiments, NIVeL produces text-to-vector graphics results of significantly better quality than the state-of-the-art. 

[PDF](http://arxiv.org/abs/2405.15217v1) 

**Summary**
æ‰©å±•å»å™ªæ‰©æ•£æ¨¡å‹åˆ°çŸ¢é‡å›¾å½¢é¢†åŸŸçš„æŒ‘æˆ˜æ€§è§£å†³æ–¹æ¡ˆNIVeLã€‚

**Key Takeaways**
â€¢ å»å™ªæ‰©æ•£æ¨¡å‹åœ¨2D rasterå›¾åƒä¸Šçš„æˆåŠŸä¿ƒä½¿ç ”ç©¶å°†å…¶æ‰©å±•åˆ°å…¶ä»–æ•°æ®è¡¨ç¤ºå½¢å¼ï¼Œå¦‚çŸ¢é‡å›¾å½¢ã€‚
â€¢ ç›´æ¥å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºçŸ¢é‡å›¾å½¢é¢†åŸŸæ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ï¼Œå› ä¸ºçŸ¢é‡å›¾å½¢å…·æœ‰å¯å˜ç»“æ„å’Œç¨€ç–çš„è®­ç»ƒæ•°æ®ã€‚
â€¢ ä½¿ç”¨Score Distillation Samplingï¼ˆSDSï¼‰ç­‰ä¼˜åŒ–æ–¹æ³•ä¹Ÿå­˜åœ¨å›°éš¾ï¼Œå› ä¸ºçŸ¢é‡è¡¨ç¤ºéš¾ä»¥ç›´æ¥ä¼˜åŒ–ï¼Œå®¹æ˜“äº§ç”Ÿä¸å¯ä¿¡çš„å‡ ä½•å½¢çŠ¶ã€‚
â€¢ NIVeLé€šè¿‡é‡æ–°è§£é‡Šé—®é¢˜åœ¨ä¸­é—´åŸŸä¸Šï¼Œä¿ç•™çŸ¢é‡å›¾å½¢çš„è‰¯å¥½å±æ€§ï¼Œä¾‹å¦‚ç¨€ç–è¡¨ç¤ºå’Œåˆ†è¾¨ç‡ç‹¬ç«‹æ€§ã€‚
â€¢ ä¸­é—´åŸŸåŸºäºå¯åˆ†è§£ã€å¯ç¼–è¾‘çš„ç¥ç»éšå¼å­—æ®µå±‚ã€‚
â€¢ å®éªŒç»“æœè¡¨æ˜ï¼ŒNIVeLç”Ÿæˆçš„æ–‡æœ¬åˆ°çŸ¢é‡å›¾å½¢ç»“æœè¿œä¼˜äºå½“å‰æœ€å…ˆè¿›çš„ç»“æœã€‚
â€¢ NIVeLè§£å†³äº†æ‰©å±•å»å™ªæ‰©æ•£æ¨¡å‹åˆ°çŸ¢é‡å›¾å½¢é¢†åŸŸçš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**


<ol>
<li>
<p>Title: NIVeL: ç¥ç»éšå¼çŸ¢é‡å›¾å½¢ç”Ÿæˆï¼ˆNeural Implicit Vector Graphics Generationï¼‰</p>
</li>
<li>
<p>Authors: Not provided</p>
</li>
<li>
<p>Affiliation: ä¸æä¾›ï¼ˆNot providedï¼‰</p>
</li>
<li>
<p>Keywords: denoising diffusion models, vector graphics, neural implicit fields</p>
</li>
<li>
<p>Urls: Not provided, Github: None</p>
</li>
<li>
<p>Summary:</p>
</li>
<li>
<p>(1):è¯¥è®ºæ–‡çš„ç ”ç©¶èƒŒæ™¯æ˜¯å°†å»å™ªæ‰©æ•£æ¨¡å‹ä»2D rasterå›¾åƒæ‰©å±•åˆ°çŸ¢é‡å›¾å½¢é¢†åŸŸï¼Œä½†çŸ¢é‡å›¾å½¢çš„å¯å˜ç»“æ„å’Œç¨€ç¼ºçš„è®­ç»ƒæ•°æ®ä½¿å¾—ç›´æ¥åº”ç”¨å»å™ªæ‰©æ•£æ¨¡å‹å˜å¾—å›°éš¾ã€‚</p>
</li>
<li>
<p>(2):è¿‡å»çš„æ–¹æ³•åŒ…æ‹¬ç›´æ¥åº”ç”¨å»å™ªæ‰©æ•£æ¨¡å‹å’ŒScore Distillation Samplingï¼ˆSDSï¼‰ä¼˜åŒ–ï¼Œä½†è¿™äº›æ–¹æ³•å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œå¦‚ç”Ÿæˆçš„çŸ¢é‡å›¾å½¢å¯èƒ½åŒ…å«å†—ä½™æˆ–è‡ªç›¸äº¤çš„å½¢çŠ¶ã€‚</p>
</li>
<li>
<p>(3):æœ¬è®ºæ–‡æå‡ºäº†NIVeLæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å°†é—®é¢˜é‡æ–°è§£é‡Šåœ¨ä¸­é—´åŸŸä¸Šï¼Œå³åŸºäºç¥ç»éšå¼å­—æ®µçš„å¯åˆ†è§£ã€å¯ç¼–è¾‘çš„å±‚æ¥ç”ŸæˆçŸ¢é‡å›¾å½¢ã€‚</p>
</li>
<li>
<p>(4):æœ¬è®ºæ–‡çš„æ–¹æ³•åœ¨æ–‡æœ¬åˆ°çŸ¢é‡å›¾å½¢ä»»åŠ¡ä¸Šå–å¾—äº†æ˜æ˜¾ä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ï¼Œè¯æ˜äº†NIVeLæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</li>
<li>
<p>æ–¹æ³•ï¼š</p>
</li>
<li>
<p>(1):å°†çŸ¢é‡å›¾å½¢ç”Ÿæˆé—®é¢˜é‡æ–°è§£é‡Šåœ¨ä¸­é—´åŸŸä¸Šï¼Œå³åŸºäºç¥ç»éšå¼å­—æ®µï¼ˆNeural Implicit Fieldsï¼‰çš„å¯åˆ†è§£ã€å¯ç¼–è¾‘çš„å±‚ï¼Œä»¥ä¾¿æ›´å¥½åœ°å¤„ç†çŸ¢é‡å›¾å½¢çš„å¯å˜ç»“æ„å’Œç¨€ç¼ºçš„è®­ç»ƒæ•°æ®ã€‚</p>
</li>
<li>
<p>(2):ä½¿ç”¨å»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆDenoising Diffusion Modelsï¼‰åœ¨ä¸­é—´åŸŸä¸Šç”Ÿæˆéšå¼è¡¨ç¤ºï¼Œç„¶åé€šè¿‡ç¥ç»éšå¼å­—æ®µå°†å…¶è½¬æ¢ä¸ºçŸ¢é‡å›¾å½¢ã€‚</p>
</li>
<li>
<p>(3):å¼•å…¥ Score Distillation Samplingï¼ˆSDSï¼‰ä¼˜åŒ–æ–¹æ³•ï¼Œä»¥æé«˜ç”ŸæˆçŸ¢é‡å›¾å½¢çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚</p>
</li>
<li>
<p>(4):åœ¨ä¸­é—´åŸŸä¸Šåº”ç”¨ç¼–è¾‘æ“ä½œï¼Œå¦‚å½¢çŠ¶å˜æ¢ã€æ‹“æ‰‘å˜åŒ–ç­‰ï¼Œä»¥å¢å¼ºç”ŸæˆçŸ¢é‡å›¾å½¢çš„å¯ç¼–è¾‘æ€§å’Œçµæ´»æ€§ã€‚</p>
</li>
<li>
<p>(5):ä½¿ç”¨æ–‡æœ¬åˆ°çŸ¢é‡å›¾å½¢ä»»åŠ¡çš„å®éªŒç»“æœéªŒè¯NIVeLæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜å…¶åœ¨ç”Ÿæˆé«˜è´¨é‡çŸ¢é‡å›¾å½¢æ–¹é¢çš„ä¼˜åŠ¿ã€‚</p>
</li>
<li>
<p>ç»“è®ºï¼š</p>
</li>
<li>
<p>(1):è¯¥ç¯‡å·¥ä½œçš„é‡è¦æ€§åœ¨äºå°†å»å™ªæ‰©æ•£æ¨¡å‹åº”ç”¨äºçŸ¢é‡å›¾å½¢ç”Ÿæˆé¢†åŸŸï¼Œè§£å†³äº†çŸ¢é‡å›¾å½¢çš„å¯å˜ç»“æ„å’Œç¨€ç¼ºçš„è®­ç»ƒæ•°æ®é—®é¢˜ï¼Œæé«˜äº†ç”ŸæˆçŸ¢é‡å›¾å½¢çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚</p>
</li>
<li>
<p>(2):åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºç¥ç»éšå¼å­—æ®µçš„çŸ¢é‡å›¾å½¢ç”Ÿæˆæ–¹æ³•ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†çŸ¢é‡å›¾å½¢çš„å¯å˜ç»“æ„å’Œç¨€ç¼ºçš„è®­ç»ƒæ•°æ®ï¼›æ€§èƒ½ï¼šåœ¨æ–‡æœ¬åˆ°çŸ¢é‡å›¾å½¢ä»»åŠ¡ä¸Šå–å¾—äº†æ˜æ˜¾ä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ï¼›å·¥ä½œé‡ï¼šéœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œè®¡ç®—èµ„æºï¼Œä¸”å½“å‰çš„è¡¨ç¤ºæ–¹å¼è¿˜å­˜åœ¨ä¸€äº›é™åˆ¶ï¼Œå¦‚å±‚çš„æ•°é‡é™åˆ¶ç­‰ã€‚</p>
</li>
</ol>



<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-deb0bce750c823b45864a06b1f2fdf37.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b05c16791ff3624415d2ca5a4bb2b01d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ddb20e736aa45d7da426d42c0386fcb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a127e1927a9826d4a5a6449d4ce7f25e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ef7a2dd3802c3e38639f59aa13e5305.jpg" align="middle">
</details>




## TerDiT: Ternary Diffusion Models with Transformers

**Authors:Xudong Lu, Aojun Zhou, Ziyi Lin, Qi Liu, Yuhui Xu, Renrui Zhang, Yafei Wen, Shuai Ren, Peng Gao, Junchi Yan, Hongsheng Li**

Recent developments in large-scale pre-trained text-to-image diffusion models have significantly improved the generation of high-fidelity images, particularly with the emergence of diffusion models based on transformer architecture (DiTs). Among these diffusion models, diffusion transformers have demonstrated superior image generation capabilities, boosting lower FID scores and higher scalability. However, deploying large-scale DiT models can be expensive due to their extensive parameter numbers. Although existing research has explored efficient deployment techniques for diffusion models such as model quantization, there is still little work concerning DiT-based models. To tackle this research gap, in this paper, we propose TerDiT, a quantization-aware training (QAT) and efficient deployment scheme for ternary diffusion models with transformers. We focus on the ternarization of DiT networks and scale model sizes from 600M to 4.2B. Our work contributes to the exploration of efficient deployment strategies for large-scale DiT models, demonstrating the feasibility of training extremely low-bit diffusion transformer models from scratch while maintaining competitive image generation capacities compared to full-precision models. Code will be available at https://github.com/Lucky-Lance/TerDiT. 

[PDF](http://arxiv.org/abs/2405.14854v1) 18 pages, 13 figures

**Summary**
å¤§è§„æ¨¡é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æœ€æ–°å‘å±•ï¼Œæå‡ºäº†ä¸€ç§é‡åŒ–æ„ŸçŸ¥è®­ç»ƒå’Œé«˜æ•ˆéƒ¨ç½²æ–¹æ¡ˆTerDiTï¼Œç”¨äºä¸‰çº§æ‰©æ•£æ¨¡å‹çš„ transformersã€‚

**Key Takeaways**
â€¢ å¤§è§„æ¨¡é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æœ€æ–°å‘å±•ï¼Œç‰¹åˆ«æ˜¯åŸºäº transformer æ¶æ„çš„æ‰©æ•£æ¨¡å‹ï¼ˆDiTsï¼‰ï¼Œç”Ÿæˆé«˜ä¿çœŸå›¾åƒçš„èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æ”¹å–„ã€‚
â€¢ æ‰©æ•£å˜å‹å™¨æ¨¡å‹å±•ç¤ºå‡ºä¼˜è¶Šçš„å›¾åƒç”Ÿæˆèƒ½åŠ›ï¼Œå…·æœ‰è¾ƒä½çš„ FID åˆ†æ•°å’Œæ›´é«˜çš„å¯æ‰©å±•æ€§ã€‚
â€¢ éƒ¨ç½²å¤§è§„æ¨¡ DiT æ¨¡å‹å¯èƒ½å¾ˆæ˜‚è´µï¼Œå› ä¸ºå®ƒä»¬å…·æœ‰åºå¤§çš„å‚æ•°æ•°é‡ã€‚
â€¢ ç°æœ‰çš„ç ”ç©¶å·²ç»æ¢ç´¢äº†æ‰©æ•£æ¨¡å‹çš„é«˜æ•ˆéƒ¨ç½²æŠ€æœ¯ï¼Œå¦‚æ¨¡å‹é‡åŒ–ï¼Œä½†å¯¹äº DiT åŸºç¡€æ¨¡å‹çš„ç ”ç©¶ä»ç„¶å¾ˆå°‘ã€‚
â€¢ æœ¬æ–‡æå‡ºäº† TerDiTï¼Œä¸€ç§é‡åŒ–æ„ŸçŸ¥è®­ç»ƒå’Œé«˜æ•ˆéƒ¨ç½²æ–¹æ¡ˆï¼Œç”¨äºä¸‰çº§æ‰©æ•£æ¨¡å‹çš„ transformersã€‚
â€¢ è¯¥æ–¹æ¡ˆå…³æ³¨ DiT ç½‘ç»œçš„ä¸‰çº§åŒ–ï¼Œå¹¶å°†æ¨¡å‹å¤§å°ä» 600M æ‰©å±•åˆ° 4.2Bã€‚
â€¢ æœ¬å·¥ä½œä¸ºå¤§è§„æ¨¡ DiT æ¨¡å‹çš„é«˜æ•ˆéƒ¨ç½²ç­–ç•¥åšå‡ºäº†è´¡çŒ®ï¼Œè¯æ˜äº†ä»å¤´è®­ç»ƒæä½ä½æ‰©æ•£å˜å‹å™¨æ¨¡å‹çš„å¯è¡Œæ€§ï¼ŒåŒæ—¶ä¿æŒäº†ä¸å…¨ç²¾åº¦æ¨¡å‹ç›¸ä¼¼çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<h2>Paper:1</h2>
<ol>
<li>
<p>Title: TerDiTï¼šå…·æœ‰å˜å‹å™¨çš„ä¸‰è¿›åˆ¶æ‰©æ•£æ¨¡å‹ (TerDiT: Ternary Diffusion Models with Transformers)</p>
</li>
<li>
<p>Authors: Xudong Lu, Aojun Zhou, Ziyi Lin, Qi Liu, Yuhui Xu, Renrui Zhang, Yafei Wen, Shuai Ren, Peng Gao, Junchi Yan, Hongsheng Li</p>
</li>
<li>
<p>Affiliation: é¦™æ¸¯ä¸­æ–‡å¤§å­¦å¤šåª’ä½“å®éªŒå®¤</p>
</li>
<li>
<p>Keywords: diffusion models, transformer architecture, quantization-aware training, efficient deployment</p>
</li>
<li>
<p>Urls: https://arxiv.org/abs/2405.14854, Github: https://github.com/Lucky-Lance/TerDiT</p>
</li>
<li>
<p>Summary:</p>
<ul>
<li>
<p>(1):æœ€è¿‘ï¼Œå¤§è§„æ¨¡é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å‘å±•æå¤§åœ°æ”¹å–„äº†é«˜ä¿çœŸå›¾åƒçš„ç”Ÿæˆï¼Œç‰¹åˆ«æ˜¯åŸºäºå˜å‹å™¨æ¶æ„ï¼ˆDiTsï¼‰çš„æ‰©æ•£æ¨¡å‹ã€‚</p>
</li>
<li>
<p>(2):ç°æœ‰çš„ç ”ç©¶å·²ç»æ¢ç´¢äº†æ‰©æ•£æ¨¡å‹çš„é«˜æ•ˆéƒ¨ç½²æŠ€æœ¯ï¼Œå¦‚æ¨¡å‹é‡åŒ–ï¼Œä½†æ˜¯åœ¨DiTæ¨¡å‹æ–¹é¢ä»ç„¶å­˜åœ¨ç ”ç©¶gapã€‚</p>
</li>
<li>
<p>(3):æœ¬æ–‡æå‡ºTerDiTï¼Œä¸€ä¸ªé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰å’Œé«˜æ•ˆéƒ¨ç½²æ–¹æ¡ˆï¼Œç”¨äºå…·æœ‰å˜å‹å™¨çš„ä¸‰è¿›åˆ¶æ‰©æ•£æ¨¡å‹ã€‚</p>
</li>
<li>
<p>(4):æœ¬æ–‡çš„æ–¹æ³•å¯ä»¥è®­ç»ƒæä½æ¯”ç‰¹æ‰©æ•£å˜å‹å™¨æ¨¡å‹ï¼Œä»è€Œå®ç°ä¸å…¨ç²¾åº¦æ¨¡å‹ç›¸åª²ç¾çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ï¼ŒåŒæ—¶ä¹Ÿå®ç°äº†é«˜æ•ˆçš„æ¨¡å‹éƒ¨ç½²ã€‚</p>
</li>
<li>æ–¹æ³•ï¼š</li>
</ul>
</li>
<li>
<p>(1)ï¼šé‡‡ç”¨å‡é‡å‡½æ•°ï¼ˆfake quant functionï¼‰å¯¹æ¨¡å‹æƒé‡è¿›è¡Œé‡åŒ–ï¼Œè®¾ç½®n_bits=4ï¼Œä¸è¿›è¡Œæ¿€æ´»é‡åŒ–ã€‚</p>
</li>
<li>
<p>(2)ï¼šå¯¹åŸDiTå—ä¸­çš„æ‰€æœ‰çº¿æ€§å±‚æƒé‡è¿›è¡Œé‡åŒ–ï¼ŒåŒ…æ‹¬è‡ªæ³¨æ„ã€å‰é¦ˆå’ŒMLPã€‚</p>
</li>
<li>
<p>(3)ï¼šä½¿ç”¨é‡åŒ–åçš„æ¨¡å‹é‡‡æ ·å›¾åƒï¼Œå¹¶ä¸å…¨ç²¾åº¦æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚</p>
</li>
<li>
<p>(4)ï¼šæå‡ºTerDiTï¼Œä¸€ä¸ªé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰å’Œé«˜æ•ˆéƒ¨ç½²æ–¹æ¡ˆï¼Œç”¨äºå…·æœ‰å˜å‹å™¨çš„ä¸‰è¿›åˆ¶æ‰©æ•£æ¨¡å‹ã€‚</p>
</li>
<li>
<p>(5)ï¼šé‡‡ç”¨å­¦ä¹ ç‡å‡å°ç­–ç•¥ï¼Œä»¥æé«˜æ¨¡å‹çš„è®­ç»ƒç»“æœã€‚</p>
</li>
<li>
<p>(6)ï¼šä½¿ç”¨RMS Normalized adaLNæ¨¡å—ï¼Œä»¥æé«˜æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ã€‚</p>
</li>
<li>
<p>(7)ï¼šè¿›è¡Œå®éªŒæ¯”è¾ƒï¼ŒéªŒè¯TerDiTæ¨¡å‹åœ¨é«˜æ•ˆéƒ¨ç½²å’Œå›¾åƒç”Ÿæˆèƒ½åŠ›æ–¹é¢çš„ä¼˜åŠ¿ã€‚</p>
</li>
<li>
<p>ç»“è®ºï¼š</p>
</li>
<li>
<p>(1):è¯¥å·¥ä½œçš„é‡è¦æ€§åœ¨äºå®ƒæ¨åŠ¨äº†å…·æœ‰å˜å‹å™¨æ¶æ„çš„æ‰©æ•£æ¨¡å‹çš„é«˜æ•ˆéƒ¨ç½²ï¼Œæ»¡è¶³äº†å®é™…åº”ç”¨ä¸­çš„ä½å»¶è¿Ÿå’Œä½è®¡ç®—èµ„æºéœ€æ±‚ã€‚</p>
</li>
<li>
<p>(2):åˆ›æ–°ç‚¹ï¼šTerDiT æ¨¡å‹æå‡ºäº†ä¸€ç§é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰å’Œé«˜æ•ˆéƒ¨ç½²æ–¹æ¡ˆï¼Œè§£å†³äº†ç°æœ‰DiT æ¨¡å‹åœ¨é«˜æ•ˆéƒ¨ç½²æ–¹é¢çš„ç ”ç©¶gapï¼›æ€§èƒ½ï¼šTerDiT æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆèƒ½åŠ›æ–¹é¢ä¸å…¨ç²¾åº¦æ¨¡å‹ç›¸åª²ç¾ï¼ŒåŒæ—¶å®ç°äº†é«˜æ•ˆçš„æ¨¡å‹éƒ¨ç½²ï¼›å·¥ä½œé‡ï¼šè¯¥å·¥ä½œéœ€è¦å¤§é‡çš„å®éªŒè®¾è®¡å’Œæ¨¡å‹è®­ç»ƒï¼Œä¸”éœ€è¦æ·±å…¥äº†è§£DiT æ¨¡å‹å’Œé‡åŒ–æŠ€æœ¯ã€‚</p>
</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c40afa8caaa8fb0e34704a216ee65f09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21147ce65723c9373a1e3d28f5c516df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b32f6ca859af81585bc0599f40dc4518.jpg" align="middle">
</details>




