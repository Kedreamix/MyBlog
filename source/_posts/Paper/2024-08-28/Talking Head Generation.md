
---
title: Talking Head Generation
date: 2024-08-28 07:48:13
author: Kedreamix
cover: https://pic1.zhimg.com/v2-4760bb1b1f83c77ff470a2676d9247aa.jpg
categories: Paper
tags:
    - Talking Head Generation
description: Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-08-28  SpeechCaps Advancing Instruction-Based Universal Speech Models with   Multi-Talker Speaking Style Captioning  
keywords: Talking Head Generation
toc:
toc_number: false
toc_style_simple: true
copyright:
copyright_author:
copyright_author_href:
copyright_url:
copyright_info:
mathjax: true
katex:
aplayer:
highlight_shrink:
aside:
---

>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹[Gemini-Pro](https://ai.google.dev/)çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨
>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼
>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© [ChatPaperFree](https://github.com/Kedreamix/ChatPaperFree) ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ [HuggingFaceå…è´¹ä½“éªŒ](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)

# 2024-08-28 æ›´æ–°


## SpeechCaps: Advancing Instruction-Based Universal Speech Models with   Multi-Talker Speaking Style Captioning

**Authors:Chien-yu Huang, Min-Han Shih, Ke-Han Lu, Chi-Yuan Hsiao, Hung-yi Lee**

Instruction-based speech processing is becoming popular. Studies show that training with multiple tasks boosts performance, but collecting diverse, large-scale tasks and datasets is expensive. Thus, it is highly desirable to design a fundamental task that benefits other downstream tasks. This paper introduces a multi-talker speaking style captioning task to enhance the understanding of speaker and prosodic information. We used large language models to generate descriptions for multi-talker speech. Then, we trained our model with pre-training on this captioning task followed by instruction tuning. Evaluation on Dynamic-SUPERB shows our model outperforming the baseline pre-trained only on single-talker tasks, particularly in speaker and emotion recognition. Additionally, tests on a multi-talker QA task reveal that current models struggle with attributes such as gender, pitch, and speaking rate. The code and dataset are available at https://github.com/cyhuang-tw/speechcaps. 

[PDF](http://arxiv.org/abs/2408.13891v1) SynData4GenAI 2024

**Summary**
å¤šè¯´è¯è€…é£æ ¼å­—å¹•ä»»åŠ¡æå‡è¯­è¨€æ¨¡å‹åœ¨è¯´è¯è€…å’ŒéŸµå¾‹ä¿¡æ¯ç†è§£ä¸Šçš„æ€§èƒ½ã€‚

**Key Takeaways**
1. æŒ‡ä»¤å¼è¯­éŸ³å¤„ç†é€æ¸æµè¡Œã€‚
2. å¤šä»»åŠ¡è®­ç»ƒæå‡æ¨¡å‹æ€§èƒ½ã€‚
3. æ”¶é›†å¤šæ ·ã€å¤§è§„æ¨¡æ•°æ®é›†æˆæœ¬é«˜æ˜‚ã€‚
4. æå‡ºå¤šè¯´è¯è€…é£æ ¼å­—å¹•ä»»åŠ¡ä½œä¸ºåŸºç¡€ä»»åŠ¡ã€‚
5. ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šè¯´è¯è€…è¯­éŸ³æè¿°ã€‚
6. æ¨¡å‹åœ¨å­—å¹•ä»»åŠ¡ä¸Šè¿›è¡Œé¢„è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒã€‚
7. åœ¨Dynamic-SUPERBä¸Šä¼˜äºä»…è®­ç»ƒå•è¯´è¯è€…ä»»åŠ¡çš„åŸºçº¿æ¨¡å‹ã€‚
8. åœ¨å¤šè¯´è¯è€…é—®ç­”ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹åœ¨æ€§åˆ«ã€éŸ³é«˜å’Œè¯­é€Ÿç­‰å±æ€§ä¸Šè¡¨ç°ä¸ä½³ã€‚
9. æ¨¡å‹ä»£ç å’Œæ•°æ®é›†å¯åœ¨GitHubä¸Šè·å–ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**



1. Title: SPEECHCAPS: Advancing Instruction-Based Universal Speech Models with Multi-Talker Speaking Style Captioning (SPEECHCAPSï¼šé€šè¿‡å¤šè¯´è¯äººè¯´è¯é£æ ¼å­—å¹•ä»»åŠ¡æ¨è¿›åŸºäºæŒ‡ä»¤çš„é€šç”¨è¯­éŸ³æ¨¡å‹)

2. Authors: Chien-yu Huang, Min-Han Shih, Ke-Han Lu, Chi-Yuan Hsiao, Hung-yi Lee

3. Affiliation: National Taiwan University, Taiwan

4. Keywords: speech captioning, speaking style, instruction tuning, large language model

5. Urls: https://arxiv.org/abs/2408.13891, Github: https://github.com/cyhuang-tw/speechcaps

6. Summary:

   - (1): ç ”ç©¶èƒŒæ™¯ä¸ºåŸºäºæŒ‡ä»¤çš„è¯­éŸ³å¤„ç†æŠ€æœ¯é€æ¸æµè¡Œï¼Œä½†æ”¶é›†å¤šæ ·åŒ–çš„å¤§è§„æ¨¡ä»»åŠ¡å’Œæ•°æ®é›†æˆæœ¬é«˜æ˜‚ã€‚å› æ­¤ï¼Œè®¾è®¡ä¸€ä¸ªå¯¹ä¸‹æ¸¸ä»»åŠ¡æœ‰ç›Šçš„åŸºç¡€ä»»åŠ¡æ˜¯é«˜åº¦æœŸæœ›çš„ã€‚
 
   - (2): è¿‡å»çš„æ–¹æ³•åŒ…æ‹¬LTU-ASã€SALMONNã€Qwen-Audioå’ŒWavLLMç­‰ï¼Œå®ƒä»¬é€šè¿‡å¤šä»»åŠ¡è®­ç»ƒæˆ–æ¿€æ´»è°ƒæ•´æ¥æå‡æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å­˜åœ¨æ•°æ®æ”¶é›†æˆæœ¬é«˜çš„é—®é¢˜ï¼Œä¸”å¯¹è¯´è¯äººå’Œæƒ…æ„Ÿç­‰ä»»åŠ¡çš„è¯†åˆ«èƒ½åŠ›ä¸è¶³ã€‚æ‰€æå‡ºçš„æ–¹æ³•æ˜¯åˆç†çš„ï¼Œå› ä¸ºå®ƒæ—¨åœ¨é€šè¿‡åŸºç¡€ä»»åŠ¡æå‡æ¨¡å‹å¯¹é€šç”¨è¯­éŸ³çš„ç†è§£èƒ½åŠ›ã€‚
 
   - (3): è®ºæ–‡ä¸­æå‡ºçš„æ–¹æ³•æ˜¯åˆ›å»ºä¸€ä¸ªåä¸ºSPEECHCAPSçš„å¤šè¯´è¯äººè¯´è¯é£æ ¼å­—å¹•ä»»åŠ¡æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šè¯´è¯äººè¯­éŸ³çš„æè¿°ã€‚ç„¶åï¼Œé€šè¿‡åœ¨å­—å¹•ä»»åŠ¡ä¸Šè¿›è¡Œé¢„è®­ç»ƒå’ŒæŒ‡ä»¤è°ƒæ•´æ¥è®­ç»ƒæ¨¡å‹ã€‚
 
   - (4): åœ¨Dynamic-SUPERBæ•°æ®é›†ä¸Šè¿›è¡Œçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¯´è¯äººå’Œæƒ…æ„Ÿè¯†åˆ«ä»»åŠ¡ä¸Šä¼˜äºä»…å¯¹å•è¯´è¯äººä»»åŠ¡è¿›è¡Œé¢„è®­ç»ƒçš„åŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œåœ¨å¤šè¯´è¯äººé—®ç­”ä»»åŠ¡ä¸Šçš„æµ‹è¯•æ˜¾ç¤ºï¼Œå½“å‰æ¨¡å‹åœ¨å¤„ç†æ€§åˆ«ã€éŸ³è°ƒå’Œè¯´è¯é€Ÿç‡ç­‰å±æ€§æ—¶å­˜åœ¨å›°éš¾ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†ç ”ç©¶çš„ç›®æ ‡ï¼Œå³é€šè¿‡åŸºç¡€ä»»åŠ¡æå‡æ¨¡å‹å¯¹è¯´è¯äººå’ŒéŸµå¾‹ä¿¡æ¯çš„ç†è§£ã€‚


8. Conclusion:

    - (1): è¿™é¡¹å·¥ä½œçš„é‡è¦æ„ä¹‰åœ¨äºï¼Œå®ƒæå‡ºäº†ä¸€ç§æ–°çš„å¤šè¯´è¯äººè¯´è¯é£æ ¼å­—å¹•ä»»åŠ¡ï¼ˆSPEECHCAPSï¼‰ï¼Œé€šè¿‡åœ¨æŒ‡ä»¤åŸºç¡€ä¸Šå¯¹é€šç”¨è¯­éŸ³æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œæœ‰æ•ˆæå‡äº†æ¨¡å‹å¯¹è¯´è¯äººå’ŒéŸµå¾‹ä¿¡æ¯çš„ç†è§£èƒ½åŠ›ï¼Œä¸ºåŸºäºæŒ‡ä»¤çš„è¯­éŸ³å¤„ç†æŠ€æœ¯æä¾›äº†æ–°çš„ç ”ç©¶æ€è·¯å’Œæ–¹å‘ã€‚

    - (2): Innovation point: SPEECHCAPSçš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ä¸ªé’ˆå¯¹å¤šè¯´è¯äººè¯´è¯é£æ ¼å­—å¹•çš„ä»»åŠ¡ï¼Œç»“åˆäº†æŒ‡ä»¤è°ƒæ•´å’Œé¢„è®­ç»ƒæŠ€æœ¯ï¼Œä¸ºé€šç”¨è¯­éŸ³æ¨¡å‹çš„è®­ç»ƒæä¾›äº†æ–°çš„è§†è§’å’Œé€”å¾„ï¼›Performance: åœ¨Dynamic-SUPERBæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨è¯´è¯äººå’Œæƒ…æ„Ÿè¯†åˆ«ä»»åŠ¡ä¸Šä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œä½†åœ¨å¤šè¯´è¯äººé—®ç­”ä»»åŠ¡ä¸Šä»å­˜åœ¨ä¸€äº›å›°éš¾ï¼›Workload: è¯¥æ–¹æ³•éœ€è¦æ”¶é›†å¤§é‡å¤šè¯´è¯äººè¯­éŸ³æ•°æ®å¹¶æ„å»ºç›¸åº”çš„å­—å¹•æ•°æ®é›†ï¼Œå¯¹æ•°æ®æ”¶é›†å’Œæ ‡æ³¨çš„å·¥ä½œé‡è¾ƒå¤§ã€‚




<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8021415f823c5ce0acd5bb92d61e09b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e1c7406db684343030a6fdc9a395106.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5d9ab1e6a16acb6ef52191ed789cd35.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5efff236d713d07c1290261d93c716a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f110c7a74d2aae8799ee5d832e200c66.jpg" align="middle">
</details>




## TalkLoRA: Low-Rank Adaptation for Speech-Driven Animation

**Authors:Jack Saunders, Vinay Namboodiri**

Speech-driven facial animation is important for many applications including TV, film, video games, telecommunication and AR/VR. Recently, transformers have been shown to be extremely effective for this task. However, we identify two issues with the existing transformer-based models. Firstly, they are difficult to adapt to new personalised speaking styles and secondly, they are slow to run for long sentences due to the quadratic complexity of the transformer. We propose TalkLoRA to address both of these issues. TalkLoRA uses Low-Rank Adaptation to effectively and efficiently adapt to new speaking styles, even with limited data. It does this by training an adaptor with a small number of parameters for each subject. We also utilise a chunking strategy to reduce the complexity of the underlying transformer, allowing for long sentences at inference time. TalkLoRA can be applied to any transformer-based speech-driven animation method. We perform extensive experiments to show that TalkLoRA archives state-of-the-art style adaptation and that it allows for an order-of-complexity reduction in inference times without sacrificing quality. We also investigate and provide insights into the hyperparameter selection for LoRA fine-tuning of speech-driven facial animation models. 

[PDF](http://arxiv.org/abs/2408.13714v1) 

**Summary**
è¯­éŸ³é©±åŠ¨çš„é¢éƒ¨åŠ¨ç”»å¯¹ç”µè§†ã€ç”µå½±ã€æ¸¸æˆç­‰é¢†åŸŸè‡³å…³é‡è¦ï¼ŒTalkLoRAé€šè¿‡ä½ç§©è‡ªé€‚åº”å’Œåˆ†å—ç­–ç•¥ï¼Œæœ‰æ•ˆè§£å†³ç°æœ‰æ¨¡å‹çš„é—®é¢˜ï¼Œå®ç°é«˜æ•ˆé£æ ¼é€‚åº”å’Œå¿«é€Ÿè¿è¡Œã€‚

**Key Takeaways**
1. è¯­éŸ³é©±åŠ¨çš„é¢éƒ¨åŠ¨ç”»åº”ç”¨å¹¿æ³›ï¼Œtransformeræ¨¡å‹æœ‰æ•ˆä½†å­˜åœ¨é€‚åº”æ€§å’Œé€Ÿåº¦é—®é¢˜ã€‚
2. TalkLoRAé€šè¿‡ä½ç§©è‡ªé€‚åº”é€‚åº”æ–°è¯´è¯é£æ ¼ï¼Œé€‚åº”æ•°æ®æœ‰é™ã€‚
3. å°å‚æ•°é€‚é…å™¨å®ç°é’ˆå¯¹ä¸åŒä¸»é¢˜çš„è®­ç»ƒã€‚
4. åˆ†å—ç­–ç•¥é™ä½transformerå¤æ‚åº¦ï¼Œæ”¯æŒé•¿å¥å¤„ç†ã€‚
5. TalkLoRAé€‚ç”¨äºä»»ä½•åŸºäºtransformerçš„è¯­éŸ³é©±åŠ¨åŠ¨ç”»æ–¹æ³•ã€‚
6. å®éªŒè¯æ˜TalkLoRAå®ç°é£æ ¼é€‚åº”çš„çªç ´ï¼Œä¸”ä¸å½±å“è´¨é‡ã€‚
7. ç ”ç©¶æä¾›LoRAå¾®è°ƒé¢éƒ¨åŠ¨ç”»æ¨¡å‹çš„è¶…å‚æ•°é€‰æ‹©è§è§£ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**



1. Title: TalkLoRA: Low-Rank Adaptation for Speech-Driven Animation
                 (æ ‡é¢˜ï¼šTalkLoRAï¼šç”¨äºè¯­éŸ³é©±åŠ¨åŠ¨ç”»çš„ä½ç§©è‡ªé€‚åº”)

2. Authors: Jack Saunders, Vinay P Namboodiri
                 (ä½œè€…ï¼šJack Saunders, Vinay P Namboodiri)

3. Affiliation: University of Bath
                 (æ‰€å±æœºæ„ï¼šå·´æ–¯å¤§å­¦)

4. Keywords: Speech-Driven Animation, Transformer, Low-Rank Adaptation, Chunking
                 (å…³é”®è¯ï¼šè¯­éŸ³é©±åŠ¨åŠ¨ç”»ï¼ŒTransformerï¼Œä½ç§©è‡ªé€‚åº”ï¼Œåˆ†å—)

5. Urls: https://jsaunders909.github.io/ or https://vinaypn.github.io/
                 (ç½‘å€ï¼šhttps://jsaunders909.github.io/ æˆ– https://vinaypn.github.io/ , Github:None)

6. Summary:

                    - (1):è¯¥æ–‡ç« çš„ç ”ç©¶èƒŒæ™¯æ˜¯è¯­éŸ³é©±åŠ¨é¢éƒ¨åŠ¨ç”»åœ¨ç”µè§†ã€ç”µå½±ã€è§†é¢‘æ¸¸æˆã€ç”µä¿¡å’ŒAR/VRç­‰é¢†åŸŸçš„åº”ç”¨ï¼Œè€ŒTransformeræ¨¡å‹åœ¨æ­¤ä»»åŠ¡ä¸­è¡¨ç°å‡ºæé«˜çš„æœ‰æ•ˆæ€§ã€‚
 
                    - (2):è¿‡å»çš„æ–¹æ³•åŒ…æ‹¬åŸºäºTransformerçš„æ¨¡å‹ï¼Œä½†å®ƒä»¬éš¾ä»¥é€‚åº”æ–°çš„ä¸ªæ€§åŒ–è¯´è¯é£æ ¼ï¼Œä¸”ç”±äºTransformerçš„äºŒæ¬¡å¤æ‚æ€§ï¼Œè¿è¡Œé•¿å¥å­æ—¶é€Ÿåº¦è¾ƒæ…¢ã€‚è¯¥ç ”ç©¶æ–¹æ³•å¾ˆå¥½åœ°è§£å†³äº†è¿™äº›é—®é¢˜ã€‚
 
                    - (3)ï¼šè¯¥æ–‡æå‡ºçš„æ–¹æ³•TalkLoRAä½¿ç”¨ä½ç§©è‡ªé€‚åº”æœ‰æ•ˆåœ°é€‚åº”æ–°çš„è¯´è¯é£æ ¼ï¼Œå³ä½¿æ•°æ®æœ‰é™ã€‚å®ƒé€šè¿‡ä¸ºæ¯ä¸ªä¸»é¢˜è®­ç»ƒå…·æœ‰å°‘é‡å‚æ•°çš„é€‚é…å™¨æ¥å®ç°ã€‚æ­¤å¤–ï¼Œè¿˜åˆ©ç”¨äº†åˆ†å—ç­–ç•¥æ¥é™ä½å¤æ‚æ€§ã€‚
 
                    - (4)ï¼šè¯¥æ–¹æ³•åœ¨è¯­éŸ³é©±åŠ¨åŠ¨ç”»ä»»åŠ¡ä¸Šå®ç°äº†æœ‰æ•ˆçš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨é€‚åº”æ–°è¯´è¯é£æ ¼å’Œæé«˜è¿è¡Œé€Ÿåº¦æ–¹é¢ã€‚å…¶æ€§èƒ½æ”¯æŒäº†ç ”ç©¶ç›®æ ‡ã€‚
7. Methods:

    - (1): æœ¬æ–‡é’ˆå¯¹ç°æœ‰çš„åŸºäºTransformerçš„è¯­éŸ³é©±åŠ¨åŠ¨ç”»ç³»ç»Ÿï¼Œæå‡ºäº†ä¸€ç§åä¸ºTalkLoRAçš„ä½ç§©è‡ªé€‚åº”æ–¹æ³•ï¼Œä»¥æå‡æ¨¡å‹é€‚åº”æ–°è¯´è¯é£æ ¼çš„èƒ½åŠ›å’Œè¿è¡Œé€Ÿåº¦ã€‚

    - (2): TalkLoRAåˆ©ç”¨ä½ç§©è‡ªé€‚åº”ï¼ˆLoRAï¼‰æŠ€æœ¯ï¼Œé€šè¿‡ä¸ºæ¨¡å‹æ·»åŠ å°‘é‡å‚æ•°çš„é€‚é…å™¨ï¼Œå®ç°å¯¹ç°æœ‰æ¨¡å‹çš„ä¸ªæ€§åŒ–è°ƒæ•´ã€‚

    - (3): ä¸ºäº†é™ä½æ¨¡å‹å¤æ‚æ€§ï¼ŒTalkLoRAå¼•å…¥äº†åˆ†å—ç­–ç•¥ï¼Œå°†è¾“å…¥éŸ³é¢‘åˆ†å‰²æˆé‡å çš„å›ºå®šå¤§å°çš„å—ï¼Œå¹¶è¡Œå¤„ç†ï¼Œä»è€Œå‡å°‘Transformerçš„ä¸Šä¸‹æ–‡çª—å£å¤§å°ã€‚

    - (4): åœ¨éŸ³é¢‘ç¼–ç å™¨éƒ¨åˆ†ï¼Œç”±äºå…¶å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼ŒTalkLoRAæ²¡æœ‰åº”ç”¨LoRAæŠ€æœ¯ï¼Œä»¥é¿å…è¿‡åº¦æ‹Ÿåˆã€‚

    - (5): å¯¹äºè§£ç å™¨éƒ¨åˆ†ï¼ŒTalkLoRAå¯ä»¥é€‰æ‹©æ€§åœ°åº”ç”¨LoRAæŠ€æœ¯åˆ°Transformerè§£ç å™¨æˆ–è¿åŠ¨è§£ç å™¨ï¼Œä»¥å®ç°æ¨¡å‹å¯¹å•ä¸ªèº«ä»½çš„é€‚åº”ã€‚

    - (6): é€šè¿‡å®éªŒç¡®å®šäº†LoRAçš„ç§©ï¼ˆrï¼‰çš„æœ€ä½³å€¼ï¼Œä»¥å¹³è¡¡æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›å’Œæ­£åˆ™åŒ–èƒ½åŠ›ã€‚

    - (7): é€šè¿‡è°ƒæ•´æ¨¡å‹æ¶æ„ï¼Œå®ç°Transformerçš„ä¸Šä¸‹æ–‡çª—å£é™åˆ¶ï¼Œä»è€Œæé«˜é•¿åºåˆ—çš„æ¨ç†é€Ÿåº¦ã€‚

    - (8): åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒTalkLoRAä½¿ç”¨AdamWä¼˜åŒ–å™¨å’Œé€‚å½“çš„å­¦ä¹ ç‡ï¼Œå¹¶åœ¨50ä¸ªepochåæ”¶æ•›ã€‚


8. Conclusion:

                    - (1): è¯¥ç ”ç©¶å·¥ä½œçš„é‡è¦æ€§åœ¨äºï¼Œå®ƒæå‡ºäº†ä¸€ç§åä¸ºTalkLoRAçš„ä½ç§©è‡ªé€‚åº”æ–¹æ³•ï¼Œæœ‰æ•ˆæé«˜äº†åŸºäºTransformerçš„è¯­éŸ³é©±åŠ¨åŠ¨ç”»æ¨¡å‹çš„é€‚åº”æ€§å’Œæ¨ç†é€Ÿåº¦ï¼Œè¿™å¯¹äºç”µè§†ã€ç”µå½±ã€è§†é¢‘æ¸¸æˆç­‰é¢†åŸŸçš„åº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚

                    - (2): Innovation point: åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä½ç§©è‡ªé€‚åº”æŠ€æœ¯åº”ç”¨äºè¯­éŸ³é©±åŠ¨åŠ¨ç”»ï¼Œå®ç°äº†å¯¹ç°æœ‰æ¨¡å‹çš„ä¸ªæ€§åŒ–è°ƒæ•´ï¼Œå¹¶é€šè¿‡åˆ†å—ç­–ç•¥é™ä½äº†æ¨¡å‹å¤æ‚æ€§ï¼›Performance: æ€§èƒ½æ–¹é¢ï¼ŒTalkLoRAåœ¨é€‚åº”æ–°è¯´è¯é£æ ¼å’Œæé«˜è¿è¡Œé€Ÿåº¦æ–¹é¢å‡ä¼˜äºç°æœ‰æ¨¡å‹ï¼›Workload: å·¥ä½œé‡æ–¹é¢ï¼ŒTalkLoRAé€šè¿‡ä¼˜åŒ–æ¨¡å‹æ¶æ„å’Œä½¿ç”¨AdamWä¼˜åŒ–å™¨ç­‰æ‰‹æ®µï¼Œä½¿å¾—è®­ç»ƒè¿‡ç¨‹é«˜æ•ˆä¸”æ”¶æ•›ã€‚




<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3313994c278d325c8ef3fb44a5ba2d76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c2db76f55115f8dd725a17800048f2f.jpg" align="middle">
</details>




## Empowering Whisper as a Joint Multi-Talker and Target-Talker Speech   Recognition System

**Authors:Lingwei Meng, Jiawen Kang, Yuejiao Wang, Zengrui Jin, Xixin Wu, Xunying Liu, Helen Meng**

Multi-talker speech recognition and target-talker speech recognition, both involve transcription in multi-talker contexts, remain significant challenges. However, existing methods rarely attempt to simultaneously address both tasks. In this study, we propose a pioneering approach to empower Whisper, which is a speech foundation model, to tackle joint multi-talker and target-talker speech recognition tasks. Specifically, (i) we freeze Whisper and plug a Sidecar separator into its encoder to separate mixed embedding for multiple talkers; (ii) a Target Talker Identifier is introduced to identify the embedding flow of the target talker on the fly, requiring only three-second enrollment speech as a cue; (iii) soft prompt tuning for decoder is explored for better task adaptation. Our method outperforms previous methods on two- and three-talker LibriMix and LibriSpeechMix datasets for both tasks, and delivers acceptable zero-shot performance on multi-talker ASR on AishellMix Mandarin dataset. 

[PDF](http://arxiv.org/abs/2407.09817v2) Accepted to INTERSPEECH 2024

**Summary**
æå‡ºæ–°æ–¹æ³•ï¼Œä½¿Whisperæ¨¡å‹åŒæ—¶å¤„ç†å¤šè¯´è¯è€…å’Œç›®æ ‡è¯´è¯è€…è¯­éŸ³è¯†åˆ«ä»»åŠ¡ã€‚

**Key Takeaways**
- åŒæ—¶å¤„ç†å¤šè¯´è¯è€…å’Œç›®æ ‡è¯´è¯è€…è¯­éŸ³è¯†åˆ«ã€‚
- ä½¿ç”¨Sidecaråˆ†ç¦»å™¨åˆ†ç¦»æ··åˆåµŒå…¥ã€‚
- å¼•å…¥ç›®æ ‡è¯´è¯è€…è¯†åˆ«å™¨ã€‚
- éœ€è¦ç®€çŸ­çš„å£°éŸ³ä½œä¸ºè¯†åˆ«çº¿ç´¢ã€‚
- è½¯æç¤ºè°ƒæ•´è§£ç å™¨ä»¥é€‚åº”ä»»åŠ¡ã€‚
- åœ¨LibriMixå’ŒLibriSpeechMixæ•°æ®é›†ä¸Šä¼˜äºå…ˆå‰æ–¹æ³•ã€‚
- åœ¨AishellMixæ•°æ®é›†ä¸Šå®ç°å¯æ¥å—çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**



1. Title: èµ‹èƒ½Whisperä½œä¸ºè”åˆå¤šè¯´è¯è€…å’Œç›®æ ‡è¯´è¯è€…è¯­éŸ³è¯†åˆ«ç³»ç»Ÿ
                 2. Authors: Lingwei Meng, Jiawen Kang, Yuejiao Wang, Zengrui Jin, Xixin Wu, Xunying Liu, Helen Meng
                 3. Affiliation: é¦™æ¸¯ä¸­æ–‡å¤§å­¦
                 4. Keywords: å¤šè¯´è¯è€…è¯­éŸ³è¯†åˆ«ï¼Œç›®æ ‡è¯´è¯è€…è¯­éŸ³è¯†åˆ«ï¼Œæç¤ºå¾®è°ƒï¼Œé¢†åŸŸè‡ªé€‚åº”
                 5. Urls: https://arxiv.org/abs/2407.09817v2 or https://github.com/LingweiMeng/Whisper-Sidecar
       
                 6. Summary:
                    - (1):è¯¥ç ”ç©¶èƒŒæ™¯æ˜¯å¤šè¯´è¯è€…å’Œç›®æ ‡è¯´è¯è€…è¯­éŸ³è¯†åˆ«åœ¨å¤šè¯´è¯è€…ç¯å¢ƒä¸‹çš„è½¬å½•ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚
 
                    - (2):è¿‡å»çš„æ–¹æ³•åŒ…æ‹¬ä¼ ç»Ÿçš„çº§è”ç³»ç»Ÿå’Œç«¯åˆ°ç«¯æ¨¡å‹ã€‚çº§è”ç³»ç»Ÿé€šå¸¸ç”±äºä¼˜åŒ–ç›®æ ‡ä¸åŒ¹é…è€Œè¡¨ç°æœ‰é™ã€‚ç«¯åˆ°ç«¯æ¨¡å‹éœ€è¦å¤æ‚çš„è®­ç»ƒç­–ç•¥ï¼Œå¦‚ç½®æ¢ä¸å˜æ€§è®­ç»ƒï¼ˆPITï¼‰ã€å¯å‘å¼é”™è¯¯åˆ†é…è®­ç»ƒï¼ˆHEATï¼‰å’Œåºåˆ—è¾“å‡ºè®­ç»ƒï¼ˆSOTï¼‰ï¼Œä¸”é€šå¸¸éœ€è¦ä»å¤´å¼€å§‹è®­ç»ƒæˆ–å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå®Œå…¨å¾®è°ƒã€‚è¿™äº›æ–¹æ³•è™½ç„¶å–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†æœªèƒ½å……åˆ†åˆ©ç”¨æ ‡å‡†å•è¯´è¯è€…ASRä¸­å¼€å‘çš„ç°æœ‰è¿›æ­¥ã€‚è¯¥ç ”ç©¶æ–¹æ³•åŠ¨æœºæ˜ç¡®ï¼Œæ—¨åœ¨æé«˜å¤šè¯´è¯è€…å’Œç›®æ ‡è¯´è¯è€…è¯­éŸ³è¯†åˆ«çš„æ€§èƒ½ã€‚
 
                    - (3)ï¼šè¯¥è®ºæ–‡æå‡ºçš„æ–¹æ³•åŒ…æ‹¬ï¼šå†»ç»“Whisperçš„æƒé‡ï¼Œå°†å…¶ç¼–ç å™¨ä¸­çš„Sidecaråˆ†ç¦»å™¨ç”¨äºå¤šè¯´è¯è€…åµŒå…¥åˆ†ç¦»ï¼›å¼•å…¥ç›®æ ‡è¯´è¯è€…è¯†åˆ«å™¨ï¼ˆTTIï¼‰æ¨¡å—ä»¥å®æ—¶è¯†åˆ«ç›®æ ‡è¯´è¯è€…çš„åµŒå…¥æµï¼Œåªéœ€3ç§’é’Ÿçš„æ³¨å†Œè¯­éŸ³ä½œä¸ºæç¤ºï¼›æ¢ç´¢è§£ç å™¨çš„è½¯æç¤ºå¾®è°ƒä»¥æ›´å¥½åœ°é€‚åº”ä»»åŠ¡ã€‚
 
                    - (4)ï¼šè¯¥æ–¹æ³•åœ¨ä¸¤ä¸ªå’Œä¸‰ä¸ªè¯´è¯è€…çš„LibriMixå’ŒLibriSpeechMixæ•°æ®é›†ä¸Šå®ç°äº†é¢†å…ˆçš„æ€§èƒ½ï¼Œåœ¨AishellMixï¼ˆæ™®é€šè¯ï¼‰æ•°æ®é›†ä¸Šè¾¾åˆ°äº†å¯æ¥å—çš„é›¶æ ·æœ¬å¤šè¯´è¯è€…ASRæ€§èƒ½ï¼Œæ”¯æŒäº†å…¶ç ”ç©¶ç›®æ ‡ã€‚
7. Methods:

    - (1): é‡‡ç”¨Whisperä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œå¹¶å¼•å…¥Sidecaråˆ†ç¦»å™¨å°†å¤šè¯´è¯è€…åµŒå…¥åˆ†ç¦»ï¼Œä»¥å¤„ç†å¤šè¯´è¯è€…è¯­éŸ³è¯†åˆ«ä»»åŠ¡ã€‚
  
    - (2): è®¾è®¡ç›®æ ‡è¯´è¯è€…è¯†åˆ«å™¨ï¼ˆTTIï¼‰æ¨¡å—ï¼Œé€šè¿‡3ç§’çš„æ³¨å†Œè¯­éŸ³ä½œä¸ºæç¤ºï¼Œå®æ—¶è¯†åˆ«ç›®æ ‡è¯´è¯è€…çš„åµŒå…¥æµã€‚
  
    - (3): å¯¹è§£ç å™¨è¿›è¡Œè½¯æç¤ºå¾®è°ƒï¼Œä»¥æ›´å¥½åœ°é€‚åº”å¤šè¯´è¯è€…å’Œç›®æ ‡è¯´è¯è€…è¯­éŸ³è¯†åˆ«ä»»åŠ¡ã€‚
  
    - (4): åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œ80%çš„æ¦‚ç‡è¿›è¡Œå¤šè¯´è¯è€…ASRè®­ç»ƒï¼Œ20%çš„æ¦‚ç‡è¿›è¡ŒåŒ…å«æ³¨å†Œè¯­éŸ³çš„è”åˆå¤šè¯´è¯è€…å’Œç›®æ ‡è¯´è¯è€…ASRè®­ç»ƒã€‚
  
    - (5): ä½¿ç”¨ç½®æ¢ä¸å˜æ€§è®­ç»ƒï¼ˆPITï¼‰è§£å†³æ ‡ç­¾æ¨¡ç³Šæ€§é—®é¢˜ï¼Œå¹¶è®¡ç®—æœ€ç»ˆçš„æŸå¤±å‡½æ•°ï¼ŒåŒ…æ‹¬ASRæŸå¤±å’ŒTTIçš„äº¤å‰ç†µæŸå¤±ã€‚


8. Conclusion:

    - (1):è¯¥ç ”ç©¶å…·æœ‰æ˜¾è‘—æ„ä¹‰ï¼Œå› ä¸ºå®ƒä¸ºå¤šè¯´è¯è€…å’Œç›®æ ‡è¯´è¯è€…è¯­éŸ³è¯†åˆ«é¢†åŸŸæä¾›äº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡å¼•å…¥Whisperæ¨¡å‹å’ŒSidecaråˆ†ç¦»å™¨ï¼Œæœ‰æ•ˆæå‡äº†å¤šè¯´è¯è€…ç¯å¢ƒä¸‹è¯­éŸ³è¯†åˆ«çš„å‡†ç¡®æ€§å’Œå®æ—¶æ€§ã€‚

    - (2):Innovation point: è¯¥æ–‡ç« çš„åˆ›æ–°ç‚¹åœ¨äºå°†Whisperæ¨¡å‹ä¸Sidecaråˆ†ç¦»å™¨ç»“åˆï¼Œå¹¶è®¾è®¡ç›®æ ‡è¯´è¯è€…è¯†åˆ«å™¨ï¼ˆTTIï¼‰æ¨¡å—ï¼Œå®ç°äº†é«˜æ•ˆçš„å¤šè¯´è¯è€…å’Œç›®æ ‡è¯´è¯è€…è¯­éŸ³è¯†åˆ«ï¼›Performance: åœ¨LibriMixå’ŒLibriSpeechMixæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†é¢†å…ˆçš„æ€§èƒ½ï¼Œåœ¨AishellMixæ•°æ®é›†ä¸Šä¹Ÿå–å¾—äº†å¯æ¥å—çš„é›¶æ ·æœ¬å¤šè¯´è¯è€…ASRæ€§èƒ½ï¼›Workload: è¯¥æ–¹æ³•åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œé™ä½äº†è®­ç»ƒå’Œè¿è¡Œçš„å·¥ä½œé‡ï¼Œé€šè¿‡è½¯æç¤ºå¾®è°ƒå’Œè§£ç å™¨ä¼˜åŒ–ï¼Œç®€åŒ–äº†è®­ç»ƒç­–ç•¥ã€‚




<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ad0809bf1f2a0e13bfb58fed883c328f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4760bb1b1f83c77ff470a2676d9247aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ba94c08ea3020d878a6417b75885d8b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbbcd66af9e5a0c566946800bba17655.jpg" align="middle">
</details>




