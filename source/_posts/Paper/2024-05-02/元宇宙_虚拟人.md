
---
title: 元宇宙/虚拟人
date: 2024-05-02 10:00:16
author: Kedreamix
cover: https://pica.zhimg.com/v2-37516691b13dabbabb3b74ea46b402d8.jpg
categories: Paper
tags:
    - 元宇宙/虚拟人
description: 元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2024-05-02  EMOPortraits Emotion-enhanced Multimodal One-shot Head Avatars  
keywords: 元宇宙/虚拟人
toc:
toc_number: false
toc_style_simple: true
copyright:
copyright_author:
copyright_author_href:
copyright_url:
copyright_info:
mathjax: true
katex:
aplayer:
highlight_shrink:
aside:
---

>⚠️ 以下所有内容总结都来自于 Google的大语言模型[Gemini-Pro](https://ai.google.dev/)的能力，如有错误，仅供参考，谨慎使用
>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！
>💗 如果您觉得我们的项目对您有帮助 [ChatPaperFree](https://github.com/Kedreamix/ChatPaperFree) ，还请您给我们一些鼓励！⭐️ [HuggingFace免费体验](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)

# 2024-05-02 更新


## EMOPortraits: Emotion-enhanced Multimodal One-shot Head Avatars

**Authors:Nikita Drobyshev, Antoni Bigata Casademunt, Konstantinos Vougioukas, Zoe Landgraf, Stavros Petridis, Maja Pantic**

Head avatars animated by visual signals have gained popularity, particularly in cross-driving synthesis where the driver differs from the animated character, a challenging but highly practical approach. The recently presented MegaPortraits model has demonstrated state-of-the-art results in this domain. We conduct a deep examination and evaluation of this model, with a particular focus on its latent space for facial expression descriptors, and uncover several limitations with its ability to express intense face motions. To address these limitations, we propose substantial changes in both training pipeline and model architecture, to introduce our EMOPortraits model, where we:   Enhance the model's capability to faithfully support intense, asymmetric face expressions, setting a new state-of-the-art result in the emotion transfer task, surpassing previous methods in both metrics and quality.   Incorporate speech-driven mode to our model, achieving top-tier performance in audio-driven facial animation, making it possible to drive source identity through diverse modalities, including visual signal, audio, or a blend of both.   We propose a novel multi-view video dataset featuring a wide range of intense and asymmetric facial expressions, filling the gap with absence of such data in existing datasets. 

[PDF](http://arxiv.org/abs/2404.19110v1) 

**Summary**
虚拟人头部通过视觉信号动画驱动，在跨驱动合成中尤其受欢迎，这是一种极具挑战但非常实用的方法。

**Key Takeaways**
- MegaPortraits 模型在表情描述符的潜在空间方面存在局限性，无法表达强烈的面部动作。
- EMOPortraits 模型对训练管道和模型架构进行了重大更改，以增强模型对强烈、不对称面部表情的忠实支持。
- EMOPortraits 模型在情感转移任务中取得了新的最先进的结果，在指标和质量方面都超越了以前的方法。
- EMOPortraits 模型结合了基于语音的模式，在音频驱动的面部动画中实现了顶级性能。
- EMOPortraits 模型支持通过视觉信号、音频或两者结合的多种方式驱动源身份。
- 提出一个新的多视图视频数据集，包含广泛的强烈和不对称面部表情。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>
<p>Title: EMOPortraits: 情绪增强多模态一发头像</p>
</li>
<li>
<p>Authors: Nikita Drobyshev, Antoni Bigata Casademunt, Konstantinos Vougioukas, Zoe Landgraf, Stavros Petridis, Maja Pantic</p>
</li>
<li>
<p>Affiliation: 帝国理工学院</p>
</li>
<li>
<p>Keywords: 头部化身、面部表情、情感传递、语音驱动、多模态</p>
</li>
<li>
<p>Urls: Paper: https://arxiv.org/abs/2404.19110v1, Github: None</p>
</li>
<li>
<p>Summary:</p>
</li>
</ol>
<p>(1): 研究背景：头部化身动画在跨驱动合成中越来越流行，其中驱动者与动画角色不同，这是一种具有挑战性但非常实用的方法。最近提出的 MegaPortraits 模型在这个领域展示了最先进的结果。</p>
<p>(2): 过去的方法：研究人员对 MegaPortraits 模型进行了深入的检查和评估，特别关注其面部表情描述符的潜在空间，发现其表达强烈面部动作的能力存在一些限制。</p>
<p>(3): 研究方法：为了解决这些限制，研究人员在训练管道和模型架构中提出了实质性的改变，引入了 EMOPortraits 模型，其中：
   - 增强了模型忠实支持强烈、不对称面部表情的能力，在情感传递任务中设定了新的最先进结果，在指标和质量上都超过了以前的方法。
   - 将语音驱动模式纳入模型，在音频驱动的面部动画中实现了顶级性能，使得可以通过视觉信号、音频或两者混合等多种方式驱动源身份。
   - 提出了一组新的多视角视频数据集，其中包含广泛的强烈和不对称面部表情，填补了现有数据集中缺乏此类数据的情况。</p>
<p>(4): 性能和效果：在情感传递任务上，EMOPortraits 模型在指标和质量上都超过了以前的方法，设定了新的最先进的结果。在音频驱动的面部动画中，该模型也取得了顶级性能。这些性能支持了研究人员的目标，即开发一个能够通过多种方式驱动源身份的头部化身动画模型。</p>
<ol>
<li>
<p>方法：
    (1): 对 MegaPortraits 模型进行深入检查和评估，发现其在表达强烈面部动作方面存在限制；
    (2): 在训练管道和模型架构中提出实质性改变，引入 EMOPortraits 模型；
    (3): 增强模型忠实支持强烈、不对称面部表情的能力；
    (4): 将语音驱动模式纳入模型，实现音频驱动的面部动画顶级性能；
    (5): 提出多视角视频数据集，填补现有数据集中缺乏强烈和不对称面部表情数据的情况。</p>
</li>
<li>
<p>结论：</p>
</li>
</ol>
<p>（1）本文提出了一种新颖的方法 EMOPortraits，用于创建神经头像，在图像驱动、跨身份情绪转换方面具有卓越的性能。我们的语音驱动模式使得可以通过多种条件（视频、音频、头部运动）来驱动面部动画。我们收集了 FEED 数据集，我们相信这将成为从事多元化以人为中心研究的研究人员的宝贵资产。然而，我们的方法也存在一些局限性。它不会生成头像的身体或肩膀，限制了一些用例。我们目前将我们的输出与源图像主体集成在一起。此外，该模型有时难以进行准确的表情转换，并且在头部大幅旋转时性能不佳。这些挑战对于未来的增强至关重要，并且仍然是我们正在进行的研究工作的核心。</p>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ae09218eb625859aeda612581ba59975.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76da3d8060bb28f6e1488ffdcf42c493.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07199851d15b47c4d1a719b68cd3f240.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29a1efddd95063c164480f3a84bf5f72.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc5fc34eb617f15c5ecceee7d25f9f5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7948dbe17eb67516e7078da09fc10ae.jpg" align="middle">
</details>




## MeGA: Hybrid Mesh-Gaussian Head Avatar for High-Fidelity Rendering and   Head Editing

**Authors:Cong Wang, Di Kang, He-Yi Sun, Shen-Han Qian, Zi-Xuan Wang, Linchao Bao, Song-Hai Zhang**

Creating high-fidelity head avatars from multi-view videos is a core issue for many AR/VR applications. However, existing methods usually struggle to obtain high-quality renderings for all different head components simultaneously since they use one single representation to model components with drastically different characteristics (e.g., skin vs. hair). In this paper, we propose a Hybrid Mesh-Gaussian Head Avatar (MeGA) that models different head components with more suitable representations. Specifically, we select an enhanced FLAME mesh as our facial representation and predict a UV displacement map to provide per-vertex offsets for improved personalized geometric details. To achieve photorealistic renderings, we obtain facial colors using deferred neural rendering and disentangle neural textures into three meaningful parts. For hair modeling, we first build a static canonical hair using 3D Gaussian Splatting. A rigid transformation and an MLP-based deformation field are further applied to handle complex dynamic expressions. Combined with our occlusion-aware blending, MeGA generates higher-fidelity renderings for the whole head and naturally supports more downstream tasks. Experiments on the NeRSemble dataset demonstrate the effectiveness of our designs, outperforming previous state-of-the-art methods and supporting various editing functionalities, including hairstyle alteration and texture editing. 

[PDF](http://arxiv.org/abs/2404.19026v1) Project page: https://conallwang.github.io/MeGA_Pages/

**Summary**
多模态表情虚拟人头部建模方法 MeGA: 使用网格融合高斯模型，为不同头部组件提供更合适的表征方法。

**Key Takeaways**
- 提出一种混合网格-高斯虚拟人头部建模方案 MeGA。
- 选择增强型 FLAME 网格作为面部表征，并预测 UV 位移图以提供逐顶点偏移，实现个性化几何细节。
- 采用延迟神经渲染技术获取面部颜色，并将神经纹理分解为三个有意义的部分，实现逼真的渲染。
- 使用 3D 高斯溅射构建静态规范头发，利用刚体变换和基于 MLP 的变形场处理复杂的动态表情。
- 结合遮挡感知融合，MeGA 为整个头部生成更高保真度的渲染，并支持更多下游任务。
- 在 NeRSemble 数据集上的实验表明，MeGA 优于现有的最先进方法，并支持发型改变和纹理编辑等多种编辑功能。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>
<p>Title: MeGA：混合网格-高斯头部头像（中文翻译：MeGA：用于高保真渲染和头部编辑的混合网格-高斯头部头像）</p>
</li>
<li>
<p>Authors: Cong Wang, Di Kang, He-Yi Sun, Shen-Han Qian, Zi-Xuan Wang, Linchao Bao, Song-Hai Zhang</p>
</li>
<li>
<p>Affiliation: 清华大学（中文翻译：清华大学）</p>
</li>
<li>
<p>Keywords: Head Avatar, High-Fidelity Rendering, Head Editing, Mesh, Gaussian</p>
</li>
<li>
<p>Urls: Paper: https://arxiv.org/abs/2404.19026 , Github: None</p>
</li>
<li>
<p>Summary:</p>
</li>
</ol>
<p>（1）：研究背景：高保真头部头像的创建是 AR/VR 应用中的核心问题，但现有方法通常难以同时为所有不同的头部组件获得高质量的渲染，因为它们使用单一表示来建模具有截然不同特征的组件（例如，皮肤与头发）。</p>
<p>（2）：过去的方法：现有方法探索了基于网格、基于 NeRF 和基于 3D 高斯的表示，取得了显着进展。然而，由于人类头部是一个包含具有截然不同特征的组件（例如，皮肤与头发）的复杂“物体”，因此可能不存在可以同时很好地建模所有这些组件的单一表示。</p>
<p>（3）：研究方法：本文提出了一种混合网格-高斯头部头像（MeGA），它使用更合适的表示来建模不同的头部组件。具体来说，我们选择一个增强的 FLAME 网格作为我们的面部表示，并预测一个 UV 置换贴图来提供每个顶点的偏移量，以改进个性化的几何细节。为了实现逼真的渲染，我们使用延迟神经渲染获取面部颜色，并将神经纹理分解为三个有意义的部分。对于头发建模，我们首先使用 3D 高斯点云构建静态规范头发。然后应用刚性变换和基于 MLP 的变形场来处理复杂的动态表情。结合我们的遮挡感知混合，MeGA 为整个头部生成更高保真的渲染，并自然地支持更多下游任务。</p>
<p>（4）：任务与性能：在 NeRSemble 数据集上的实验表明了我们设计的有效性，优于以前的最先进方法，并支持各种编辑功能，包括发型改变和纹理编辑。</p>
<ol>
<li>
<p>Methods: </p>
<pre><code>            (1):提出混合网格-高斯头部头像（MeGA），使用更合适的表示来建模不同的头部组件；

            (2):选择增强的 FLAME 网格作为面部表示，预测 UV 置换贴图提供顶点偏移量，改进个性化几何细节；

            (3):使用延迟神经渲染获取面部颜色，将神经纹理分解为三个有意义的部分，实现逼真渲染；

            (4):使用 3D 高斯点云构建静态规范头发，应用刚性变换和基于 MLP 的变形场处理表情；

            (5):结合遮挡感知混合，生成更高保真的渲染，支持发型改变和纹理编辑等下游任务。
</code></pre>
</li>
<li>
<p>结论：</p>
</li>
</ol>
<p>（1）：本文提出了混合网格-高斯头部头像（MeGA），它使用神经网格进行面部建模，使用 3DGS 进行头发建模。为了获得高质量的面部模型，我们增强了 FLAME 网格并解码了一个 UV 置换贴图以获得几何细节。面部颜色是从神经纹理贴图中解码的，该贴图由解耦漫反射纹理 ˆTdi、视点相关纹理 ˆTv 和动态纹理 ˆTdy 组成。为了获得高质量的头发模型，我们构建了一个静态 3DGS 头发，并采用刚性变换结合基于 MLP 的变形场进行动画处理。最终的渲染是通过将头发和头部部分与我们的遮挡感知混合模块混合获得的。此外，MeGA 自然支持各种编辑功能，包括发型更改和纹理编辑。</p>
<p>（2）：创新点：提出混合网格-高斯头部头像（MeGA），使用更合适的表示来建模不同的头部组件；性能：在 NeRSemble 数据集上的实验表明了我们设计的有效性，优于以前的最先进方法；工作量：.......</p>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-37516691b13dabbabb3b74ea46b402d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfaebdbe659151ed19833d736cf99b64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-179dcf87ee24bc1a697ea323e2d146c3.jpg" align="middle">
</details>




