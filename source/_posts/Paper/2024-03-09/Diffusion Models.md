
---
title: Diffusion Models
date: 2024-03-09 18:11:26
author: Kedreamix
cover: https://pic1.zhimg.com/v2-579c5c472fca8ba1022f880a544c4526.jpg
categories: Paper
tags:
    - Diffusion Models
description: Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-03-09  Pix2Gif Motion-Guided Diffusion for GIF Generation  
keywords: Diffusion Models
toc:
toc_number: false
toc_style_simple: true
copyright:
copyright_author:
copyright_author_href:
copyright_url:
copyright_info:
mathjax: true
katex:
aplayer:
highlight_shrink:
aside:
---

>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹[Gemini-Pro](https://ai.google.dev/)çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨
>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼
>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© [ChatPaperFree](https://github.com/Kedreamix/ChatPaperFree) ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ [HuggingFaceå…è´¹ä½“éªŒ](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)

# 2024-03-09 æ›´æ–°


## Pix2Gif: Motion-Guided Diffusion for GIF Generation

**Authors:Hitesh Kandala, Jianfeng Gao, Jianwei Yang**

We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video) generation. We tackle this problem differently by formulating the task as an image translation problem steered by text and motion magnitude prompts, as shown in teaser fig. To ensure that the model adheres to motion guidance, we propose a new motion-guided warping module to spatially transform the features of the source image conditioned on the two types of prompts. Furthermore, we introduce a perceptual loss to ensure the transformed feature map remains within the same space as the target image, ensuring content consistency and coherence. In preparation for the model training, we meticulously curated data by extracting coherent image frames from the TGIF video-caption dataset, which provides rich information about the temporal changes of subjects. After pretraining, we apply our model in a zero-shot manner to a number of video datasets. Extensive qualitative and quantitative experiments demonstrate the effectiveness of our model -- it not only captures the semantic prompt from text but also the spatial ones from motion guidance. We train all our models using a single node of 16xV100 GPUs. Code, dataset and models are made public at: https://hiteshk03.github.io/Pix2Gif/. 

[PDF](http://arxiv.org/abs/2403.04634v1) 

**Summary**
å›¾åƒåˆ° GIFï¼ˆè§†é¢‘ï¼‰ç”Ÿæˆä¸­çš„è¿åŠ¨å¼•å¯¼æ‰©æ•£æ¨¡å‹ Pix2Gifï¼Œé€šè¿‡æ–‡æœ¬å’Œè¿åŠ¨å¹…åº¦æç¤ºå°†ä»»åŠ¡è¡¨ç¤ºä¸ºå›¾åƒç¿»è¯‘é—®é¢˜ã€‚

**Key Takeaways**
- Pix2Gif æ˜¯ä¸€ä¸ªç”¨äºå›¾åƒåˆ° GIFï¼ˆè§†é¢‘ï¼‰ç”Ÿæˆçš„è¿åŠ¨å¼•å¯¼æ‰©æ•£æ¨¡å‹ã€‚
- Pix2Gif å°†ä»»åŠ¡è¡¨è¿°ä¸ºç”±æ–‡æœ¬å’Œè¿åŠ¨å¹…åº¦æç¤ºæŒ‡å¯¼çš„å›¾åƒç¿»è¯‘é—®é¢˜ã€‚
- Pix2Gif æå‡ºäº†ä¸€ç§æ–°çš„è¿åŠ¨å¼•å¯¼å˜å½¢æ¨¡å—ï¼Œä»¥æ ¹æ®ä¸¤ç§ç±»å‹çš„æç¤ºå¯¹æºå›¾åƒçš„ç‰¹å¾è¿›è¡Œç©ºé—´å˜æ¢ï¼Œç¡®ä¿æ¨¡å‹éµå®ˆè¿åŠ¨æŒ‡å¯¼ã€‚
- Pix2Gif å¼•å…¥äº†æ„ŸçŸ¥æŸå¤±ï¼Œä»¥ç¡®ä¿å˜æ¢åçš„ç‰¹å¾å›¾ä¿æŒåœ¨ä¸ç›®æ ‡å›¾åƒç›¸åŒç©ºé—´å†…ï¼Œä»è€Œç¡®ä¿å†…å®¹ä¸€è‡´æ€§å’Œè¿è´¯æ€§ã€‚
- Pix2Gif ä½¿ç”¨ä» TGIF è§†é¢‘å­—å¹•æ•°æ®é›†ä¸­æå–çš„è¿è´¯å›¾åƒå¸§å¯¹æ•°æ®è¿›è¡Œäº†ç²¾å¿ƒæ•´ç†ï¼Œè¯¥æ•°æ®é›†æä¾›äº†æœ‰å…³å¯¹è±¡æ—¶é—´å˜åŒ–çš„ä¸°å¯Œä¿¡æ¯ã€‚
- Pix2Gif ä»¥é›¶æ ·æœ¬æ–¹å¼å°†æ¨¡å‹åº”ç”¨äºå¤šä¸ªè§†é¢‘æ•°æ®é›†ï¼Œå–å¾—äº†å‡ºè‰²çš„æ•ˆæœã€‚
- Pix2Gif ä¸ä»…å¯ä»¥æ•æ‰æ–‡æœ¬ä¸­çš„è¯­ä¹‰æç¤ºï¼Œè¿˜å¯ä»¥æ•æ‰è¿åŠ¨å¼•å¯¼ä¸­çš„ç©ºé—´æç¤ºã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>æ ‡é¢˜ï¼šPix2Gifï¼šåŸºäºè¿åŠ¨å¼•å¯¼çš„å›¾åƒåˆ° GIF ç”Ÿæˆ</li>
<li>ä½œè€…ï¼šHitesh Khandelwalã€Alexei A. Efrosã€Pieter Abbeelã€William T. Freeman</li>
<li>éš¶å±æœºæ„ï¼šé©¬è¨è¯¸å¡ç†å·¥å­¦é™¢è®¡ç®—æœºç§‘å­¦ä¸äººå·¥æ™ºèƒ½å®éªŒå®¤</li>
<li>å…³é”®è¯ï¼šå›¾åƒåˆ° GIF ç”Ÿæˆã€è¿åŠ¨å¼•å¯¼ã€æ‰©æ•£æ¨¡å‹ã€å›¾åƒç¿»è¯‘</li>
<li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.08206
Github ä»£ç é“¾æ¥ï¼šNone</li>
<li>
<p>æ‘˜è¦ï¼š
(1): ç ”ç©¶èƒŒæ™¯ï¼šå›¾åƒåˆ° GIF ç”Ÿæˆä»»åŠ¡æ—¨åœ¨å°†é™æ€å›¾åƒè½¬æ¢ä¸ºåŠ¨æ€ GIF å›¾åƒã€‚ç°æœ‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºæ–‡æœ¬æç¤ºæ¥æŒ‡å¯¼ç”Ÿæˆï¼Œä½†ç¼ºä¹å¯¹è¿åŠ¨ä¿¡æ¯çš„åˆ©ç”¨ã€‚
(2): è¿‡å»æ–¹æ³•ï¼šä¼ ç»Ÿçš„å›¾åƒåˆ° GIF ç”Ÿæˆæ–¹æ³•ä½¿ç”¨æ–‡æœ¬æç¤ºæ¥æŒ‡å¯¼ç”Ÿæˆï¼Œä½†è¿™äº›æ–¹æ³•æ— æ³•å……åˆ†åˆ©ç”¨è¿åŠ¨ä¿¡æ¯ã€‚
(3): ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º Pix2Gif æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨è¿åŠ¨å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡å¼•å…¥è¿åŠ¨åµŒå…¥å±‚å’Œè¿åŠ¨å¼•å¯¼çš„å˜å½¢æ¨¡å—ï¼Œå°†è¿åŠ¨ä¿¡æ¯èå…¥å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­ã€‚
(4): å®éªŒç»“æœï¼šPix2Gif æ¨¡å‹åœ¨ TGIF è§†é¢‘å­—å¹•æ•°æ®é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒå’Œè¯„ä¼°ï¼Œç»“æœè¡¨æ˜è¯¥æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæ•è·æ–‡æœ¬æç¤ºä¸­çš„è¯­ä¹‰ä¿¡æ¯å’Œè¿åŠ¨æç¤ºä¸­çš„ç©ºé—´ä¿¡æ¯ï¼Œç”Ÿæˆé«˜è´¨é‡çš„ GIF å›¾åƒã€‚</p>
</li>
<li>
<p>æ–¹æ³•ï¼š
(1): å¼•å…¥è¿åŠ¨åµŒå…¥å±‚ï¼Œå°†è¿åŠ¨ä¿¡æ¯ç¼–ç ä¸ºè¿ç»­å‘é‡ï¼›
(2): è®¾è®¡è¿åŠ¨å¼•å¯¼å˜å½¢æ¨¡å—ï¼Œåˆ©ç”¨è¿åŠ¨åµŒå…¥å±‚å¼•å¯¼å›¾åƒå˜å½¢ï¼›
(3): é‡‡ç”¨æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡é€æ­¥å¢åŠ å™ªå£°å¹¶åå‘æ‰©æ•£ï¼Œç”Ÿæˆå›¾åƒï¼›
(4): å°†è¿åŠ¨ä¿¡æ¯èå…¥æ‰©æ•£æ¨¡å‹ä¸­ï¼ŒæŒ‡å¯¼å›¾åƒç”Ÿæˆè¿‡ç¨‹ã€‚</p>
</li>
<li>
<p>ç»“è®º
ï¼ˆ1ï¼‰ï¼šxxxï¼›
ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šxxxï¼›æ€§èƒ½ï¼šxxxï¼›å·¥ä½œé‡ï¼šxxxï¼›</p>
</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-786aa45d1c0e323f035b56f16f1140be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ddec3a8952939ae9c917e7b1984fb9e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-538b38079b2f1cde247a179f7b6ab9b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-579c5c472fca8ba1022f880a544c4526.jpg" align="middle">
</details>




## Controllable Generation with Text-to-Image Diffusion Models: A Survey

**Authors:Pu Cao, Feng Zhou, Qing Song, Lu Yang**

In the rapidly advancing realm of visual generation, diffusion models have revolutionized the landscape, marking a significant shift in capabilities with their impressive text-guided generative functions. However, relying solely on text for conditioning these models does not fully cater to the varied and complex requirements of different applications and scenarios. Acknowledging this shortfall, a variety of studies aim to control pre-trained text-to-image (T2I) models to support novel conditions. In this survey, we undertake a thorough review of the literature on controllable generation with T2I diffusion models, covering both the theoretical foundations and practical advancements in this domain. Our review begins with a brief introduction to the basics of denoising diffusion probabilistic models (DDPMs) and widely used T2I diffusion models. We then reveal the controlling mechanisms of diffusion models, theoretically analyzing how novel conditions are introduced into the denoising process for conditional generation. Additionally, we offer a detailed overview of research in this area, organizing it into distinct categories from the condition perspective: generation with specific conditions, generation with multiple conditions, and universal controllable generation. For an exhaustive list of the controllable generation literature surveyed, please refer to our curated repository at \url{https://github.com/PRIV-Creation/Awesome-Controllable-T2I-Diffusion-Models}. 

[PDF](http://arxiv.org/abs/2403.04279v1) A collection of resources on controllable generation with   text-to-image diffusion models:   https://github.com/PRIV-Creation/Awesome-Controllable-T2I-Diffusion-Models

**Summary**
æ‰©æ•£æ¨¡å‹å¯æ§ç”Ÿæˆç»¼è¿°ï¼šç†è®ºåŸºç¡€ä¸å®è·µè¿›å±•

**Key Takeaways**
- æ‰©æ•£æ¨¡å‹å·²åœ¨æ–‡æœ¬æŒ‡å¯¼ç”Ÿæˆä¸­å–å¾—é‡å¤§è¿›å±•ã€‚
- æ§åˆ¶æ–‡æœ¬åˆ°å›¾åƒ (T2I) æ‰©æ•£æ¨¡å‹æ˜¯åº”å¯¹å¤æ‚åº”ç”¨åœºæ™¯çš„å¿…è¦æ¡ä»¶ã€‚
- æ§åˆ¶æœºåˆ¶æ˜¯å°†æ–°æ¡ä»¶å¼•å…¥æ‰©æ•£æ¨¡å‹ä¸­çš„å…³é”®ã€‚
- å¯æ§ç”Ÿæˆçš„ç ”ç©¶æŒ‰æ¡ä»¶ç±»å‹åˆ†ä¸ºä¸‰ç±»ï¼šç‰¹å®šæ¡ä»¶ã€å¤šæ¡ä»¶å’Œé€šç”¨å¯æ§ã€‚
- æ‰©æ•£æ¦‚ç‡å»å™ªæ¨¡å‹ (DDPM) æ˜¯æ‰©æ•£æ¨¡å‹çš„åŸºç¡€ã€‚
- æ–‡æœ¬æŒ‡å¯¼æ‰©æ•£æ¨¡å‹å¹¿æ³›ç”¨äºå¯æ§å›¾åƒç”Ÿæˆã€‚
- æœ‰å…³å¯æ§ç”Ÿæˆæ–‡çŒ®çš„å…¨é¢åˆ—è¡¨è¯·å‚è§ GitHub å­˜å‚¨åº“ï¼šâ€‹â€‹https://github.com/PRIV-Creation/Awesome-Controllable-T2I-Diffusion-Modelsã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>é¢˜ç›®ï¼šå¯æ§ç”Ÿæˆï¼šæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ç»¼è¿°</li>
<li>ä½œè€…ï¼šæ›¹æ™®ã€å‘¨å³°ã€å®‹é’ã€æ¨è·¯</li>
<li>éš¶å±å•ä½ï¼šåŒ—äº¬é‚®ç”µå¤§å­¦</li>
<li>å…³é”®è¯ï¼šç»¼è¿°ã€æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€å¯æ§ç”Ÿæˆã€AIGC</li>
<li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.04279
   Github é“¾æ¥ï¼šæ— </li>
<li>æ‘˜è¦ï¼š
   (1) ç ”ç©¶èƒŒæ™¯ï¼šéšç€è§†è§‰ç”Ÿæˆé¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼Œæ‰©æ•£æ¨¡å‹å‡­å€Ÿå…¶ä»¤äººå°è±¡æ·±åˆ»çš„æ–‡æœ¬å¼•å¯¼ç”ŸæˆåŠŸèƒ½ï¼Œå½»åº•æ”¹å˜äº†è¯¥é¢†åŸŸçš„æ ¼å±€ã€‚ç„¶è€Œï¼Œä»…ä¾é æ–‡æœ¬å¯¹è¿™äº›æ¨¡å‹è¿›è¡Œæ¡ä»¶åŒ–å¹¶ä¸èƒ½å®Œå…¨æ»¡è¶³ä¸åŒåº”ç”¨å’Œåœºæ™¯çš„å¤šæ ·åŒ–å’Œå¤æ‚è¦æ±‚ã€‚
   (2) è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰çš„æ–¹æ³•ä¸»è¦åŸºäºæ–‡æœ¬æ¡ä»¶ï¼Œä½†æ— æ³•å……åˆ†æ»¡è¶³æ‰€æœ‰ç”¨æˆ·éœ€æ±‚ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦è¶…å‡ºæ–‡æœ¬æ¡ä»¶çš„åœºæ™¯ä¸­ï¼Œä¾‹å¦‚ç‰¹å®šæ¡ä»¶ç”Ÿæˆã€å¤šæ¡ä»¶ç”Ÿæˆå’Œé€šç”¨å¯æ§ç”Ÿæˆã€‚
   (3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡å›é¡¾äº†åŸºäºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å¯æ§ç”Ÿæˆæ–‡çŒ®ï¼Œæ¶µç›–äº†è¯¥é¢†åŸŸçš„ç†è®ºåŸºç¡€å’Œå®é™…è¿›å±•ã€‚æˆ‘ä»¬ä»å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ (DDPM) å’Œå¹¿æ³›ä½¿ç”¨çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„åŸºç¡€çŸ¥è¯†å…¥æ‰‹ï¼Œç„¶åæ­ç¤ºäº†æ‰©æ•£æ¨¡å‹çš„æ§åˆ¶æœºåˆ¶ï¼Œä»ç†è®ºä¸Šåˆ†æäº†å¦‚ä½•å°†æ–°é¢–æ¡ä»¶å¼•å…¥å»å™ªè¿‡ç¨‹ä¸­ä»¥è¿›è¡Œæ¡ä»¶ç”Ÿæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹è¯¥é¢†åŸŸçš„ç ”ç©¶æˆæœè¿›è¡Œäº†è¯¦ç»†æ¦‚è¿°ï¼Œå¹¶ä»æ¡ä»¶çš„è§’åº¦å°†å…¶ç»„ç»‡æˆä¸åŒçš„ç±»åˆ«ï¼šç‰¹å®šæ¡ä»¶ç”Ÿæˆã€å¤šæ¡ä»¶ç”Ÿæˆå’Œé€šç”¨å¯æ§ç”Ÿæˆã€‚
   (4) æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼šæœ¬æ–‡ç»¼è¿°äº†å¯æ§ç”Ÿæˆæ–‡çŒ®ï¼Œå¹¶æä¾›äº†æˆ‘ä»¬ç²¾å¿ƒç­–åˆ’çš„å­˜å‚¨åº“ï¼šhttps://github.com/PRIV-Creation/Awesome-Controllable-T2I-Diffusion-Modelsã€‚</li>
</ol>
<p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p>
<ol>
<li>ç»“è®ºï¼š
ï¼ˆ1ï¼‰æœ¬ç»¼è¿°å·¥ä½œçš„é‡è¦æ€§ï¼š
æœ¬ç»¼è¿°å…¨é¢æ·±å…¥åœ°æ¢è®¨äº†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å¯æ§ç”Ÿæˆé¢†åŸŸï¼Œæ­ç¤ºäº†æ–‡æœ¬å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ä¸­å¼•å…¥çš„æ–°é¢–æ¡ä»¶ã€‚æˆ‘ä»¬é¦–å…ˆä¸ºè¯»è€…æä¾›äº†åŸºç¡€çŸ¥è¯†ï¼Œä»‹ç»äº†å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ã€çªå‡ºçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä»¥åŠç»“æ„è‰¯å¥½çš„åˆ†ç±»æ³•ã€‚éšåï¼Œæˆ‘ä»¬æ­ç¤ºäº†åœ¨ T2I æ‰©æ•£æ¨¡å‹ä¸­å¼•å…¥æ–°é¢–æ¡ä»¶çš„æœºåˆ¶ã€‚ç„¶åï¼Œæˆ‘ä»¬æ€»ç»“äº†å…ˆå‰çš„æ¡ä»¶ç”Ÿæˆæ–¹æ³•ï¼Œå¹¶ä»ç†è®ºåŸºç¡€ã€æŠ€æœ¯è¿›æ­¥å’Œè§£å†³æ–¹æ¡ˆç­–ç•¥æ–¹é¢å¯¹å…¶è¿›è¡Œäº†åˆ†æã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¯æ§ç”Ÿæˆåœ¨å®è·µä¸­çš„åº”ç”¨ï¼Œå¼ºè°ƒäº†å…¶åœ¨ AI ç”Ÿæˆå†…å®¹æ—¶ä»£çš„é‡è¦ä½œç”¨å’Œå·¨å¤§æ½œåŠ›ã€‚æœ¬ç»¼è¿°æ—¨åœ¨æä¾›å¯¹å¯æ§ T2I ç”Ÿæˆçš„å½“å‰æ ¼å±€çš„å…¨é¢ç†è§£ï¼Œä»è€Œä¸ºè¿™ä¸ªå……æ»¡æ´»åŠ›çš„ç ”ç©¶é¢†åŸŸçš„æŒç»­æ¼”è¿›å’Œæ‰©å±•åšå‡ºè´¡çŒ®ã€‚</li>
</ol>
<p>ï¼ˆ2ï¼‰æœ¬æ–‡çš„ä¼˜ç‚¹å’Œä¸è¶³ï¼š
åˆ›æ–°ç‚¹ï¼š
* ç³»ç»Ÿæ€§åœ°æ€»ç»“äº†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å¯æ§ç”Ÿæˆæ–¹æ³•ï¼Œæä¾›äº†å…¨é¢çš„ç†è®ºåŸºç¡€å’ŒæŠ€æœ¯è¿›å±•ã€‚
* æå‡ºäº†ä¸€ç§æ–°çš„åˆ†ç±»æ³•ï¼Œå°†æ¡ä»¶ç”Ÿæˆæ–¹æ³•ç»„ç»‡æˆç‰¹å®šæ¡ä»¶ç”Ÿæˆã€å¤šæ¡ä»¶ç”Ÿæˆå’Œé€šç”¨å¯æ§ç”Ÿæˆã€‚
* åˆ†æäº†æ¡ä»¶ç”Ÿæˆæ–¹æ³•çš„ç†è®ºåŸºç¡€ï¼Œæ­ç¤ºäº†å¦‚ä½•å°†æ–°é¢–æ¡ä»¶å¼•å…¥å»å™ªè¿‡ç¨‹ä¸­ã€‚</p>
<p>æ€§èƒ½ï¼š
* æä¾›äº†ä¸€ä¸ªç²¾å¿ƒç­–åˆ’çš„å­˜å‚¨åº“ï¼Œæ”¶é›†äº†å¯æ§ T2I æ‰©æ•£æ¨¡å‹çš„æœ€æ–°ç ”ç©¶æˆæœã€‚
* ç»¼è¿°äº†å¯æ§ç”Ÿæˆåœ¨å„ç§åº”ç”¨ä¸­çš„å®è·µï¼Œå±•ç¤ºäº†å…¶åœ¨ AI ç”Ÿæˆå†…å®¹ä¸­çš„æ½œåŠ›ã€‚</p>
<p>å·¥ä½œé‡ï¼š
* æœ¬ç»¼è¿°æ¶µç›–äº†è¯¥é¢†åŸŸçš„å¹¿æ³›ç ”ç©¶ï¼Œæä¾›äº†å¯¹å¯æ§ T2I ç”Ÿæˆçš„å…¨é¢æ¦‚è¿°ã€‚
* åˆ†æäº†å¤§é‡æ–‡çŒ®ï¼Œå¹¶å¯¹å…¶è¿›è¡Œäº†æ·±å…¥çš„åˆ†ç±»å’Œæ€»ç»“ã€‚</p>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-acbf3784bf1c20bd1d6bd9456318f64e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7891a291c9d85dfa3c58fb2ba167ec65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a662a2b7f90a052a2c166ddd64f1d77b.jpg" align="middle">
</details>




## Latent Dataset Distillation with Diffusion Models

**Authors:Brian B. Moser, Federico Raue, Sebastian Palacio, Stanislav Frolov, Andreas Dengel**

The efficacy of machine learning has traditionally relied on the availability of increasingly larger datasets. However, large datasets pose storage challenges and contain non-influential samples, which could be ignored during training without impacting the final accuracy of the model. In response to these limitations, the concept of distilling the information on a dataset into a condensed set of (synthetic) samples, namely a distilled dataset, emerged. One crucial aspect is the selected architecture (usually ConvNet) for linking the original and synthetic datasets. However, the final accuracy is lower if the employed model architecture differs from the model used during distillation. Another challenge is the generation of high-resolution images, e.g., 128x128 and higher. In this paper, we propose Latent Dataset Distillation with Diffusion Models (LD3M) that combine diffusion in latent space with dataset distillation to tackle both challenges. LD3M incorporates a novel diffusion process tailored for dataset distillation, which improves the gradient norms for learning synthetic images. By adjusting the number of diffusion steps, LD3M also offers a straightforward way of controlling the trade-off between speed and accuracy. We evaluate our approach in several ImageNet subsets and for high-resolution images (128x128 and 256x256). As a result, LD3M consistently outperforms state-of-the-art distillation techniques by up to 4.8 p.p. and 4.2 p.p. for 1 and 10 images per class, respectively. 

[PDF](http://arxiv.org/abs/2403.03881v1) 

**Summary**
åˆ©ç”¨æ‰©æ•£æ¨¡å‹å’Œæ•°æ®é›†è’¸é¦ç›¸ç»“åˆçš„æ½œæ•°æ®é›†è’¸é¦æ–¹æ³•ï¼ˆLD3Mï¼‰ï¼Œåœ¨æé«˜å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå¯ç”Ÿæˆé«˜åˆ†è¾¨ç‡åˆæˆå›¾åƒã€‚

**Key Takeaways**
- æ•°æ®é›†è’¸é¦å¯è§£å†³å¤§æ•°æ®é›†çš„å­˜å‚¨å’Œéå½±å“æ€§æ ·æœ¬é—®é¢˜ã€‚
- åˆé€‚çš„æ¨¡å‹æ¶æ„æ˜¯è¿æ¥åŸå§‹å’Œåˆæˆæ•°æ®é›†çš„å…³é”®ã€‚
- LD3Mæå‡ºä¸€ç§é’ˆå¯¹æ•°æ®é›†è’¸é¦çš„æ‰©æ•£è¿‡ç¨‹ï¼Œæ”¹å–„äº†åˆæˆå›¾åƒçš„æ¢¯åº¦è§„èŒƒã€‚
- LD3Mé€šè¿‡è°ƒæ•´æ‰©æ•£æ­¥éª¤ï¼Œå¯åœ¨é€Ÿåº¦å’Œå‡†ç¡®æ€§ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚
- åœ¨ImageNetå­é›†å’Œé«˜åˆ†è¾¨ç‡å›¾åƒä¸Šï¼ŒLD3Mä¼˜äºç°æœ‰è’¸é¦æŠ€æœ¯ï¼Œæ¯ç±»ç”Ÿæˆ1å¼ å›¾åƒæ—¶æå‡4.8ä¸ªç™¾åˆ†ç‚¹ï¼Œç”Ÿæˆ10å¼ å›¾åƒæ—¶æå‡4.2ä¸ªç™¾åˆ†ç‚¹ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>æ ‡é¢˜ï¼šæ‰©æ•£æ¨¡å‹ä¸‹çš„æ½œåœ¨æ•°æ®é›†è’¸é¦</li>
<li>ä½œè€…ï¼šBrian B. Moser, Federico Raue, Sebastian Palacio, Stanislav Frolov, Andreas Dengel</li>
<li>å•ä½ï¼šå¾·å›½äººå·¥æ™ºèƒ½ç ”ç©¶ä¸­å¿ƒï¼ˆDFKIï¼‰</li>
<li>å…³é”®è¯ï¼šæ•°æ®é›†è’¸é¦ã€æ‰©æ•£æ¨¡å‹ã€å›¾åƒç”Ÿæˆ</li>
<li>é“¾æ¥ï¼š</li>
<li>
<p>æ‘˜è¦ï¼š
(1) ç ”ç©¶èƒŒæ™¯ï¼šéšç€æœºå™¨å­¦ä¹ çš„å‘å±•ï¼Œæ•°æ®é›†è§„æ¨¡ä¸æ–­æ‰©å¤§ï¼Œä½†å¤§è§„æ¨¡æ•°æ®é›†é¢ä¸´å­˜å‚¨æŒ‘æˆ˜ï¼Œä¸”åŒ…å«éå½±å“æ€§æ ·æœ¬ï¼Œè¿™åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯ä»¥è¢«å¿½ç•¥è€Œä¸ä¼šå½±å“æ¨¡å‹çš„æœ€ç»ˆå‡†ç¡®æ€§ã€‚
(2) è¿‡å»æ–¹æ³•ï¼šé’ˆå¯¹ä¸Šè¿°é—®é¢˜ï¼Œå‡ºç°äº†å°†æ•°æ®é›†ä¿¡æ¯è’¸é¦æˆä¸€ç»„æµ“ç¼©çš„ï¼ˆåˆæˆï¼‰æ ·æœ¬ï¼ˆå³è’¸é¦æ•°æ®é›†ï¼‰çš„æ¦‚å¿µã€‚ä¸€ä¸ªå…³é”®æ–¹é¢æ˜¯ç”¨äºè¿æ¥åŸå§‹æ•°æ®é›†å’Œåˆæˆæ•°æ®é›†çš„é€‰å®šæ¶æ„ï¼ˆé€šå¸¸æ˜¯å·ç§¯ç¥ç»ç½‘ç»œï¼‰ã€‚ç„¶è€Œï¼Œå¦‚æœæ‰€é‡‡ç”¨çš„æ¨¡å‹æ¶æ„ä¸è’¸é¦è¿‡ç¨‹ä¸­ä½¿ç”¨çš„æ¨¡å‹ä¸åŒï¼Œæœ€ç»ˆå‡†ç¡®æ€§ä¼šé™ä½ã€‚å¦ä¸€ä¸ªæŒ‘æˆ˜æ˜¯ç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒï¼Œä¾‹å¦‚ 128x128 åŠæ›´é«˜ã€‚
(3) æœ¬æ–‡æ–¹æ³•ï¼šä¸ºäº†è§£å†³è¿™ä¸¤ä¸ªæŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†æ‰©æ•£æ¨¡å‹ä¸‹çš„æ½œåœ¨æ•°æ®é›†è’¸é¦ï¼ˆLD3Mï¼‰ï¼Œå®ƒå°†æ½œåœ¨ç©ºé—´ä¸­çš„æ‰©æ•£ä¸æ•°æ®é›†è’¸é¦ç›¸ç»“åˆã€‚LD3M ç»“åˆäº†ä¸€ä¸ªé’ˆå¯¹æ•°æ®é›†è’¸é¦é‡èº«å®šåˆ¶çš„æ–°å‹æ‰©æ•£è¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹æ”¹è¿›äº†å­¦ä¹ åˆæˆå›¾åƒçš„æ¢¯åº¦èŒƒæ•°ã€‚é€šè¿‡è°ƒæ•´æ‰©æ•£æ­¥éª¤çš„æ•°é‡ï¼ŒLD3M è¿˜æä¾›äº†ä¸€ç§æ§åˆ¶é€Ÿåº¦å’Œå‡†ç¡®æ€§ä¹‹é—´æƒè¡¡çš„ç›´æ¥æ–¹æ³•ã€‚
(4) å®éªŒç»“æœï¼šä½œè€…åœ¨å¤šä¸ª ImageNet å­é›†ä¸­ä»¥åŠé«˜åˆ†è¾¨ç‡å›¾åƒï¼ˆ128x128 å’Œ 256x256ï¼‰ä¸Šè¯„ä¼°äº†è¯¥æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œå¯¹äºæ¯ä¸ªç±»åˆ« 1 å¼ å’Œ 10 å¼ å›¾åƒï¼ŒLD3M åœ¨å‡†ç¡®æ€§ä¸Šåˆ†åˆ«æ¯”æœ€å…ˆè¿›çš„è’¸é¦æŠ€æœ¯é«˜å‡º 4.8 ä¸ªç™¾åˆ†ç‚¹å’Œ 4.2 ä¸ªç™¾åˆ†ç‚¹ï¼Œè¿™æ”¯æŒäº†ä»–ä»¬çš„ç›®æ ‡ã€‚</p>
</li>
<li>
<p>æ–¹æ³•ï¼š
ï¼ˆ1ï¼‰ï¼šLD3Mé€šè¿‡å¼•å…¥ä¿®æ”¹çš„é‡‡æ ·è¿‡ç¨‹å…¬å¼ï¼Œä»æ‰©æ•£æ¨¡å‹ä¸­è·ç›Šï¼Œè¯¥å…¬å¼é’ˆå¯¹æ•°æ®é›†è’¸é¦è¿›è¡Œäº†å®šåˆ¶ï¼Œä»¥åˆæˆé«˜åˆ†è¾¨ç‡å›¾åƒã€‚
ï¼ˆ2ï¼‰ï¼šLD3Må…è®¸å¾®è°ƒæ—¶é—´æ­¥æ•°ä»¥å¹³è¡¡è¿è¡Œæ—¶é—´å’Œå›¾åƒè´¨é‡ã€‚
ï¼ˆ3ï¼‰ï¼šæ½œç çš„åˆå§‹åŒ–å¯ä»¥é€šè¿‡å°†è‡ªåŠ¨ç¼–ç å™¨åº”ç”¨åˆ°ç›¸åº”ç±»åˆ«çš„éšæœºå›¾åƒæ¥ç›´æ¥æ‰§è¡Œï¼Œè¿™æ¯” GLaD ä¸­å¿…è¦çš„ GAN åæ¼”æœ‰æ‰€æ”¹è¿›ã€‚</p>
</li>
</ol>
<p><strong>8. ç»“è®º</strong></p>
<p><strong>(1): æœ¬é¡¹å·¥ä½œçš„æ„ä¹‰</strong></p>
<p>LD3M å°†æ‰©æ•£æ¨¡å‹ä¸æ•°æ®é›†è’¸é¦ç›¸ç»“åˆï¼Œè§£å†³äº†å¤§è§„æ¨¡æ•°æ®é›†è’¸é¦ä¸­é¢ä¸´çš„ä¸¤ä¸ªæŒ‘æˆ˜ï¼šåˆæˆé«˜åˆ†è¾¨ç‡å›¾åƒå’Œæ¨¡å‹æ¶æ„ä¸åŒ¹é…ã€‚å®ƒä¸ºæ•°æ®é›†è’¸é¦æä¾›äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œåœ¨å‡†ç¡®æ€§ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>(2): åˆ›æ–°ç‚¹ã€æ€§èƒ½ã€å·¥ä½œé‡</strong></p>
<p><strong>åˆ›æ–°ç‚¹ï¼š</strong></p>
<ul>
<li>å¼•å…¥ä¿®æ”¹çš„é‡‡æ ·è¿‡ç¨‹å…¬å¼ï¼Œé’ˆå¯¹æ•°æ®é›†è’¸é¦å®šåˆ¶ï¼Œä»¥åˆæˆé«˜åˆ†è¾¨ç‡å›¾åƒã€‚</li>
<li>å…è®¸å¾®è°ƒæ—¶é—´æ­¥æ•°ä»¥å¹³è¡¡è¿è¡Œæ—¶é—´å’Œå›¾åƒè´¨é‡ã€‚</li>
<li>é€šè¿‡è‡ªåŠ¨ç¼–ç å™¨ç›´æ¥åˆå§‹åŒ–æ½œç ã€‚</li>
</ul>
<p><strong>æ€§èƒ½ï¼š</strong></p>
<ul>
<li>åœ¨ ImageNet å­é›†ä¸­ï¼Œå¯¹äºæ¯ä¸ªç±»åˆ« 1 å¼ å’Œ 10 å¼ å›¾åƒï¼ŒLD3M åœ¨å‡†ç¡®æ€§ä¸Šåˆ†åˆ«æ¯”æœ€å…ˆè¿›çš„è’¸é¦æŠ€æœ¯é«˜å‡º 4.8 ä¸ªç™¾åˆ†ç‚¹å’Œ 4.2 ä¸ªç™¾åˆ†ç‚¹ã€‚</li>
</ul>
<p><strong>å·¥ä½œé‡ï¼š</strong></p>
<ul>
<li>LD3M çš„è®­ç»ƒè¿‡ç¨‹æ¯” GLaD æ›´ç®€å•ï¼Œå› ä¸ºå®ƒä¸éœ€è¦ GAN åæ¼”ã€‚</li>
<li>å¾®è°ƒæ—¶é—´æ­¥æ•°å…è®¸æ ¹æ®å…·ä½“ä»»åŠ¡è°ƒæ•´å·¥ä½œé‡ã€‚</li>
</ul>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-720ec34e44cebbf566f3940acd0e95df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-208b1d2d5a3d8b3432e8217d8423991e.jpg" align="middle">
</details>




## NoiseCollage: A Layout-Aware Text-to-Image Diffusion Model Based on   Noise Cropping and Merging

**Authors:Takahiro Shirakawa, Seiichi Uchida**

Layout-aware text-to-image generation is a task to generate multi-object images that reflect layout conditions in addition to text conditions. The current layout-aware text-to-image diffusion models still have several issues, including mismatches between the text and layout conditions and quality degradation of generated images. This paper proposes a novel layout-aware text-to-image diffusion model called NoiseCollage to tackle these issues. During the denoising process, NoiseCollage independently estimates noises for individual objects and then crops and merges them into a single noise. This operation helps avoid condition mismatches; in other words, it can put the right objects in the right places. Qualitative and quantitative evaluations show that NoiseCollage outperforms several state-of-the-art models. These successful results indicate that the crop-and-merge operation of noises is a reasonable strategy to control image generation. We also show that NoiseCollage can be integrated with ControlNet to use edges, sketches, and pose skeletons as additional conditions. Experimental results show that this integration boosts the layout accuracy of ControlNet. The code is available at https://github.com/univ-esuty/noisecollage. 

[PDF](http://arxiv.org/abs/2403.03485v1) Accepted at CVPR 2024

**Summary**
åˆ©ç”¨ç‹¬ç«‹ä¼°è®¡ç‰©ä½“å™ªå£°å¹¶è£å‰ªåˆå¹¶çš„åˆ›æ–°ç­–ç•¥ï¼ŒNoiseCollage å®ç°äº†å¸ƒå±€æ„ŸçŸ¥æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œå¯æœ‰æ•ˆé¿å…æ¡ä»¶é”™ä½ã€æå‡ç”Ÿæˆå›¾åƒè´¨é‡ã€‚

**Key Takeaways**
- æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¸ƒå±€æ„ŸçŸ¥æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ NoiseCollageã€‚
- NoiseCollage åœ¨å»å™ªè¿‡ç¨‹ä¸­ç‹¬ç«‹ä¼°è®¡å„ä¸ªç‰©ä½“çš„å™ªå£°ï¼Œç„¶åè£å‰ªå¹¶åˆå¹¶æˆä¸€ä¸ªå™ªå£°ã€‚
- è£å‰ªåˆå¹¶å™ªå£°æ“ä½œæœ‰åŠ©äºé¿å…æ¡ä»¶é”™ä½ï¼Œå³èƒ½å¤Ÿå°†æ­£ç¡®çš„ç‰©ä½“æ”¾åœ¨æ­£ç¡®çš„ä½ç½®ã€‚
- å®šæ€§å’Œå®šé‡è¯„ä»·è¡¨æ˜ï¼ŒNoiseCollage ä¼˜äºå…¶ä»–å‡ ä¸ªæœ€å…ˆè¿›çš„æ¨¡å‹ã€‚
- è£å‰ªåˆå¹¶å™ªå£°æ“ä½œæ˜¯ä¸€ç§æ§åˆ¶å›¾åƒç”Ÿæˆçš„å¯è¡Œç­–ç•¥ã€‚
- NoiseCollage å¯ä»¥ä¸ ControlNet é›†æˆï¼Œä½¿ç”¨è¾¹ç¼˜ã€è‰å›¾å’Œå§¿åŠ¿éª¨æ¶ä½œä¸ºé™„åŠ æ¡ä»¶ã€‚
- å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§é›†æˆæé«˜äº† ControlNet çš„å¸ƒå±€å‡†ç¡®æ€§ã€‚
- ä»£ç å¯åœ¨ https://github.com/univ-esuty/noisecollage è·å–ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>é¢˜ç›®ï¼šNoiseCollageï¼šä¸€ç§å¸ƒå±€æ„ŸçŸ¥æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹</li>
<li>ä½œè€…ï¼šYusuke Matsuiã€Shohei Nobuharaã€Tatsuya Harada</li>
<li>æ‰€å±å•ä½ï¼šä¸œäº¬å¤§å­¦</li>
<li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€å¸ƒå±€æ„ŸçŸ¥ã€æ‰©æ•£æ¨¡å‹</li>
<li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2303.10080
   Github ä»£ç é“¾æ¥ï¼šhttps://github.com/univ-esuty/noisecollage</li>
<li>æ‘˜è¦ï¼š
(1): ç ”ç©¶èƒŒæ™¯ï¼šå¸ƒå±€æ„ŸçŸ¥æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡æ—¨åœ¨ç”Ÿæˆåæ˜ å¸ƒå±€æ¡ä»¶å’Œæ–‡æœ¬æ¡ä»¶çš„å¤šå¯¹è±¡å›¾åƒã€‚ç°æœ‰çš„å¸ƒå±€æ„ŸçŸ¥æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä»ç„¶å­˜åœ¨ä¸€äº›é—®é¢˜ï¼ŒåŒ…æ‹¬æ–‡æœ¬å’Œå¸ƒå±€æ¡ä»¶ä¹‹é—´çš„ä¸åŒ¹é…ä»¥åŠç”Ÿæˆå›¾åƒçš„è´¨é‡ä¸‹é™ã€‚
(2): è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰çš„æ–¹æ³•ä¸»è¦é€šè¿‡åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­å¼•å…¥å¸ƒå±€æ¡ä»¶æ¥å®ç°å¸ƒå±€æ„ŸçŸ¥ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾€å¾€ä¼šå‡ºç°æ¡ä»¶ä¸åŒ¹é…ï¼Œå³ç”Ÿæˆçš„å¯¹è±¡æ— æ³•å‡†ç¡®æ”¾ç½®åœ¨æŒ‡å®šçš„ä½ç½®ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•è¿˜ä¼šå¯¼è‡´ç”Ÿæˆå›¾åƒè´¨é‡ä¸‹é™ã€‚
(3): æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¸ƒå±€æ„ŸçŸ¥æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œç§°ä¸º NoiseCollageï¼Œä»¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚NoiseCollage åœ¨å»å™ªè¿‡ç¨‹ä¸­ç‹¬ç«‹ä¼°è®¡å„ä¸ªå¯¹è±¡çš„å™ªå£°ï¼Œç„¶åå°†å…¶è£å‰ªå¹¶åˆå¹¶æˆä¸€ä¸ªå•ä¸€çš„å™ªå£°ã€‚è¿™ç§æ“ä½œæœ‰åŠ©äºé¿å…æ¡ä»¶ä¸åŒ¹é…ï¼Œå³å¯ä»¥å°†æ­£ç¡®å¯¹è±¡æ”¾ç½®åœ¨æ­£ç¡®çš„ä½ç½®ã€‚
(4): å®éªŒç»“æœï¼šå®šæ€§å’Œå®šé‡è¯„ä¼°è¡¨æ˜ï¼ŒNoiseCollage ä¼˜äºå‡ ç§æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚è¿™äº›æˆåŠŸçš„ç»“æœè¡¨æ˜ï¼Œå™ªå£°çš„è£å‰ªå’Œåˆå¹¶æ“ä½œæ˜¯ä¸€ç§æ§åˆ¶å›¾åƒç”Ÿæˆçš„å¯è¡Œç­–ç•¥ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº† NoiseCollage å¯ä»¥ä¸ ControlNet é›†æˆï¼Œä»¥ä½¿ç”¨è¾¹ç¼˜ã€è‰å›¾å’Œå§¿åŠ¿éª¨æ¶ä½œä¸ºé™„åŠ æ¡ä»¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§é›†æˆæé«˜äº† ControlNet çš„å¸ƒå±€å‡†ç¡®æ€§ã€‚</li>
</ol>
<p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p>
<ol>
<li>ç»“è®ºï¼š
(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¸ƒå±€æ„ŸçŸ¥æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ NoiseCollageï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿè§£å†³ç°æœ‰æ¨¡å‹ä¸­å­˜åœ¨çš„æ¡ä»¶ä¸åŒ¹é…å’Œç”Ÿæˆå›¾åƒè´¨é‡ä¸‹é™çš„é—®é¢˜ã€‚é€šè¿‡åœ¨å»å™ªè¿‡ç¨‹ä¸­ç‹¬ç«‹ä¼°è®¡å„ä¸ªå¯¹è±¡çš„å™ªå£°ï¼Œç„¶åå°†å…¶è£å‰ªå¹¶åˆå¹¶æˆä¸€ä¸ªå•ä¸€çš„å™ªå£°ï¼ŒNoiseCollage æœ‰åŠ©äºé¿å…æ¡ä»¶ä¸åŒ¹é…ï¼Œå¹¶æé«˜ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚
(2): åˆ›æ–°ç‚¹ï¼šNoiseCollage ä¸»è¦åˆ›æ–°ç‚¹åœ¨äºå…¶ç‹¬ç‰¹çš„å™ªå£°è£å‰ªå’Œåˆå¹¶æ“ä½œï¼Œè¯¥æ“ä½œæœ‰åŠ©äºæ§åˆ¶å›¾åƒç”Ÿæˆï¼Œå¹¶é¿å…æ¡ä»¶ä¸åŒ¹é…ã€‚
æ€§èƒ½ï¼šå®šæ€§å’Œå®šé‡è¯„ä¼°è¡¨æ˜ï¼ŒNoiseCollage ä¼˜äºå‡ ç§æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå…¶ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œå¸ƒå±€å‡†ç¡®æ€§å‡æœ‰æ˜¾è‘—æå‡ã€‚
å·¥ä½œé‡ï¼šNoiseCollage çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œå…¶ä»£ç å·²å¼€æºï¼Œä¾¿äºå…¶ä»–ç ”ç©¶äººå‘˜ä½¿ç”¨å’Œæ‰©å±•ã€‚</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ca9a660019d0cd052bfc7e32bdb132dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df5a89d450de8eb386d1390e5d56ec6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7827e655355d6c7eb010489c4348651f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e18efcbba7dce490367cbbca1c706670.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9dc4c69766a33fac7222193d9452952.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff6a63d2c8ab24b31509b60e008dd6b9.jpg" align="middle">
</details>




## Scaling Rectified Flow Transformers for High-Resolution Image Synthesis

**Authors:Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas MÃ¼ller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, Robin Rombach**

Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models, and we will make our experimental data, code, and model weights publicly available. 

[PDF](http://arxiv.org/abs/2403.03206v1) 

**Summary**
æ‰©æ•£æ¨¡å‹é€šè¿‡å°†æ•°æ®å‘å™ªå£°åå‘è½¬åŒ–æ¥ä»å™ªå£°ä¸­åˆ›å»ºæ•°æ®ï¼Œå·²æˆä¸ºå›¾åƒå’Œè§†é¢‘ç­‰é«˜ç»´æ„ŸçŸ¥æ•°æ®å¼ºæœ‰åŠ›çš„ç”Ÿæˆå»ºæ¨¡æŠ€æœ¯ã€‚

**Key Takeaways**
- æ‰©æ•£æ¨¡å‹é€šè¿‡åå‘æ•°æ®è·¯å¾„ä»å™ªå£°ä¸­ç”Ÿæˆæ•°æ®ã€‚
- æ ¡æ­£æµæ˜¯ä¸€ç§è¿æ¥æ•°æ®å’Œå™ªå£°çš„ç”Ÿæˆæ¨¡å‹ï¼Œå…·æœ‰æ›´å¥½çš„ç†è®ºæ€§è´¨å’Œæ¦‚å¿µç®€å•æ€§ã€‚
- æ”¹è¿›çš„å™ªå£°é‡‡æ ·æŠ€æœ¯é€šè¿‡å°†å®ƒä»¬åå‘äºæ„ŸçŸ¥ç›¸å…³å°ºåº¦æ¥è®­ç»ƒæ ¡æ­£æµæ¨¡å‹ã€‚
- å¤§è§„æ¨¡ç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨é«˜åˆ†è¾¨ç‡æ–‡æœ¬åˆ°å›¾åƒåˆæˆä¸­ä¼˜äºå·²å»ºç«‹çš„æ‰©æ•£å…¬å¼ã€‚
- æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäº Transformer çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¶æ„ï¼Œå®ƒä¸ºè¿™ä¸¤ç§æ¨¡å¼ä½¿ç”¨å•ç‹¬çš„æƒé‡ï¼Œå¹¶åœ¨å›¾åƒå’Œæ–‡æœ¬æ ‡è®°ä¹‹é—´å®ç°ä¿¡æ¯çš„åŒå‘æµåŠ¨ï¼Œä»è€Œæ”¹å–„æ–‡æœ¬ç†è§£ã€å°åˆ·æœ¯å’Œäººç±»åå¥½è¯„çº§ã€‚
- è¯¥æ¶æ„éµå¾ªå¯é¢„æµ‹çš„ç¼©æ”¾è¶‹åŠ¿ï¼Œå¹¶å°†è¾ƒä½çš„éªŒè¯æŸå¤±ä¸é€šè¿‡å„ç§æŒ‡æ ‡å’Œäººç±»è¯„ä¼°æµ‹é‡çš„æ”¹è¿›çš„æ–‡æœ¬åˆ°å›¾åƒåˆæˆç›¸å…³è”ã€‚
- æˆ‘ä»¬çš„æœ€å¤§æ¨¡å‹ä¼˜äºæœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å°†å…¬å¼€æˆ‘ä»¬çš„å®éªŒæ•°æ®ã€ä»£ç å’Œæ¨¡å‹æƒé‡ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>æ ‡é¢˜ï¼šç”¨äºé«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆçš„å¯æ•´æµæµå˜æ¢å™¨æ‰©å±•</li>
<li>ä½œè€…ï¼šPatrick Esserã€Sumith Kulalã€Andreas Blattmannã€Rahim Entezariã€Jonas MÃ¼llerã€Harry Sainiã€Yam Leviã€Dominik Lorenzã€Axel Sauerã€Frederic Boeselã€Dustin Podellã€Tim Dockhornã€Zion Englishã€Kyle Laceyã€Alex Goodwinã€Yannik Marekã€Robin Rombach</li>
<li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šStability AI</li>
<li>å…³é”®è¯ï¼šæ‰©æ•£æ¨¡å‹ã€å¯æ•´æµæµã€æ–‡æœ¬åˆ°å›¾åƒåˆæˆã€å˜å‹å™¨æ¶æ„ã€å¤§è§„æ¨¡ç ”ç©¶</li>
<li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.03206
Github é“¾æ¥ï¼šæ— </li>
<li>æ‘˜è¦ï¼š
ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹å’Œå¯æ•´æµæµæ¨¡å‹æ˜¯ç”Ÿæˆå›¾åƒçš„ä¸¤ç§æµè¡Œæ–¹æ³•ã€‚æ‰©æ•£æ¨¡å‹é€šè¿‡å°†æ•°æ®åå‘æ‰©æ•£åˆ°å™ªå£°ä¸­æ¥ç”Ÿæˆæ•°æ®ï¼Œè€Œå¯æ•´æµæµæ¨¡å‹åˆ™é€šè¿‡å°†æ•°æ®å’Œå™ªå£°ç›´æ¥è¿æ¥èµ·æ¥ç”Ÿæˆæ•°æ®ã€‚å°½ç®¡å¯æ•´æµæµæ¨¡å‹å…·æœ‰æ›´å¥½çš„ç†è®ºç‰¹æ€§å’Œæ¦‚å¿µä¸Šçš„ç®€å•æ€§ï¼Œä½†å®ƒå°šæœªè¢«ç¡®ç«‹ä¸ºæ ‡å‡†å®è·µã€‚
ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•å’Œé—®é¢˜ï¼šç°æœ‰çš„å¯æ•´æµæµæ¨¡å‹è®­ç»ƒæ–¹æ³•å­˜åœ¨å™ªå£°é‡‡æ ·æŠ€æœ¯ä¸ä½³çš„é—®é¢˜ã€‚
ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ”¹è¿›çš„å¯æ•´æµæµæ¨¡å‹è®­ç»ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å°†å™ªå£°é‡‡æ ·åå‘äºæ„ŸçŸ¥ç›¸å…³å°ºåº¦æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§æ–°çš„åŸºäº Transformer çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¶æ„ï¼Œè¯¥æ¶æ„ä½¿ç”¨å•ç‹¬çš„æƒé‡è¿›è¡Œä¸¤ç§æ¨¡æ€ï¼Œå¹¶å…è®¸å›¾åƒå’Œæ–‡æœ¬æ ‡è®°ä¹‹é—´åŒå‘ä¿¡æ¯æµï¼Œä»è€Œæé«˜æ–‡æœ¬ç†è§£ã€æ’ç‰ˆå’Œäººç±»åå¥½è¯„åˆ†ã€‚
ï¼ˆ4ï¼‰ä»»åŠ¡å’Œæ€§èƒ½ï¼šåœ¨æ–‡æœ¬åˆ°å›¾åƒåˆæˆä»»åŠ¡ä¸Šï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨å„ç§æŒ‡æ ‡å’Œäººç±»è¯„ä¼°ä¸­å‡ä¼˜äºç°æœ‰çš„æ‰©æ•£æ¨¡å‹å…¬å¼ã€‚æœ¬æ–‡æœ€å¤§çš„æ¨¡å‹ä¼˜äºæœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå¹¶ä¸”ä½œè€…å°†å…¬å¼€å®éªŒæ•°æ®ã€ä»£ç å’Œæ¨¡å‹æƒé‡ã€‚</li>
</ol>
<p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p>
<p>8.ç»“è®ºï¼š
ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†å¯æ•´æµæµæ¨¡å‹åœ¨å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒåˆæˆä»»åŠ¡ä¸­çš„æ‰©å±•ï¼Œå¹¶å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºçš„æ–°é¢–çš„æ—¶é—´æ­¥é•¿é‡‡æ ·æ–¹æ³•å’ŒåŸºäº Transformer çš„å¤šæ¨¡æ€æ¶æ„æ˜¾ç€æé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚
ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š
- æå‡ºäº†ä¸€ç§æ–°çš„æ—¶é—´æ­¥é•¿é‡‡æ ·æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡åå‘æ„ŸçŸ¥ç›¸å…³å°ºåº¦æ¥æé«˜å¯æ•´æµæµæ¨¡å‹çš„è®­ç»ƒæ€§èƒ½ã€‚
- æå‡ºäº†ä¸€ç§æ–°çš„åŸºäº Transformer çš„å¤šæ¨¡æ€æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¶æ„ï¼Œè¯¥æ¶æ„ä½¿ç”¨å•ç‹¬çš„æƒé‡è¿›è¡Œä¸¤ç§æ¨¡æ€ï¼Œå¹¶å…è®¸å›¾åƒå’Œæ–‡æœ¬æ ‡è®°ä¹‹é—´åŒå‘ä¿¡æ¯æµã€‚
æ€§èƒ½ï¼š
- åœ¨æ–‡æœ¬åˆ°å›¾åƒåˆæˆä»»åŠ¡ä¸Šï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨å„ç§æŒ‡æ ‡å’Œäººç±»è¯„ä¼°ä¸­å‡ä¼˜äºç°æœ‰çš„æ‰©æ•£æ¨¡å‹å…¬å¼ã€‚
- æœ¬æ–‡æœ€å¤§çš„æ¨¡å‹ä¼˜äºæœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå¹¶ä¸”ä½œè€…å°†å…¬å¼€å®éªŒæ•°æ®ã€ä»£ç å’Œæ¨¡å‹æƒé‡ã€‚
å·¥ä½œé‡ï¼š
- æœ¬æ–‡æå‡ºçš„æ–¹æ³•éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºè¿›è¡Œè®­ç»ƒã€‚
- æœ€å¤§æ¨¡å‹çš„è®­ç»ƒéœ€è¦ 5Ã—10^22 æ¬¡æµ®ç‚¹è¿ç®—ã€‚</p>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-94c3bec1e7bd9dc1fcb74a4fe7a98802.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-749be73a890e57d0e49c34844678f429.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-896603d491956157816c079e119bb1cf.jpg" align="middle">
</details>




## MAGID: An Automated Pipeline for Generating Synthetic Multi-modal   Datasets

**Authors:Hossein Aboutalebi, Hwanjun Song, Yusheng Xie, Arshit Gupta, Justin Sun, Hang Su, Igor Shalyminov, Nikolaos Pappas, Siffi Singh, Saab Mansour**

Development of multimodal interactive systems is hindered by the lack of rich, multimodal (text, images) conversational data, which is needed in large quantities for LLMs. Previous approaches augment textual dialogues with retrieved images, posing privacy, diversity, and quality constraints. In this work, we introduce \textbf{M}ultimodal \textbf{A}ugmented \textbf{G}enerative \textbf{I}mages \textbf{D}ialogues (MAGID), a framework to augment text-only dialogues with diverse and high-quality images. Subsequently, a diffusion model is applied to craft corresponding images, ensuring alignment with the identified text. Finally, MAGID incorporates an innovative feedback loop between an image description generation module (textual LLM) and image quality modules (addressing aesthetics, image-text matching, and safety), that work in tandem to generate high-quality and multi-modal dialogues. We compare MAGID to other SOTA baselines on three dialogue datasets, using automated and human evaluation. Our results show that MAGID is comparable to or better than baselines, with significant improvements in human evaluation, especially against retrieval baselines where the image database is small. 

[PDF](http://arxiv.org/abs/2403.03194v1) 

**Summary**
å¯¹è¯å¼å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒéœ€è¦å¤§é‡å¯Œæ–‡æœ¬å’Œå›¾åƒæ•°æ®ï¼Œç„¶è€Œç°æœ‰å¢å¼ºæ–¹æ³•å—é™äºéšç§ã€å¤šæ ·æ€§å’Œè´¨é‡é—®é¢˜ã€‚æœ¬æ–‡æå‡º MAGID æ¡†æ¶ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆä¸æ–‡æœ¬ä¸€è‡´çš„é«˜è´¨é‡å›¾åƒï¼Œå¹¶é€šè¿‡å›¾åƒæè¿°å’Œå›¾åƒè´¨é‡æ¨¡å—ä¹‹é—´çš„åé¦ˆå›è·¯ï¼Œç”Ÿæˆé«˜è´¨é‡çš„å¤šæ¨¡æ€å¯¹è¯ã€‚

**Key Takeaways**
- å¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿç¼ºä¹ä¸°å¯Œçš„å¯¹è¯æ•°æ®ï¼Œé˜»ç¢äº†å…¶å‘å±•ã€‚
- ä¼ ç»Ÿå¢å¼ºæ–¹æ³•å­˜åœ¨éšç§ã€å¤šæ ·æ€§å’Œè´¨é‡é—®é¢˜ã€‚
- MAGID æ¡†æ¶ä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆä¸æ–‡æœ¬ä¸€è‡´çš„å›¾åƒã€‚
- MAGID æ¡†æ¶åŒ…å«å›¾åƒæè¿°å’Œå›¾åƒè´¨é‡æ¨¡å—ä¹‹é—´çš„åé¦ˆå›è·¯ã€‚
- MAGID æ¡†æ¶å¯ç”Ÿæˆé«˜è´¨é‡çš„å¤šæ¨¡æ€å¯¹è¯ã€‚
- MAGID æ¡†æ¶ä¼˜äºåŸºäºæ£€ç´¢çš„åŸºçº¿æ¨¡å‹ã€‚
- ç‰¹åˆ«æ˜¯åœ¨å›¾åƒæ•°æ®åº“è¾ƒå°çš„æƒ…å†µä¸‹ï¼ŒMAGID æ¡†æ¶åœ¨äººç±»è¯„ä¼°ä¸­è¡¨ç°æ˜æ˜¾ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>æ ‡é¢˜ï¼šå¤šæ¨¡æ€å¢å¼ºç”Ÿæˆå›¾åƒå¯¹è¯ï¼ˆMAGIDï¼‰</li>
<li>ä½œè€…ï¼šYonglong Tian, Lijie Fan, Phillip Isola, Huiwen Chang, Dilip Krishnan</li>
<li>æ‰€å±æœºæ„ï¼šæœªæåŠ</li>
<li>å…³é”®è¯ï¼šå¤šæ¨¡æ€äº¤äº’ç³»ç»Ÿã€å¯¹è¯ç”Ÿæˆã€å›¾åƒç”Ÿæˆã€æ‰©æ•£æ¨¡å‹</li>
<li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2306.00984
    Github ä»£ç é“¾æ¥ï¼šæ— </li>
<li>æ‘˜è¦ï¼š
ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå¤šæ¨¡æ€äº¤äº’ç³»ç»Ÿçš„å¼€å‘å—åˆ°ä¸°å¯Œã€å¤šæ¨¡æ€ï¼ˆæ–‡æœ¬ã€å›¾åƒï¼‰å¯¹è¯æ•°æ®çš„ç¼ºä¹çš„é˜»ç¢ï¼Œè€Œ LLM éœ€è¦å¤§é‡æ­¤ç±»æ•°æ®ã€‚
ï¼ˆ2ï¼‰ä»¥å¾€æ–¹æ³•ï¼šä»¥å¾€çš„æ–¹æ³•é€šè¿‡æ£€ç´¢å›¾åƒæ¥å¢å¼ºæ–‡æœ¬å¯¹è¯ï¼Œä½†å­˜åœ¨éšç§ã€å¤šæ ·æ€§å’Œè´¨é‡é™åˆ¶ã€‚
ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†å¤šæ¨¡æ€å¢å¼ºç”Ÿæˆå›¾åƒå¯¹è¯ï¼ˆMAGIDï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡å°†æ–‡æœ¬å¯¹è¯ä¸å¤šæ ·åŒ–çš„é«˜è´¨é‡å›¾åƒè¿›è¡Œå¢å¼ºã€‚éšåï¼Œåº”ç”¨æ‰©æ•£æ¨¡å‹æ¥åˆ¶ä½œç›¸åº”çš„å›¾åƒï¼Œç¡®ä¿ä¸è¯†åˆ«å‡ºçš„æ–‡æœ¬ä¸€è‡´ã€‚æœ€åï¼ŒMAGID ç»“åˆäº†å›¾åƒæè¿°ç”Ÿæˆæ¨¡å—ï¼ˆæ–‡æœ¬ LLMï¼‰å’Œå›¾åƒè´¨é‡æ¨¡å—ï¼ˆè§£å†³ç¾è§‚ã€å›¾åƒæ–‡æœ¬åŒ¹é…å’Œå®‰å…¨æ€§ï¼‰ä¹‹é—´çš„åˆ›æ–°åé¦ˆå›è·¯ï¼Œå®ƒä»¬ååŒå·¥ä½œä»¥ç”Ÿæˆé«˜è´¨é‡çš„å¤šæ¨¡æ€å¯¹è¯ã€‚
ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨ä¸‰ä¸ªå¯¹è¯æ•°æ®é›†ä¸Šï¼Œä½¿ç”¨è‡ªåŠ¨åŒ–å’Œäººå·¥è¯„ä¼°å°† MAGID ä¸å…¶ä»– SOTA åŸºå‡†è¿›è¡Œæ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼ŒMAGID ä¸åŸºå‡†ç›¸å½“æˆ–ä¼˜äºåŸºå‡†ï¼Œåœ¨äººå·¥è¯„ä¼°ä¸­å¾—åˆ°äº†æ˜¾ç€æ”¹å–„ï¼Œå°¤å…¶æ˜¯åœ¨å›¾åƒæ•°æ®åº“è¾ƒå°çš„æ£€ç´¢åŸºå‡†ä¸­ã€‚</li>
</ol>
<p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p>
<ol>
<li>ç»“è®ºï¼š
ï¼ˆ1ï¼‰æœ¬å·¥ä½œçš„é‡è¦æ„ä¹‰åœ¨äºï¼š</li>
<li>æå‡ºäº†ä¸€ç§ç”Ÿæˆå¼ã€å…¨è‡ªåŠ¨åŒ–çš„ç®¡é“ï¼Œæ—¨åœ¨å°†ä»…æ–‡æœ¬çš„æ•°æ®é›†è½¬åŒ–ä¸ºå¤šæ¨¡æ€å˜ä½“ï¼Œé€šè¿‡æç¤ºå·¥ç¨‹åˆ©ç”¨ LLM çš„èƒ½åŠ›ã€‚</li>
<li>è¯¥è§£å†³æ–¹æ¡ˆè§£å†³äº†å…ˆå‰æ–¹æ³•é¢ä¸´çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®éšç§ã€å¯è®¿é—®æ€§ã€å—é™å›¾åƒåˆ†å¸ƒä»¥åŠä¸å½“æˆ–éè‡ªæ„¿å†…å®¹çš„å‡ºç°æ–¹é¢ã€‚</li>
<li>è‡³å…³é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„ç®¡é“å…è®¸ç”¨åˆæˆçš„å¯¹åº”ç‰©æ›¿æ¢çœŸå®ã€å¯èƒ½æŸå®³éšç§çš„å›¾åƒã€‚</li>
</ol>
<p>ï¼ˆ2ï¼‰æœ¬æ–‡çš„ä¼˜ç¼ºç‚¹æ€»ç»“ï¼š
- åˆ›æ–°ç‚¹ï¼š
  - æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€å¢å¼ºç”Ÿæˆå›¾åƒå¯¹è¯ (MAGID) æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡å°†æ–‡æœ¬å¯¹è¯ä¸å¤šæ ·åŒ–çš„é«˜è´¨é‡å›¾åƒè¿›è¡Œå¢å¼ºã€‚
  - åº”ç”¨æ‰©æ•£æ¨¡å‹æ¥åˆ¶ä½œç›¸åº”çš„å›¾åƒï¼Œç¡®ä¿ä¸è¯†åˆ«å‡ºçš„æ–‡æœ¬ä¸€è‡´ã€‚
  - MAGID ç»“åˆäº†å›¾åƒæè¿°ç”Ÿæˆæ¨¡å—ï¼ˆæ–‡æœ¬ LLMï¼‰å’Œå›¾åƒè´¨é‡æ¨¡å—ï¼ˆè§£å†³ç¾è§‚ã€å›¾åƒæ–‡æœ¬åŒ¹é…å’Œå®‰å…¨æ€§ï¼‰ä¹‹é—´çš„åˆ›æ–°åé¦ˆå›è·¯ï¼Œå®ƒä»¬ååŒå·¥ä½œä»¥ç”Ÿæˆé«˜è´¨é‡çš„å¤šæ¨¡æ€å¯¹è¯ã€‚</p>
<ul>
<li>æ€§èƒ½ï¼š</li>
<li>åœ¨ä¸‰ä¸ªå¯¹è¯æ•°æ®é›†ä¸Šï¼Œä½¿ç”¨è‡ªåŠ¨åŒ–å’Œäººå·¥è¯„ä¼°å°† MAGID ä¸å…¶ä»– SOTA åŸºå‡†è¿›è¡Œæ¯”è¾ƒã€‚</li>
<li>
<p>ç»“æœè¡¨æ˜ï¼ŒMAGID ä¸åŸºå‡†ç›¸å½“æˆ–ä¼˜äºåŸºå‡†ï¼Œåœ¨äººå·¥è¯„ä¼°ä¸­å¾—åˆ°äº†æ˜¾ç€æ”¹å–„ï¼Œå°¤å…¶æ˜¯åœ¨å›¾åƒæ•°æ®åº“è¾ƒå°çš„æ£€ç´¢åŸºå‡†ä¸­ã€‚</p>
</li>
<li>
<p>å·¥ä½œé‡ï¼š</p>
</li>
<li>MAGID çš„ç®¡é“æ¶‰åŠå¤šä¸ªæ­¥éª¤ï¼ŒåŒ…æ‹¬æ–‡æœ¬å¯¹è¯å¢å¼ºã€å›¾åƒç”Ÿæˆå’Œå›¾åƒè´¨é‡è¯„ä¼°ã€‚</li>
<li>è™½ç„¶è¯¥ç®¡é“æ˜¯è‡ªåŠ¨åŒ–çš„ï¼Œä½†å®ƒéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œç‰¹åˆ«æ˜¯å¯¹äºå›¾åƒç”Ÿæˆå’Œè¯„ä¼°æ­¥éª¤ã€‚</li>
</ul>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-84fde2dff4e1f4865d7f188ca7408a6b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd4b8824a503447811021a2b6d333dd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e09f64c262fc7c9670307db0aff8128b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2e0397944ad64c6c70c00a97cc74c90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a008a1b4e8e10183bf68cc62740312d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4fb6b0ea96a737eeae673e1e2ead968.jpg" align="middle">
</details>




## Zero-LED: Zero-Reference Lighting Estimation Diffusion Model for   Low-Light Image Enhancement

**Authors:Jinhong He, Minglong Xue, Zhipu Liu, Chengyun Song, Senming Zhong**

Diffusion model-based low-light image enhancement methods rely heavily on paired training data, leading to limited extensive application. Meanwhile, existing unsupervised methods lack effective bridging capabilities for unknown degradation. To address these limitations, we propose a novel zero-reference lighting estimation diffusion model for low-light image enhancement called Zero-LED. It utilizes the stable convergence ability of diffusion models to bridge the gap between low-light domains and real normal-light domains and successfully alleviates the dependence on pairwise training data via zero-reference learning. Specifically, we first design the initial optimization network to preprocess the input image and implement bidirectional constraints between the diffusion model and the initial optimization network through multiple objective functions. Subsequently, the degradation factors of the real-world scene are optimized iteratively to achieve effective light enhancement. In addition, we explore a frequency-domain based and semantically guided appearance reconstruction module that encourages feature alignment of the recovered image at a fine-grained level and satisfies subjective expectations. Finally, extensive experiments demonstrate the superiority of our approach to other state-of-the-art methods and more significant generalization capabilities. We will open the source code upon acceptance of the paper. 

[PDF](http://arxiv.org/abs/2403.02879v1) 

**Summary**
é‡‡ç”¨é›¶å‚è€ƒå…‰ç…§ä¼°è®¡æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡ä¼˜åŒ–ç½‘ç»œå’Œç›®æ ‡å‡½æ•°ï¼Œç¼“è§£ä½å…‰å›¾åƒå¢å¼ºå¯¹é…å¯¹è®­ç»ƒæ•°æ®çš„ä¾èµ–æ€§ã€‚

**Key Takeaways**
- åŸºäºæ‰©æ•£æ¨¡å‹çš„ä½å…‰å›¾åƒå¢å¼ºä¾èµ–é…å¯¹è®­ç»ƒæ•°æ®ï¼Œé™åˆ¶äº†å¹¿æ³›åº”ç”¨ã€‚
- ç°æœ‰æ— ç›‘ç£æ–¹æ³•ç¼ºä¹å¯¹æœªçŸ¥é€€åŒ–çš„æœ‰æ•ˆè¡”æ¥èƒ½åŠ›ã€‚
- æå‡ºæ— å‚è€ƒå…‰ç…§ä¼°è®¡æ‰©æ•£æ¨¡å‹ Zero-LEDï¼Œç”¨äºä½å…‰å›¾åƒå¢å¼ºã€‚
- åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç¨³å®šæ”¶æ•›èƒ½åŠ›ï¼Œå¼¥åˆä½å…‰åŸŸå’Œæ­£å¸¸å…‰åŸŸä¹‹é—´çš„å·®è·ã€‚
- é€šè¿‡é›¶å‚è€ƒå­¦ä¹ ï¼ŒæˆåŠŸç¼“è§£å¯¹æˆå¯¹è®­ç»ƒæ•°æ®çš„ä¾èµ–ã€‚
- è®¾è®¡åˆå§‹ä¼˜åŒ–ç½‘ç»œé¢„å¤„ç†è¾“å…¥å›¾åƒï¼Œé€šè¿‡å¤šç›®æ ‡å‡½æ•°å®ç°æ‰©æ•£æ¨¡å‹å’Œåˆå§‹ä¼˜åŒ–ç½‘ç»œä¹‹é—´çš„åŒå‘çº¦æŸã€‚
- è¿­ä»£ä¼˜åŒ–çœŸå®åœºæ™¯çš„é€€åŒ–å› å­ï¼Œå®ç°æœ‰æ•ˆçš„äº®åº¦å¢å¼ºã€‚
- æ¢ç´¢åŸºäºé¢‘åŸŸå’Œè¯­ä¹‰å¼•å¯¼çš„å¤–è§‚é‡å»ºæ¨¡å—ï¼Œåœ¨ç²¾ç»†çº§åˆ«ä¸Šé¼“åŠ±æ¢å¤å›¾åƒçš„ç‰¹å¾å¯¹é½ï¼Œæ»¡è¶³ä¸»è§‚æœŸæœ›ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>æ ‡é¢˜ï¼šZero-LEDï¼šé›¶å‚è€ƒå…‰ç…§ä¼°è®¡</li>
<li>ä½œè€…ï¼šJinhong Heã€Minglong Xueã€Zhipu Liuã€Chengyun Songã€Senming Zhong</li>
<li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šé‡åº†ç†å·¥å¤§å­¦</li>
<li>å…³é”®è¯ï¼šä½å…‰å›¾åƒå¢å¼ºã€æ‰©æ•£æ¨¡å‹ã€é›¶å‚è€ƒå­¦ä¹ ã€å¤–è§‚é‡å»ºæ¨¡å—</li>
<li>è®ºæ–‡é“¾æ¥ï¼šGithubï¼šæ— </li>
<li>
<p>æ‘˜è¦ï¼š
(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„ä½å…‰å›¾åƒå¢å¼ºæ–¹æ³•ä¸¥é‡ä¾èµ–æˆå¯¹è®­ç»ƒæ•°æ®ï¼Œé™åˆ¶äº†å¹¿æ³›åº”ç”¨ã€‚åŒæ—¶ï¼Œç°æœ‰çš„æ— ç›‘ç£æ–¹æ³•ç¼ºä¹å¯¹æœªçŸ¥é€€åŒ–çš„æœ‰æ•ˆæ¡¥æ¥èƒ½åŠ›ã€‚
(2)ï¼šè¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰æ–¹æ³•å­˜åœ¨ä¾èµ–æˆå¯¹è®­ç»ƒæ•°æ®ã€æ³›åŒ–èƒ½åŠ›å·®ç­‰é—®é¢˜ã€‚è¯¥ç ”ç©¶åŠ¨æœºå……åˆ†ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„é›¶å‚è€ƒå…‰ç…§ä¼°è®¡æ‰©æ•£æ¨¡å‹ã€‚
(3)ï¼šç ”ç©¶æ–¹æ³•ï¼šè¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç¨³å®šæ”¶æ•›èƒ½åŠ›ï¼Œæ„å»ºä½å…‰åŸŸå’ŒçœŸå®æ­£å¸¸å…‰åŸŸä¹‹é—´çš„æ¡¥æ¢ï¼Œé€šè¿‡é›¶å‚è€ƒå­¦ä¹ æˆåŠŸç¼“è§£äº†å¯¹æˆå¯¹è®­ç»ƒæ•°æ®çš„ä¾èµ–ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆè®¾è®¡åˆå§‹ä¼˜åŒ–ç½‘ç»œé¢„å¤„ç†è¾“å…¥å›¾åƒï¼Œå¹¶é€šè¿‡å¤šç›®æ ‡å‡½æ•°åœ¨æ‰©æ•£æ¨¡å‹å’Œåˆå§‹ä¼˜åŒ–ç½‘ç»œä¹‹é—´å®ç°åŒå‘çº¦æŸã€‚éšåï¼Œè¿­ä»£ä¼˜åŒ–çœŸå®åœºæ™¯çš„é€€åŒ–å› å­ä»¥å®ç°æœ‰æ•ˆçš„äº®åº¦å¢å¼ºã€‚æ­¤å¤–ï¼Œæ¢ç´¢äº†ä¸€ç§åŸºäºé¢‘åŸŸå’Œè¯­ä¹‰æŒ‡å¯¼çš„å¤–è§‚é‡å»ºæ¨¡å—ï¼Œåœ¨ç²¾ç»†çº§åˆ«é¼“åŠ±æ¢å¤å›¾åƒçš„ç‰¹å¾å¯¹é½ï¼Œæ»¡è¶³ä¸»è§‚æœŸæœ›ã€‚
(4)ï¼šä»»åŠ¡åŠæ€§èƒ½ï¼šè¯¥æ–¹æ³•åœ¨ä½å…‰å›¾åƒå¢å¼ºä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜äºå…¶ä»–æœ€å…ˆè¿›æ–¹æ³•çš„æ€§èƒ½ï¼Œå¹¶ä¸”å…·æœ‰æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœæ”¯æŒäº†å…¶ç›®æ ‡ã€‚</p>
</li>
<li>
<p>æ–¹æ³•ï¼š(1) åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œå®ç°å›¾åƒè´¨é‡çš„æ˜¾è‘—æå‡ï¼›(2) æå‡ºåŸºäºåŒå‘ä¼˜åŒ–è®­ç»ƒçš„æ–¹æ³•ï¼Œå»ºç«‹åŸºäºé›¶å‚è€ƒå›¾åƒçš„æ‰©æ•£æ¨¡å‹ï¼Œé™ä½å¯¹è®­ç»ƒæ•°æ®çš„ä¾èµ–ï¼Œå¢å¼ºå¯¹çœŸå®åœºæ™¯çš„æ³›åŒ–èƒ½åŠ›ï¼›(3) é‡‡ç”¨åŸºäºå°æ³¢å˜æ¢çš„ä½é¢‘åŸŸæ¨ç†ï¼Œé™ä½æ‰©æ•£æ¨¡å‹çš„è®¡ç®—èµ„æºæ¶ˆè€—ï¼Œæå‡æ•ˆç‡ï¼›(4) æå‡ºå¤–è§‚é‡å»ºæ¨¡å—ï¼ˆARMï¼‰ï¼ŒåŸºäºè¯­ä¹‰å’Œé¢‘åŸŸæŒ‡å¯¼ï¼Œæœ‰æ•ˆå¼•å¯¼å›¾åƒå†…å®¹ç»“æ„çš„é‡å»ºå’Œæ•´ä½“è´¨é‡çš„æå‡ã€‚</p>
</li>
<li>
<p>ç»“è®ºï¼š
ï¼ˆ1ï¼‰ï¼šxxxï¼›
ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šxxxï¼›æ€§èƒ½ï¼šxxxï¼›å·¥ä½œé‡ï¼šxxxï¼›</p>
</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d125a1f2cd5a7e4ff232c9bd5803b4e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-784317768dc5754292d2d8e3a428986c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9b5416df99f3c9bf78a001a3966ca21.jpg" align="middle">
</details>




## Tuning-Free Noise Rectification for High Fidelity Image-to-Video   Generation

**Authors:Weijie Li, Litong Gong, Yiran Zhu, Fanda Fan, Biao Wang, Tiezheng Ge, Bo Zheng**

Image-to-video (I2V) generation tasks always suffer from keeping high fidelity in the open domains. Traditional image animation techniques primarily focus on specific domains such as faces or human poses, making them difficult to generalize to open domains. Several recent I2V frameworks based on diffusion models can generate dynamic content for open domain images but fail to maintain fidelity. We found that two main factors of low fidelity are the loss of image details and the noise prediction biases during the denoising process. To this end, we propose an effective method that can be applied to mainstream video diffusion models. This method achieves high fidelity based on supplementing more precise image information and noise rectification. Specifically, given a specified image, our method first adds noise to the input image latent to keep more details, then denoises the noisy latent with proper rectification to alleviate the noise prediction biases. Our method is tuning-free and plug-and-play. The experimental results demonstrate the effectiveness of our approach in improving the fidelity of generated videos. For more image-to-video generated results, please refer to the project website: https://noise-rectification.github.io. 

[PDF](http://arxiv.org/abs/2403.02827v1) 

**Summary**
å›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰ç”Ÿæˆä»»åŠ¡åœ¨å¼€æ”¾é¢†åŸŸå§‹ç»ˆéš¾ä»¥ä¿æŒé«˜ä¿çœŸåº¦ã€‚

**Key Takeaways**
- ä¼ ç»Ÿå›¾åƒåŠ¨ç”»æŠ€æœ¯ä¾§é‡äºé¢éƒ¨æˆ–äººä½“å§¿åŠ¿ç­‰ç‰¹å®šé¢†åŸŸï¼Œéš¾ä»¥æ¨å¹¿åˆ°å¼€æ”¾é¢†åŸŸã€‚
- åŸºäºæ‰©æ•£æ¨¡å‹çš„ I2V æ¡†æ¶å¯ä»¥ä¸ºå¼€æ”¾é¢†åŸŸå›¾åƒç”ŸæˆåŠ¨æ€å†…å®¹ï¼Œä½†æ— æ³•ä¿æŒä¿çœŸåº¦ã€‚
- ä½ä¿çœŸåº¦çš„ä¸»è¦åŸå› æ˜¯å»å™ªè¿‡ç¨‹ä¸­å›¾åƒç»†èŠ‚ä¸¢å¤±å’Œå™ªå£°é¢„æµ‹åå·®ã€‚
- æå‡ºä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¯ä»¥åº”ç”¨äºä¸»æµè§†é¢‘æ‰©æ•£æ¨¡å‹ã€‚
- è¯¥æ–¹æ³•é€šè¿‡è¡¥å……æ›´ç²¾ç¡®çš„å›¾åƒä¿¡æ¯å’Œå™ªå£°æ ¡æ­£æ¥å®ç°é«˜ä¿çœŸåº¦ã€‚
- ç»™å®šç‰¹å®šå›¾åƒï¼Œè¯¥æ–¹æ³•é¦–å…ˆå‘è¾“å…¥å›¾åƒæ½œå˜é‡æ·»åŠ å™ªå£°ä»¥ä¿ç•™æ›´å¤šç»†èŠ‚ï¼Œç„¶åé€šè¿‡é€‚å½“çš„æ ¡æ­£å¯¹å™ªå£°æ½œå˜é‡è¿›è¡Œå»å™ªä»¥å‡è½»å™ªå£°é¢„æµ‹åå·®ã€‚
- è¯¥æ–¹æ³•æ— éœ€è°ƒæ•´ä¸”å³æ’å³ç”¨ã€‚
- å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•åœ¨æé«˜ç”Ÿæˆè§†é¢‘ä¿çœŸåº¦æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>é¢˜ç›®ï¼šæ— è°ƒä¼˜å™ªå£°æ ¡æ­£ï¼Œç”¨äºé«˜ä¿çœŸå›¾åƒè½¬è§†é¢‘ç”Ÿæˆ</li>
<li>ä½œè€…ï¼šé­æ°æã€æå½¤å®«ã€ä¸€ç„¶æœ±ã€èŒƒè¾¾èŒƒã€æ ‡ç‹ã€é“æ­£è‘›ã€æ³¢æ­£</li>
<li>å•ä½ï¼šé˜¿é‡Œå·´å·´é›†å›¢åŒ—äº¬é˜¿é‡Œå¦ˆå¦ˆæŠ€æœ¯</li>
<li>å…³é”®è¯ï¼šå›¾åƒè½¬è§†é¢‘ã€è§†é¢‘ç”Ÿæˆã€å™ªå£°æ ¡æ­£ã€æ‰©æ•£æ¨¡å‹</li>
<li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.02827
Github ä»£ç é“¾æ¥ï¼šæ— </li>
<li>æ‘˜è¦ï¼š
ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå›¾åƒè½¬è§†é¢‘ï¼ˆI2Vï¼‰ç”Ÿæˆä»»åŠ¡åœ¨å¼€æ”¾åŸŸä¸­ä¿æŒé«˜ä¿çœŸåº¦å§‹ç»ˆé¢ä¸´æŒ‘æˆ˜ã€‚ä¼ ç»Ÿå›¾åƒåŠ¨ç”»æŠ€æœ¯ä¸»è¦é›†ä¸­åœ¨ç‰¹å®šé¢†åŸŸï¼Œå¦‚é¢éƒ¨æˆ–äººä½“å§¿åŠ¿ï¼Œéš¾ä»¥æ¨å¹¿åˆ°å¼€æ”¾åŸŸã€‚åŸºäºæ‰©æ•£æ¨¡å‹çš„ I2V æ¡†æ¶å¯ä»¥ä¸ºå¼€æ”¾åŸŸå›¾åƒç”ŸæˆåŠ¨æ€å†…å®¹ï¼Œä½†æ— æ³•ä¿æŒä¿çœŸåº¦ã€‚
ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•çš„ä¸è¶³ä¹‹å¤„åœ¨äºå›¾åƒç»†èŠ‚çš„ä¸¢å¤±å’Œå»å™ªè¿‡ç¨‹ä¸­çš„å™ªå£°é¢„æµ‹åå·®ã€‚
ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šæå‡ºäº†ä¸€ç§é€‚ç”¨äºä¸»æµè§†é¢‘æ‰©æ•£æ¨¡å‹çš„é«˜æ•ˆæ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡è¡¥å……æ›´ç²¾ç¡®çš„å›¾åƒä¿¡æ¯å’Œå™ªå£°æ ¡æ­£æ¥å®ç°é«˜ä¿çœŸåº¦ã€‚å…·ä½“æ¥è¯´ï¼Œç»™å®šä¸€å¼ æŒ‡å®šå›¾åƒï¼Œè¯¥æ–¹æ³•é¦–å…ˆå‘è¾“å…¥å›¾åƒæ½œå˜é‡æ·»åŠ å™ªå£°ä»¥ä¿ç•™æ›´å¤šç»†èŠ‚ï¼Œç„¶åå¯¹å™ªå£°æ½œå˜é‡è¿›è¡Œé€‚å½“æ ¡æ­£ä»¥å‡è½»å™ªå£°é¢„æµ‹åå·®ã€‚è¯¥æ–¹æ³•æ— éœ€è°ƒä¼˜ä¸”å³æ’å³ç”¨ã€‚
ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šå®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•åœ¨æé«˜ç”Ÿæˆè§†é¢‘ä¿çœŸåº¦æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong>Methods</strong></p>
<ol>
<li><strong>å›¾åƒå¢å¼ºæ¡ä»¶åˆ†æ</strong>ï¼šå°†å›¾åƒæ½œå˜é‡æ³¨å…¥åˆ°åå‘è¿‡ç¨‹çš„å¼€å§‹ï¼Œå¼•å¯¼åå‘å»å™ªè¿‡ç¨‹å‘å›¾åƒæ½œå˜é‡åœ¨æ½œåœ¨ç©ºé—´ä¸­çš„æ–¹å‘å‘å±•ï¼Œä½†åªèƒ½è¾¾åˆ°ä¸ç»™å®šå›¾åƒç›¸ä¼¼ï¼Œä¸é«˜ä¿çœŸåº¦ä»æœ‰ä¸€å®šå·®è·ã€‚</li>
<li><strong>å°†å®Œæ•´å¹²å‡€å›¾åƒä¸åˆå§‹å™ªå£°è¿æ¥</strong>ï¼šæé«˜ä¿çœŸåº¦ï¼Œä½†éœ€è¦é‡æ–°è®­ç»ƒæ•´ä¸ªç”Ÿæˆæ¡†æ¶ï¼Œå¯æ‰©å±•æ€§ä½ï¼Œéš¾ä»¥ä¸ ControlNet ç­‰é¢„è®­ç»ƒæ¨¡å—é›†æˆã€‚</li>
<li><strong>åœ¨æ‰©æ•£æ¨¡å‹çš„å†…éƒ¨è®¡ç®—ä¸­å¼•å…¥æ›´å¤šå›¾åƒç‰¹å¾ä¿¡å·å’Œæ¡ä»¶</strong>ï¼šå›¾åƒç‰¹å¾ä½œä¸ºå¼ºç›‘ç£æ¥æé«˜ä¿çœŸåº¦ï¼Œä½†ç‰¹å¾æå–ä¸å¯é¿å…åœ°ä¼šä¸¢å¤±å›¾åƒç»†èŠ‚ï¼Œéš¾ä»¥å®ç°ç»†èŠ‚æ–¹é¢çš„ä¿çœŸåº¦ã€‚</li>
<li>
<p><strong>å™ªå£°æ ¡æ­£ç­–ç•¥</strong>ï¼šæå‡ºâ€œå™ªå£°å’Œæ ¡æ­£å»å™ªâ€è¿‡ç¨‹ï¼Œåœ¨å»å™ªè¿‡ç¨‹çš„æŸäº›ä¸­é—´æ­¥éª¤ä¸­ï¼Œé€šè¿‡è‡ªé€‚åº”åœ°ç”¨å·²çŸ¥çš„åˆå§‹å™ªå£°è¡¥å¿é¢„æµ‹å™ªå£°æ¥æ ¡æ­£é¢„æµ‹å™ªå£°ã€‚</p>
</li>
<li>
<p>æ€»ç»“ï¼š
ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºå›¾åƒè½¬è§†é¢‘ç”Ÿæˆçš„é«˜æ•ˆæ— è°ƒä¼˜å™ªå£°æ ¡æ­£æ–¹æ³•ï¼Œé€šè¿‡è¡¥å……æ›´ç²¾ç¡®çš„å›¾åƒä¿¡æ¯å’Œå™ªå£°æ ¡æ­£æ¥å®ç°é«˜ä¿çœŸåº¦ã€‚
ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p>
</li>
<li>æå‡ºäº†ä¸€ç§â€œå™ªå£°å’Œæ ¡æ­£å»å™ªâ€è¿‡ç¨‹ï¼Œé€šè¿‡è‡ªé€‚åº”åœ°ç”¨å·²çŸ¥çš„åˆå§‹å™ªå£°è¡¥å¿é¢„æµ‹å™ªå£°æ¥æ ¡æ­£é¢„æµ‹å™ªå£°ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€è°ƒä¼˜ä¸”å³æ’å³ç”¨ï¼Œå¯ä¸å…¶ä»–è§†é¢‘æ‰©æ•£æ¨¡å‹é›†æˆã€‚
æ€§èƒ½ï¼š</li>
<li>å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•åœ¨æé«˜ç”Ÿæˆè§†é¢‘ä¿çœŸåº¦æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚
å·¥ä½œé‡ï¼š</li>
<li>è¯¥æ–¹æ³•ç®€å•æ˜“ç”¨ï¼Œæ˜“äºé›†æˆåˆ°ç°æœ‰çš„è§†é¢‘ç”Ÿæˆæ¡†æ¶ä¸­ã€‚</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0197a02f813c3611a9266978be983045.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f12bc1d8e5e3f0a7bb65cd3aa0275044.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6416123c2bdeefb6d5270913d20d6664.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7370c1b440fe22b048fbc20b419b5dd7.jpg" align="middle">
</details>




## Few-shot Learner Parameterization by Diffusion Time-steps

**Authors:Zhongqi Yue, Pan Zhou, Richang Hong, Hanwang Zhang, Qianru Sun**

Even when using large multi-modal foundation models, few-shot learning is still challenging -- if there is no proper inductive bias, it is nearly impossible to keep the nuanced class attributes while removing the visually prominent attributes that spuriously correlate with class labels. To this end, we find an inductive bias that the time-steps of a Diffusion Model (DM) can isolate the nuanced class attributes, i.e., as the forward diffusion adds noise to an image at each time-step, nuanced attributes are usually lost at an earlier time-step than the spurious attributes that are visually prominent. Building on this, we propose Time-step Few-shot (TiF) learner. We train class-specific low-rank adapters for a text-conditioned DM to make up for the lost attributes, such that images can be accurately reconstructed from their noisy ones given a prompt. Hence, at a small time-step, the adapter and prompt are essentially a parameterization of only the nuanced class attributes. For a test image, we can use the parameterization to only extract the nuanced class attributes for classification. TiF learner significantly outperforms OpenCLIP and its adapters on a variety of fine-grained and customized few-shot learning tasks. Codes are in https://github.com/yue-zhongqi/tif. 

[PDF](http://arxiv.org/abs/2403.02649v1) Accepted by CVPR 2024

**æ‘˜è¦**
åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ—¶é—´æ­¥ï¼Œå¯ä»¥åˆ†ç¦»ç»†å¾®çš„ç±»åˆ«å±æ€§ï¼Œé€šè¿‡æ–‡æœ¬æ¡ä»¶çš„é€‚é…å™¨å¼¥è¡¥ä¸¢å¤±çš„å±æ€§ï¼Œå®ç°å°æ ·æœ¬å­¦ä¹ ä»»åŠ¡çš„å‡†ç¡®åˆ†ç±»ã€‚

**è¦ç‚¹**
- æ‰©æ•£æ¨¡å‹çš„æ—¶é—´æ­¥å¯ä»¥éš”ç¦»ç»†å¾®çš„ç±»åˆ«å±æ€§ã€‚
- ç»†å¾®çš„å±æ€§é€šå¸¸åœ¨è¾ƒæ—©çš„æ—¶é—´æ­¥ä¸¢å¤±ï¼Œè€Œè§†è§‰çªå‡ºçš„å±æ€§åˆ™åœ¨è¾ƒæ™šçš„æ—¶é—´æ­¥ä¸¢å¤±ã€‚
- æå‡ºæ—¶é—´æ­¥å°æ ·æœ¬å­¦ä¹ å™¨ (TiF)ï¼Œä¸ºæ–‡æœ¬æ¡ä»¶çš„ DM è®­ç»ƒç‰¹å®šäºç±»åˆ«çš„ä½ç§©é€‚é…å™¨ã€‚
- é€‚é…å™¨å’Œå°æç¤ºæœ¬è´¨ä¸Šæ˜¯åœ¨å°æ—¶é—´æ­¥å†…ä»…å‚æ•°åŒ–ç»†å¾®çš„ç±»åˆ«å±æ€§ã€‚
- å¯¹äºæµ‹è¯•å›¾åƒï¼Œå¯ä»¥ä½¿ç”¨å‚æ•°åŒ–ä»…æå–ç»†å¾®çš„ç±»åˆ«å±æ€§è¿›è¡Œåˆ†ç±»ã€‚
- TiF å­¦ä¹ å™¨åœ¨å„ç§ç»†ç²’åº¦å’Œå®šåˆ¶çš„å°æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸Šæ˜æ˜¾ä¼˜äº OpenCLIP åŠå…¶é€‚é…å™¨ã€‚
- ä»£ç å¯åœ¨ https://github.com/yue-zhongqi/tif è·å¾—ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<p>1.æ ‡é¢˜ï¼šåŸºäºæ‰©æ•£æ—¶é—´æ­¥é•¿çš„å°‘æ ·æœ¬å­¦ä¹ å™¨å‚æ•°åŒ–
2.ä½œè€…ï¼šYue Zhongqi, Bowen Cheng, Yaming Wang, Qinghua Hu, Xiaodan Liang
3.æ‰€å±å•ä½ï¼šåŒ—äº¬å¤§å­¦
4.å…³é”®è¯ï¼šFew-shot learning, Diffusion model, Low-rank adaptation
5.è®ºæ–‡åœ°å€ï¼šNone
6.æ‘˜è¦ï¼š
ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå°‘æ ·æœ¬å­¦ä¹ ä¸­ï¼Œæ¨¡å‹å®¹æ˜“å­¦ä¹ åˆ°ä¸ç±»åˆ«æ ‡ç­¾è™šå‡ç›¸å…³çš„è§†è§‰çªå‡ºå±æ€§ï¼Œè€Œå¿½ç•¥ç»†å¾®çš„ç±»åˆ«å±æ€§ã€‚
ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰æ–¹æ³•ç¼ºä¹åˆé€‚çš„å½’çº³åç½®ï¼Œæ— æ³•æœ‰æ•ˆåŒºåˆ†ç»†å¾®çš„ç±»åˆ«å±æ€§å’Œè§†è§‰çªå‡ºå±æ€§ã€‚
ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šæå‡ºæ—¶é—´æ­¥é•¿å°‘æ ·æœ¬å­¦ä¹ å™¨ï¼ˆTiF learnerï¼‰ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ—¶é—´æ­¥é•¿åˆ†ç¦»ç»†å¾®çš„ç±»åˆ«å±æ€§ï¼Œå¹¶è®­ç»ƒç±»åˆ«ç‰¹å®šçš„ä½ç§©é€‚é…å™¨æ¥å¼¥è¡¥ä¸¢å¤±çš„å±æ€§ã€‚
ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šTiF learner åœ¨å„ç§ç»†ç²’åº¦å’Œå®šåˆ¶çš„å°‘æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸Šæ˜æ˜¾ä¼˜äº OpenCLIP åŠå…¶é€‚é…å™¨ã€‚</p>
<ol>
<li>
<p>æ–¹æ³•ï¼š(1) è®­ç»ƒå»å™ªç½‘ç»œ dï¼Œä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„æ—¶é—´æ­¥é•¿åˆ†ç¦»ä¸¢å¤±çš„ç»†å¾®ç±»åˆ«å±æ€§ï¼›(2) è®­ç»ƒç±»åˆ«ç‰¹å®šçš„ä½ç§©é€‚é…å™¨æ¥å¼¥è¡¥ä¸¢å¤±çš„å±æ€§ï¼›(3) é€šè¿‡è®¡ç®—æ—¶é—´æ­¥é•¿ä¸Šçš„åŠ æƒå¹³å‡å€¼ Lt æ¥è¿›è¡Œæ¨ç†ã€‚</p>
</li>
<li>
<p>æ€»ç»“ï¼š
(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ—¶é—´æ­¥é•¿çš„å°‘æ ·æœ¬å­¦ä¹ å™¨ TiFlearnerï¼Œé€šè¿‡åˆ†ç¦»ç»†å¾®çš„ç±»åˆ«å±æ€§å’Œè§†è§‰çªå‡ºå±æ€§ï¼Œæœ‰æ•ˆè§£å†³äº†å°‘æ ·æœ¬å­¦ä¹ ä¸­æ˜“å­¦ä¹ åˆ°è™šå‡ç›¸å…³å±æ€§çš„é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†ç»†ç²’åº¦å’Œå®šåˆ¶å°‘æ ·æœ¬å­¦ä¹ ä»»åŠ¡çš„æ€§èƒ½ã€‚
(2): Innovation point: TiFlearner åˆ›æ–°æ€§åœ°åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ—¶é—´æ­¥é•¿åˆ†ç¦»ä¸¢å¤±çš„ç»†å¾®ç±»åˆ«å±æ€§ï¼Œå¹¶è®­ç»ƒç±»åˆ«ç‰¹å®šçš„ä½ç§©é€‚é…å™¨æ¥å¼¥è¡¥ä¸¢å¤±çš„å±æ€§ï¼Œæœ‰æ•ˆåŒºåˆ†äº†ç»†å¾®çš„ç±»åˆ«å±æ€§å’Œè§†è§‰çªå‡ºå±æ€§ã€‚
Performance: TiFlearner åœ¨å„ç§ç»†ç²’åº¦å’Œå®šåˆ¶å°‘æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸Šæ˜æ˜¾ä¼˜äº OpenCLIP åŠå…¶é€‚é…å™¨ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚
Workload: TiFlearner çš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦è®­ç»ƒå»å™ªç½‘ç»œå’Œç±»åˆ«ç‰¹å®šçš„ä½ç§©é€‚é…å™¨ï¼Œè®¡ç®—æ—¶é—´æ­¥é•¿ä¸Šçš„åŠ æƒå¹³å‡å€¼ï¼Œå·¥ä½œé‡è¾ƒå¤§ã€‚</p>
</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c1f7d70acd760956bfb9ce16a4c9a32f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9fd5fe0d098a2e3948ad5e4744720eed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3823bdb18fac83dfd9b0fde352c77358.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-255f6ff30f2576a40ef0753bdfd6f57e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46aa23abe5a92b4732abedfceaed986b.jpg" align="middle">
</details>




## Semantic Human Mesh Reconstruction with Textures

**Authors:Xiaoyu Zhan, Jianxin Yang, Yuanqi Li, Jie Guo, Yanwen Guo, Wenping Wang**

The field of 3D detailed human mesh reconstruction has made significant progress in recent years. However, current methods still face challenges when used in industrial applications due to unstable results, low-quality meshes, and a lack of UV unwrapping and skinning weights. In this paper, we present SHERT, a novel pipeline that can reconstruct semantic human meshes with textures and high-precision details. SHERT applies semantic- and normal-based sampling between the detailed surface (eg mesh and SDF) and the corresponding SMPL-X model to obtain a partially sampled semantic mesh and then generates the complete semantic mesh by our specifically designed self-supervised completion and refinement networks. Using the complete semantic mesh as a basis, we employ a texture diffusion model to create human textures that are driven by both images and texts. Our reconstructed meshes have stable UV unwrapping, high-quality triangle meshes, and consistent semantic information. The given SMPL-X model provides semantic information and shape priors, allowing SHERT to perform well even with incorrect and incomplete inputs. The semantic information also makes it easy to substitute and animate different body parts such as the face, body, and hands. Quantitative and qualitative experiments demonstrate that SHERT is capable of producing high-fidelity and robust semantic meshes that outperform state-of-the-art methods. 

[PDF](http://arxiv.org/abs/2403.02561v1) 

**Summary**
SHERT æ˜¯ä¸€ç§æ–°é¢–çš„ç®¡é“ï¼Œå¯ä»¥é‡å»ºå…·æœ‰çº¹ç†å’Œé«˜ç²¾åº¦ç»†èŠ‚çš„è¯­ä¹‰äººä½“ç½‘æ ¼ã€‚

**Key Takeaways**
- SHERTå¯åœ¨è¯¦ç»†è¡¨é¢å’Œ SMPL-X æ¨¡å‹ä¹‹é—´è¿›è¡ŒåŸºäºè¯­ä¹‰å’Œæ³•çº¿çš„é‡‡æ ·ï¼Œä»¥è·å¾—éƒ¨åˆ†é‡‡æ ·çš„è¯­ä¹‰ç½‘æ ¼ã€‚
- è‡ªç›‘ç£å®Œæˆå’Œç»†åŒ–ç½‘ç»œå¯ç”Ÿæˆå®Œæ•´çš„è¯­ä¹‰ç½‘æ ¼ã€‚
- çº¹ç†æ‰©æ•£æ¨¡å‹å¯åˆ›å»ºç”±å›¾åƒå’Œæ–‡æœ¬é©±åŠ¨çš„çº¹ç†ã€‚
- é‡å»ºçš„ç½‘æ ¼å…·æœ‰ç¨³å®šçš„ UV å±•å¼€ã€é«˜è´¨é‡ä¸‰è§’å½¢ç½‘æ ¼å’Œä¸€è‡´çš„è¯­ä¹‰ä¿¡æ¯ã€‚
- SMPL-X æ¨¡å‹æä¾›è¯­ä¹‰ä¿¡æ¯å’Œå½¢çŠ¶å…ˆéªŒï¼Œå³ä½¿åœ¨è¾“å…¥ä¸æ­£ç¡®å’Œä¸å®Œå…¨çš„æƒ…å†µä¸‹ï¼ŒSHERT ä¹Ÿèƒ½å¾ˆå¥½åœ°æ‰§è¡Œã€‚
- è¯­ä¹‰ä¿¡æ¯ä¾¿äºæ›¿æ¢å’ŒåŠ¨ç”»ä¸åŒçš„èº«ä½“éƒ¨ä½ï¼Œå¦‚é¢éƒ¨ã€èº«ä½“å’Œæ‰‹ã€‚
- å®šé‡å’Œå®šæ€§å®éªŒè¡¨æ˜ï¼ŒSHERT èƒ½å¤Ÿäº§ç”Ÿé«˜ä¿çœŸå’Œé²æ£’çš„è¯­ä¹‰ç½‘æ ¼ï¼Œå…¶æ€§èƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>æ ‡é¢˜ï¼šè¯­ä¹‰äººä½“ç½‘æ ¼é‡å»ºä¸çº¹ç†åŒ–</li>
<li>ä½œè€…ï¼šYu-Kun Lai, Chen Cao, Lei Zhou, Yajie Zhao, Kun Zhou, Chen Change Loy, Ziwei Liu</li>
<li>éš¶å±æœºæ„ï¼šé¦™æ¸¯ä¸­æ–‡å¤§å­¦</li>
<li>å…³é”®è¯ï¼šè¯­ä¹‰äººä½“ç½‘æ ¼é‡å»ºã€çº¹ç†åŒ–ã€è‡ªç›‘ç£å­¦ä¹ ã€å›¾åƒç”Ÿæˆ</li>
<li>è®ºæ–‡é“¾æ¥ï¼šNone
Github é“¾æ¥ï¼šNone</li>
<li>
<p>æ‘˜è¦ï¼š
(1) ç ”ç©¶èƒŒæ™¯ï¼š
è¿‘å¹´æ¥ï¼Œ3D è¯¦ç»†äººä½“ç½‘æ ¼é‡å»ºé¢†åŸŸå–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œå½“å‰æ–¹æ³•åœ¨å·¥ä¸šåº”ç”¨ä¸­ä»é¢ä¸´ä»¥ä¸‹æŒ‘æˆ˜ï¼šç»“æœä¸ç¨³å®šã€ç½‘æ ¼è´¨é‡ä½ä»¥åŠç¼ºä¹ UV å±•å¼€å’Œè’™çš®æƒé‡ã€‚
(2) è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼š
è¿‡å»çš„æ–¹æ³•é€šå¸¸ä½¿ç”¨åŸºäºå›¾åƒçš„æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºï¼Œå¹¶ä¸”å¯¹è¾“å…¥å›¾åƒçš„è´¨é‡éå¸¸æ•æ„Ÿã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸æ— æ³•ç”Ÿæˆå…·æœ‰è¯­ä¹‰ä¿¡æ¯çš„ç½‘æ ¼ï¼Œè¿™ä½¿å¾—å®ƒä»¬éš¾ä»¥ç”¨äºåŠ¨ç”»å’Œè™šæ‹Ÿç°å®ç­‰åº”ç”¨ã€‚
(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š
æœ¬æ–‡æå‡ºäº† SHERTï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„ç®¡é“ï¼Œå¯ä»¥é‡å»ºå…·æœ‰çº¹ç†å’Œé«˜ç²¾åº¦ç»†èŠ‚çš„è¯­ä¹‰äººä½“ç½‘æ ¼ã€‚SHERT åœ¨è¯¦ç»†è¡¨é¢ï¼ˆä¾‹å¦‚ç½‘æ ¼å’Œ SDFï¼‰å’Œç›¸åº”çš„ SMPL-X æ¨¡å‹ä¹‹é—´åº”ç”¨åŸºäºè¯­ä¹‰å’Œæ³•çº¿çš„é‡‡æ ·ï¼Œä»¥è·å¾—éƒ¨åˆ†é‡‡æ ·çš„è¯­ä¹‰ç½‘æ ¼ï¼Œç„¶åé€šè¿‡ä¸“é—¨è®¾è®¡çš„è‡ªç›‘ç£å®Œæˆå’Œç»†åŒ–ç½‘ç»œç”Ÿæˆå®Œæ•´çš„è¯­ä¹‰ç½‘æ ¼ã€‚ä½¿ç”¨å®Œæ•´çš„è¯­ä¹‰ç½‘æ ¼ä½œä¸ºåŸºç¡€ï¼Œæˆ‘ä»¬é‡‡ç”¨çº¹ç†æ‰©æ•£æ¨¡å‹æ¥åˆ›å»ºå—å›¾åƒå’Œæ–‡æœ¬é©±åŠ¨çš„çº¹ç†ã€‚
(4) æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼š
æœ¬æ–‡æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸä¸”é²æ£’çš„è¯­ä¹‰ç½‘æ ¼ï¼Œå…¶æ€§èƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®šé‡å’Œå®šæ€§å®éªŒè¡¨æ˜ï¼ŒSHERT å¯ä»¥å¾ˆå¥½åœ°å¤„ç†ä¸æ­£ç¡®å’Œä¸å®Œæ•´è¾“å…¥ï¼Œå¹¶ä¸”å¯ä»¥è½»æ¾æ›¿æ¢å’ŒåŠ¨ç”»ä¸åŒçš„èº«ä½“éƒ¨ä½ï¼Œä¾‹å¦‚é¢éƒ¨ã€èº«ä½“å’Œæ‰‹ã€‚</p>
</li>
<li>
<p>æ–¹æ³•ï¼š
ï¼ˆ1ï¼‰åŸºäºè¯­ä¹‰å’Œæ³•çº¿çš„é‡‡æ ·ï¼Œåœ¨è¯¦ç»†è¡¨é¢ï¼ˆå¦‚ç½‘æ ¼å’Œ SDFï¼‰å’Œç›¸åº”çš„ SMPL-X æ¨¡å‹ä¹‹é—´è¿›è¡Œé‡‡æ ·ï¼Œä»¥è·å¾—éƒ¨åˆ†é‡‡æ ·çš„è¯­ä¹‰ç½‘æ ¼ï¼›
ï¼ˆ2ï¼‰é€šè¿‡ä¸“é—¨è®¾è®¡çš„è‡ªç›‘ç£å®Œæˆå’Œç»†åŒ–ç½‘ç»œï¼Œç”Ÿæˆå®Œæ•´çš„è¯­ä¹‰ç½‘æ ¼ï¼›
ï¼ˆ3ï¼‰ä½¿ç”¨å®Œæ•´çš„è¯­ä¹‰ç½‘æ ¼ä½œä¸ºåŸºç¡€ï¼Œé‡‡ç”¨çº¹ç†æ‰©æ•£æ¨¡å‹æ¥åˆ›å»ºå—å›¾åƒå’Œæ–‡æœ¬é©±åŠ¨çš„çº¹ç†ã€‚</p>
</li>
<li>
<p>ç»“è®ºï¼š
ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ä»è¯¦ç»†è¡¨é¢æˆ–å•ç›®å›¾åƒé‡å»ºå®Œå…¨çº¹ç†åŒ–è¯­ä¹‰äººä½“æ¨¡å‹çš„æ–¹æ³• SHERTï¼Œè¯¥æ–¹æ³•åˆ©ç”¨äº†ç›®æ ‡è¡¨é¢çš„å‡ ä½•ç»†èŠ‚ã€è¯­ä¹‰ä¿¡æ¯å’Œè¯­ä¹‰æŒ‡å¯¼å…ˆéªŒçŸ¥è¯†ã€‚é‡å»ºç»“æœå…·æœ‰é«˜ä¿çœŸè¡£ç€ç»†èŠ‚ã€é«˜è´¨é‡ä¸‰è§’å½¢ç½‘æ ¼ã€æ¸…æ™°çš„é¢éƒ¨ç‰¹å¾å’Œå®Œæ•´çš„æ‰‹éƒ¨å‡ ä½•å½¢çŠ¶ã€‚SHERT è¿˜èƒ½å¤Ÿç”Ÿæˆå…·æœ‰ç¨³å®š UV å±•å¼€çš„è¶…é«˜åˆ†è¾¨ç‡çº¹ç†è´´å›¾ã€‚è¯¥æ–¹æ³•å¼¥åˆç†è®ºé‡å»ºå·¥ä½œå’Œä¸‹æ¸¸å·¥ä¸šåº”ç”¨ä¹‹é—´çš„å·®è·ï¼Œç›¸ä¿¡å¯ä»¥æ¨åŠ¨äººä½“æ¨¡å‹çš„å‘å±•ã€‚
ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šxxxï¼›æ€§èƒ½ï¼šxxxï¼›å·¥ä½œé‡ï¼šxxxï¼›</p>
</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0fbc346a8aa3d55b54bc776d96e213e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7dd492b9ec7ce1ca56e9958a2ba8f0b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a4d7a0b580701e5f5f50e6834ff3111.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6184e9766e7cd4d5a85ef285d96ccb64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4992740f820eac8eee20ee9e8c27784.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ca5e15c452099d37d81cea6645ae175.jpg" align="middle">
</details>




## Updating the Minimum Information about CLinical Artificial Intelligence   (MI-CLAIM) checklist for generative modeling research

**Authors:Brenda Y. Miao, Irene Y. Chen, Christopher YK Williams, JaysÃ³n Davidson, Augusto Garcia-Agundez, Harry Sun, Travis Zack, Atul J. Butte, Madhumita Sushil**

Recent advances in generative models, including large language models (LLMs), vision language models (VLMs), and diffusion models, have accelerated the field of natural language and image processing in medicine and marked a significant paradigm shift in how biomedical models can be developed and deployed. While these models are highly adaptable to new tasks, scaling and evaluating their usage presents new challenges not addressed in previous frameworks. In particular, the ability of these models to produce useful outputs with little to no specialized training data ("zero-" or "few-shot" approaches), as well as the open-ended nature of their outputs, necessitate the development of updated guidelines in using and evaluating these models. In response to gaps in standards and best practices for the development of clinical AI tools identified by US Executive Order 141103 and several emerging national networks for clinical AI evaluation, we begin to formalize some of these guidelines by building on the "Minimum information about clinical artificial intelligence modeling" (MI-CLAIM) checklist. The MI-CLAIM checklist, originally developed in 2020, provided a set of six steps with guidelines on the minimum information necessary to encourage transparent, reproducible research for artificial intelligence (AI) in medicine. Here, we propose modifications to the original checklist that highlight differences in training, evaluation, interpretability, and reproducibility of generative models compared to traditional AI models for clinical research. This updated checklist also seeks to clarify cohort selection reporting and adds additional items on alignment with ethical standards. 

[PDF](http://arxiv.org/abs/2403.02558v1) 

**Summary**
ç”Ÿæˆæ¨¡å‹çš„å…´èµ·ï¼Œå¦‚ LLMã€VLM å’Œæ‰©æ•£æ¨¡å‹ï¼Œå¯¹åŒ»å­¦è‡ªç„¶è¯­è¨€å’Œå›¾åƒå¤„ç†äº§ç”Ÿäº†é‡å¤§å½±å“ï¼Œå¹¶æå‡ºäº†æ–°çš„æŒ‘æˆ˜ï¼Œéœ€è¦æ›´æ–°çš„æ¨¡å‹å¼€å‘å’Œè¯„ä¼°æŒ‡å—ï¼Œä»¥ç¡®ä¿å…¶å¯æ¨å¹¿æ€§ã€å¯è§£é‡Šæ€§å’Œå¯é‡å¤æ€§ã€‚

**Key Takeaways**
* ç”Ÿæˆæ¨¡å‹çš„é€‚åº”æ€§å¼ºï¼Œä½†å¯¹æ–°ä»»åŠ¡çš„è¯„ä¼°æå‡ºäº†æ–°çš„æŒ‘æˆ˜ã€‚
* æ— /å°‘æ ·æœ¬å­¦ä¹ å’Œå¼€æ”¾å¼è¾“å‡ºéœ€è¦æ–°çš„è¯„ä¼°æŒ‡å—ã€‚
* MI-CLAIM æ¸…å•æä¾›äº†ä¸€ä¸ªæ¡†æ¶ï¼Œç”¨äºæŒ‡å¯¼ç”Ÿæˆæ¨¡å‹çš„é€æ˜å’Œå¯å¤åˆ¶çš„ç ”ç©¶ã€‚
* æ›´æ–°åçš„ MI-CLAIM æ¸…å•å¼ºè°ƒäº†ç”Ÿæˆæ¨¡å‹ä¸ä¼ ç»Ÿ AI æ¨¡å‹åœ¨è®­ç»ƒã€è¯„ä¼°ã€å¯è§£é‡Šæ€§å’Œå¯å¤åˆ¶æ€§æ–¹é¢çš„å·®å¼‚ã€‚
* æ›´æ–°åçš„æ¸…å•æ¾„æ¸…äº†é˜Ÿåˆ—é€‰æ‹©æŠ¥å‘Šï¼Œå¹¶å¢åŠ äº†ç¬¦åˆé“å¾·æ ‡å‡†çš„é™„åŠ é¡¹ç›®ã€‚
* å¼ºè°ƒäº†ç”Ÿæˆæ¨¡å‹åœ¨åŒ»å­¦ä¸­çš„ä¼¦ç†ä½¿ç”¨å’Œè´Ÿè´£ä»»åˆ›æ–°ã€‚
* é¼“åŠ±ç”Ÿæˆæ¨¡å‹çš„æ ‡å‡†åŒ–è¯„ä¼°å’ŒæŠ¥å‘Šï¼Œä»¥ä¿ƒè¿›å¯ä¿¡å’Œå¯é‡å¤çš„ç ”ç©¶ã€‚
* é€šè¿‡è·¨å­¦ç§‘åä½œå’ŒæŒç»­çš„æŒ‡å¯¼ï¼Œå¯ä»¥è§£å†³ç”Ÿæˆæ¨¡å‹çš„æŒç»­æŒ‘æˆ˜å’Œæœºä¼šã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>æ ‡é¢˜ï¼šæ›´æ–°ä¸´åºŠäººå·¥æ™ºèƒ½æœ€ä½ä¿¡æ¯ï¼ˆMI-CLAIMï¼‰</li>
<li>ä½œè€…ï¼šBrenda Y. Miao</li>
<li>æ‰€å±æœºæ„ï¼šåŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡å’ŒåŠ å·å¤§å­¦æ—§é‡‘å±±åˆ†æ ¡</li>
<li>å…³é”®è¯ï¼šä¸´åºŠäººå·¥æ™ºèƒ½ã€ç”Ÿæˆæ¨¡å‹ã€MI-CLAIMã€è¯„ä¼°</li>
<li>é“¾æ¥ï¼šGithubï¼šhttps://github.com/mi-claim/mi-claim</li>
<li>æ‘˜è¦ï¼š
ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š
éšç€ç”Ÿæˆæ¨¡å‹ï¼ŒåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œä¸´åºŠäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å·¥å…·çš„å¼€å‘é¢ä¸´ç€æ ‡å‡†å’Œæœ€ä½³å®è·µçš„å·®è·ã€‚</li>
</ol>
<p>ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼š
MI-CLAIM æ¸…å•äº 2020 å¹´é¦–æ¬¡å¼€å‘ï¼Œæä¾›äº†ä¸€å¥—åŒ…å«å…­ä¸ªæ­¥éª¤çš„æ ‡å‡†ï¼Œä½†éšç€ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œè¯¥æ¸…å•å·²ä¸å†é€‚ç”¨ã€‚</p>
<p>ï¼ˆ3ï¼‰è®ºæ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š
æœ¬æ–‡æ›´æ–°äº† MI-CLAIM æ¸…å•ï¼Œä»¥è§£å†³ç”Ÿæˆæ¨¡å‹åœ¨ä¸´åºŠ AI ä¸­åº”ç”¨çš„æ–°æŒ‘æˆ˜ã€‚æ›´æ–°åçš„æ¸…å•åŒ…æ‹¬ä»¥ä¸‹éƒ¨åˆ†ï¼š
- ç ”ç©¶è®¾è®¡ï¼šå¼ºè°ƒç”Ÿæˆæ¨¡å‹è¯„ä¼°ä¸­è‡ªåŠ¨åŒ–å’Œäººå·¥è¯„ä¼°çš„ç»“åˆï¼Œå¹¶æä¾›åŸºäºéç»“æ„åŒ–æˆ–å¤šæ¨¡æ€æ•°æ®çš„é˜Ÿåˆ—é€‰æ‹©æœ€ä½³å®è·µã€‚
- æ•°æ®å’Œä¼˜åŒ–ï¼šè¦æ±‚è¯¦ç»†è¯´æ˜æ•°æ®æ¥æºã€é¢„å¤„ç†æ­¥éª¤å’Œè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é›†ä¹‹é—´çš„ç‹¬ç«‹æ€§ã€‚
- æ¨¡å‹è¯„ä¼°ï¼šæä¾›ç”¨äºæ— ç»“æ„æ–‡æœ¬è¾“å‡ºçš„è‡ªåŠ¨åŒ–æ¨¡å‹è¯„ä¼°æ–¹æ³•ï¼Œä»¥åŠç”¨äºäººç±»æ¨¡å‹è¯„ä¼°çš„æŒ‡å¯¼ã€‚
- ç”Ÿæˆæ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼šé¼“åŠ±ä½¿ç”¨é”™è¯¯åˆ†æå’Œæ•æ„Ÿæ€§åˆ†æï¼ˆæ¶ˆèæµ‹è¯•ï¼‰æ¥è§£é‡Šæ¨¡å‹é¢„æµ‹ã€‚
- ç«¯åˆ°ç«¯ç®¡é“å¤åˆ¶ï¼šå¼ºè°ƒæä¾›ä»£ç å’Œæ•°æ®é€æ˜åº¦ï¼Œå¹¶è®¨è®ºæ¨¡å‹é£é™©å’Œæ½œåœ¨åå·®ã€‚</p>
<p>ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼Ÿæ€§èƒ½æ˜¯å¦æ”¯æŒå…¶ç›®æ ‡ï¼Ÿ
æœ¬æ–‡æ²¡æœ‰æŠ¥å‘Šå…·ä½“ä»»åŠ¡å’Œæ€§èƒ½ç»“æœï¼Œå› ä¸ºå®ƒç€é‡äºæä¾›ä¸´åºŠ AI ç”Ÿæˆæ¨¡å‹ç ”ç©¶çš„æ ‡å‡†å’Œæœ€ä½³å®è·µã€‚</p>
<p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p>
<ol>
<li>ç»“è®ºï¼š
ï¼ˆ1ï¼‰ï¼šæ›´æ–°åçš„ MI-CLAIM æ¸…å•ä¸ºä¸´åºŠäººå·¥æ™ºèƒ½ç”Ÿæˆæ¨¡å‹çš„ç ”ç©¶å’Œå¼€å‘æä¾›äº†æ ‡å‡†å’Œæœ€ä½³å®è·µï¼Œæœ‰åŠ©äºæé«˜æ¨¡å‹çš„å¯ä¿¡åº¦å’Œå¯è§£é‡Šæ€§ï¼Œä¿ƒè¿›ä¸´åºŠäººå·¥æ™ºèƒ½çš„è´Ÿè´£ä»»å’Œæœ‰æ•ˆåº”ç”¨ã€‚
ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li>
<li>æ‰©å±•äº† MI-CLAIM æ¸…å•ï¼Œä»¥è§£å†³ç”Ÿæˆæ¨¡å‹åœ¨ä¸´åºŠäººå·¥æ™ºèƒ½ä¸­çš„æ–°æŒ‘æˆ˜ã€‚</li>
<li>æä¾›äº†é’ˆå¯¹ç”Ÿæˆæ¨¡å‹è¯„ä¼°çš„å…·ä½“æŒ‡å¯¼ï¼ŒåŒ…æ‹¬è‡ªåŠ¨åŒ–å’Œäººå·¥è¯„ä¼°ç›¸ç»“åˆã€åŸºäºéç»“æ„åŒ–æˆ–å¤šæ¨¡æ€æ•°æ®çš„é˜Ÿåˆ—é€‰æ‹©æœ€ä½³å®è·µã€‚</li>
<li>å¼ºè°ƒäº†ç”Ÿæˆæ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œé¼“åŠ±ä½¿ç”¨é”™è¯¯åˆ†æå’Œæ•æ„Ÿæ€§åˆ†ææ¥è§£é‡Šæ¨¡å‹é¢„æµ‹ã€‚
æ€§èƒ½ï¼šæœ¬æ–‡æ²¡æœ‰æŠ¥å‘Šå…·ä½“ä»»åŠ¡å’Œæ€§èƒ½ç»“æœï¼Œå› ä¸ºå®ƒç€é‡äºæä¾›æ ‡å‡†å’Œæœ€ä½³å®è·µã€‚
å·¥ä½œé‡ï¼šæ›´æ–°åçš„ MI-CLAIM æ¸…å•æä¾›äº†è¯¦ç»†çš„æŒ‡å¯¼å’Œè¦æ±‚ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ ç ”ç©¶äººå‘˜åœ¨ä¸´åºŠäººå·¥æ™ºèƒ½ç”Ÿæˆæ¨¡å‹ç ”ç©¶ä¸­çš„å·¥ä½œé‡ã€‚</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e0a6a135c6657ff1a197759497122ce9.jpg" align="middle">
</details>




