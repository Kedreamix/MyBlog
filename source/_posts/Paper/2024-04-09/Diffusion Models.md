
---
title: Diffusion Models
date: 2024-04-09 16:10:25
author: Kedreamix
cover: https://pica.zhimg.com/v2-4b111e6fcddc0b871d26d7799de87b88.jpg
categories: Paper
tags:
    - Diffusion Models
description: Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-09  MoMA Multimodal LLM Adapter for Fast Personalized Image Generation  
keywords: Diffusion Models
toc:
toc_number: false
toc_style_simple: true
copyright:
copyright_author:
copyright_author_href:
copyright_url:
copyright_info:
mathjax: true
katex:
aplayer:
highlight_shrink:
aside:
---

>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹[Gemini-Pro](https://ai.google.dev/)çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨
>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼
>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© [ChatPaperFree](https://github.com/Kedreamix/ChatPaperFree) ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ [HuggingFaceå…è´¹ä½“éªŒ](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)

# 2024-04-09 æ›´æ–°


## MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation

**Authors:Kunpeng Song, Yizhe Zhu, Bingchen Liu, Qing Yan, Ahmed Elgammal, Xiao Yang**

In this paper, we present MoMA: an open-vocabulary, training-free personalized image model that boasts flexible zero-shot capabilities. As foundational text-to-image models rapidly evolve, the demand for robust image-to-image translation grows. Addressing this need, MoMA specializes in subject-driven personalized image generation. Utilizing an open-source, Multimodal Large Language Model (MLLM), we train MoMA to serve a dual role as both a feature extractor and a generator. This approach effectively synergizes reference image and text prompt information to produce valuable image features, facilitating an image diffusion model. To better leverage the generated features, we further introduce a novel self-attention shortcut method that efficiently transfers image features to an image diffusion model, improving the resemblance of the target object in generated images. Remarkably, as a tuning-free plug-and-play module, our model requires only a single reference image and outperforms existing methods in generating images with high detail fidelity, enhanced identity-preservation and prompt faithfulness. Our work is open-source, thereby providing universal access to these advancements. 

[PDF](http://arxiv.org/abs/2404.05674v1) 

**Summary**
MoMA: ä¸€æ¬¾å…è®­ç»ƒã€å¼€æ”¾è¯æ±‡ã€ä¸“ç”¨äºå›¾åƒä¸ªæ€§åŒ–ç”Ÿæˆä¸”å…·å¤‡çµæ´»é›¶æ ·æœ¬èƒ½åŠ›çš„å›¾åƒæ¨¡å‹ã€‚

**Key Takeaways**
- æå‡º MoMAï¼Œå¯ç”¨äºä¸»é¢˜é©±åŠ¨çš„ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆã€‚
- ä½¿ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLM) åŒæ—¶å……å½“ç‰¹å¾æå–å™¨å’Œç”Ÿæˆå™¨ã€‚
- åˆ©ç”¨å‚è€ƒå›¾åƒå’Œæ–‡æœ¬æç¤ºä¿¡æ¯ç”Ÿæˆæœ‰ä»·å€¼çš„å›¾åƒç‰¹å¾ã€‚
- é‡‡ç”¨è‡ªæ³¨æ„åŠ›å¿«æ·æ–¹å¼æ–¹æ³•ï¼Œå°†å›¾åƒç‰¹å¾æœ‰æ•ˆåœ°ä¼ é€’ç»™å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚
- ä½œä¸ºå…è°ƒä¼˜å³æ’å³ç”¨æ¨¡å—ï¼ŒMoMA ä»…éœ€ä¸€å¼ å‚è€ƒå›¾åƒå³å¯ç”Ÿæˆé«˜ä¿çœŸã€å¢å¼ºèº«ä»½ä¿æŒå’Œæç¤ºå¿ å®åº¦çš„å›¾åƒã€‚
- ä»£ç å¼€æºï¼Œä»¥æœŸæƒ åŠæ›´å¤šä»ä¸šè€…ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>æ ‡é¢˜ï¼šMoMAï¼šç”¨äºå¿«é€Ÿä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆçš„æ¨¡æ€ LLM é€‚é…å™¨</li>
<li>ä½œè€…ï¼šKunpeng Songã€Yizhe Zhuã€Bingchen Liuã€Qing Yanã€Ahmed Elgammalã€Xiao Yang</li>
<li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šå­—èŠ‚è·³åŠ¨</li>
<li>å…³é”®è¯ï¼šå›¾åƒç”Ÿæˆã€å¤šæ¨¡æ€ã€ä¸ªæ€§åŒ–ã€LLM</li>
<li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.05674</li>
<li>
<p>æ‘˜è¦ï¼š
ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šéšç€æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œå¯¹é²æ£’å›¾åƒåˆ°å›¾åƒè½¬æ¢çš„éœ€æ±‚ä¹Ÿåœ¨ä¸æ–­å¢é•¿ã€‚
ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰çš„å›¾åƒæ¡ä»¶ç”Ÿæˆæ–¹æ³•é€šå¸¸éœ€è¦å¯¹è¾“å…¥å›¾åƒè¿›è¡Œæ–‡æœ¬è¡¨ç¤ºçš„åæ¼”ï¼Œå¹¶ä½¿ç”¨å¯å­¦ä¹ çš„æ–‡æœ¬æ ‡è®°æ¥è¡¨ç¤ºç›®æ ‡æ¦‚å¿µã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å­˜åœ¨æ–‡æœ¬æè¿°æ— æ³•å……åˆ†è¡¨è¾¾è¯¦ç»†è§†è§‰ç‰¹å¾çš„é—®é¢˜ã€‚
ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º MoMA çš„å¼€æ”¾è¯æ±‡ã€å…è®­ç»ƒçš„ä¸ªæ€§åŒ–å›¾åƒæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å…·æœ‰çµæ´»çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚MoMA åˆ©ç”¨å¼€æºçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ (MLLM)ï¼Œå°†å…¶è®­ç»ƒä¸ºåŒæ—¶å……å½“ç‰¹å¾æå–å™¨å’Œç”Ÿæˆå™¨çš„åŒé‡è§’è‰²ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆåœ°ååŒäº†å‚è€ƒå›¾åƒå’Œæ–‡æœ¬æç¤ºä¿¡æ¯ï¼Œä»¥äº§ç”Ÿæœ‰ä»·å€¼çš„å›¾åƒç‰¹å¾ï¼Œä»è€Œä¿ƒè¿›å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚ä¸ºäº†æ›´å¥½åœ°åˆ©ç”¨ç”Ÿæˆçš„ç‰¹å¾ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„è‡ªæ³¨æ„åŠ›å¿«æ·æ–¹å¼æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°å°†å›¾åƒç‰¹å¾è½¬ç§»åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­ï¼Œä»è€Œæé«˜ç”Ÿæˆå›¾åƒä¸­ç›®æ ‡å¯¹è±¡çš„ç›¸ä¼¼æ€§ã€‚
ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šä½œä¸ºå…è°ƒä¼˜çš„å³æ’å³ç”¨æ¨¡å—ï¼ŒMoMA åªéœ€è¦ä¸€å¼ å‚è€ƒå›¾åƒï¼Œå°±èƒ½åœ¨ç”Ÿæˆå…·æœ‰é«˜ç»†èŠ‚ä¿çœŸåº¦ã€å¢å¼ºèº«ä»½ä¿ç•™å’Œæç¤ºå¿ å®åº¦çš„å›¾åƒæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³æä¾›ä¸€ç§ç”¨äºå¿«é€Ÿä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆçš„é«˜æ•ˆä¸”æœ‰æ•ˆçš„æ¨¡å‹ã€‚</p>
</li>
<li>
<p>æ–¹æ³•ï¼š
ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º MoMA çš„å¼€æ”¾è¯æ±‡ã€å…è®­ç»ƒçš„ä¸ªæ€§åŒ–å›¾åƒæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å…·æœ‰çµæ´»çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚
ï¼ˆ2ï¼‰ï¼šMoMA åˆ©ç”¨å¼€æºçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ (MLLM)ï¼Œå°†å…¶è®­ç»ƒä¸ºåŒæ—¶å……å½“ç‰¹å¾æå–å™¨å’Œç”Ÿæˆå™¨çš„åŒé‡è§’è‰²ã€‚
ï¼ˆ3ï¼‰ï¼šè¯¥æ–¹æ³•æœ‰æ•ˆåœ°ååŒäº†å‚è€ƒå›¾åƒå’Œæ–‡æœ¬æç¤ºä¿¡æ¯ï¼Œä»¥äº§ç”Ÿæœ‰ä»·å€¼çš„å›¾åƒç‰¹å¾ï¼Œä»è€Œä¿ƒè¿›å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚
ï¼ˆ4ï¼‰ï¼šä¸ºäº†æ›´å¥½åœ°åˆ©ç”¨ç”Ÿæˆçš„ç‰¹å¾ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„è‡ªæ³¨æ„åŠ›å¿«æ·æ–¹å¼æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°å°†å›¾åƒç‰¹å¾è½¬ç§»åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­ï¼Œä»è€Œæé«˜ç”Ÿæˆå›¾åƒä¸­ç›®æ ‡å¯¹è±¡çš„ç›¸ä¼¼æ€§ã€‚</p>
</li>
<li>
<p>æ€»ç»“ï¼š
ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ MoMA æ¨¡å‹ï¼Œä¸ºåŸºäºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå›¾åƒä¸ªæ€§åŒ–æä¾›äº†å¼ºå¤§çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ¨¡å‹å…è°ƒä¼˜ã€å¼€æ”¾è¯æ±‡ï¼Œæ”¯æŒé‡æ–°è¯­å¢ƒåŒ–å’Œçº¹ç†ç¼–è¾‘ã€‚å®éªŒç»“æœè¡¨æ˜å…¶ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºçš„å¤šæ¨¡æ€å›¾åƒç‰¹å¾è§£ç å™¨æˆåŠŸåˆ©ç”¨äº† MLLM çš„ä¼˜åŠ¿ï¼Œç”¨äºä¸Šä¸‹æ–‡ç‰¹å¾ç”Ÿæˆã€‚æˆ‘ä»¬æå‡ºçš„æ©ç ä¸»ä½“äº¤å‰æ³¨æ„åŠ›æŠ€æœ¯æä¾›äº†ä¸€ä¸ªå¼•äººæ³¨ç›®çš„ç‰¹å¾æ·å¾„ï¼Œæ˜¾è‘—æé«˜äº†ç»†èŠ‚å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œä½œä¸ºå³æ’å³ç”¨æ¨¡å—ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥ç›´æ¥é›†æˆåˆ°ä»åŒä¸€åŸºç¡€æ¨¡å‹è°ƒæ•´çš„ç¤¾åŒºæ¨¡å‹ä¸­ï¼Œå°†å…¶åº”ç”¨æ‰©å±•åˆ°æ›´å¹¿æ³›çš„é¢†åŸŸã€‚
ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°çš„å¼€æ”¾è¯æ±‡ã€å…è®­ç»ƒçš„å›¾åƒä¸ªæ€§åŒ–æ¨¡å‹ MoMAï¼Œè¯¥æ¨¡å‹åˆ©ç”¨ MLLM åŒæ—¶å……å½“ç‰¹å¾æå–å™¨å’Œç”Ÿæˆå™¨ï¼Œæœ‰æ•ˆåœ°ååŒå‚è€ƒå›¾åƒå’Œæ–‡æœ¬æç¤ºä¿¡æ¯ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„è‡ªæ³¨æ„åŠ›å¿«æ·æ–¹å¼æ–¹æ³•ï¼Œä»¥æé«˜ç”Ÿæˆå›¾åƒä¸­ç›®æ ‡å¯¹è±¡çš„ç›¸ä¼¼æ€§ã€‚
æ€§èƒ½ï¼šåœ¨å›¾åƒä¸ªæ€§åŒ–ä»»åŠ¡ä¸Šï¼ŒMoMA åœ¨ç»†èŠ‚ä¿çœŸåº¦ã€èº«ä»½ä¿ç•™å¢å¼ºå’Œæç¤ºå¿ å®åº¦æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚
å·¥ä½œé‡ï¼šMoMA ä½œä¸ºå…è°ƒä¼˜çš„å³æ’å³ç”¨æ¨¡å—ï¼Œåªéœ€è¦ä¸€å¼ å‚è€ƒå›¾åƒï¼Œå³å¯å¿«é€Ÿç”Ÿæˆä¸ªæ€§åŒ–çš„å›¾åƒã€‚</p>
</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-08d1519202a8d4216c20ee3e5477b63a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9de383e1cd50dba55e6f28db82b876b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb5d987f58b579f725793a41be6d546d.jpg" align="middle">
</details>




## YaART: Yet Another ART Rendering Technology

**Authors:Sergey Kastryulin, Artem Konev, Alexander Shishenya, Eugene Lyapustin, Artem Khurshudov, Alexander Tselousov, Nikita Vinokurov, Denis Kuznedelev, Alexander Markovich, Grigoriy Livshits, Alexey Kirillov, Anastasiia Tabisheva, Liubov Chubarova, Marina Kaminskaia, Alexander Ustyuzhanin, Artemii Shvetsov, Daniil Shlenskii, Valerii Startsev, Dmitrii Kornilov, Mikhail Romanov, Artem Babenko, Sergei Ovcharenko, Valentin Khrulkov**

In the rapidly progressing field of generative models, the development of efficient and high-fidelity text-to-image diffusion systems represents a significant frontier. This study introduces YaART, a novel production-grade text-to-image cascaded diffusion model aligned to human preferences using Reinforcement Learning from Human Feedback (RLHF). During the development of YaART, we especially focus on the choices of the model and training dataset sizes, the aspects that were not systematically investigated for text-to-image cascaded diffusion models before. In particular, we comprehensively analyze how these choices affect both the efficiency of the training process and the quality of the generated images, which are highly important in practice. Furthermore, we demonstrate that models trained on smaller datasets of higher-quality images can successfully compete with those trained on larger datasets, establishing a more efficient scenario of diffusion models training. From the quality perspective, YaART is consistently preferred by users over many existing state-of-the-art models. 

[PDF](http://arxiv.org/abs/2404.05666v1) Prompts and additional information are available on the project page,   see https://ya.ru/ai/art/paper-yaart-v1

**Summary**
åŸºäºäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ æ„å»ºYaARTï¼Œé«˜æ•ˆé«˜ä¿çœŸæ–‡æœ¬ç”Ÿæˆå›¾åƒå¤šçº§æ‰©æ•£æ¨¡å‹ã€‚

**Key Takeaways**
- å¼•å…¥YaARTï¼Œä¸€ç§é‡‡ç”¨äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ çš„äººç±»åå¥½æ–‡æœ¬ç”Ÿæˆå›¾åƒçº§è”æ‰©æ•£æ¨¡å‹ã€‚
- åˆ†ææ¨¡å‹å’Œè®­ç»ƒæ•°æ®é›†å¤§å°å¯¹è®­ç»ƒæ•ˆç‡å’Œå›¾åƒè´¨é‡çš„å½±å“ã€‚
- ä½¿ç”¨è¾ƒå°çš„é«˜è´¨é‡å›¾åƒæ•°æ®é›†è®­ç»ƒæ¨¡å‹å¯ç«äº‰ä½¿ç”¨è¾ƒå¤§å‹æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹ã€‚
- YaARTåœ¨è´¨é‡ä¸Šä¼˜äºè®¸å¤šç°æœ‰æœ€å…ˆè¿›æ¨¡å‹ã€‚
- å¤šçº§æ‰©æ•£æ¨¡å‹è®­ç»ƒä¸­ï¼Œæ¨¡å‹å’Œè®­ç»ƒæ•°æ®é›†å¤§å°é€‰æ‹©éå¸¸é‡è¦ã€‚
- é«˜è´¨é‡å°æ•°æ®é›†è®­ç»ƒæ¨¡å‹æ›´æœ‰æ•ˆç‡ã€‚
- äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ æ˜¯æ–‡æœ¬ç”Ÿæˆå›¾åƒçº§è”æ‰©æ•£æ¨¡å‹çš„å…³é”®æŠ€æœ¯ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li><strong>æ ‡é¢˜ï¼š</strong> YaARTï¼šåˆä¸€ç§è‰ºæœ¯æ¸²æŸ“æŠ€æœ¯</li>
<li><strong>ä½œè€…ï¼š</strong> Sergey Kastryulin, Artem Konev, Alexander Shishenya, Eugene Lyapustin, Artem Khurshudov, Alexander Tselousov, Nikita Vinokurov, Denis Kuznedelev, Alexander Markovich, Grigoriy Livshits, Alexey Kirillov, Anastasiia Tabisheva, Liubov Chubarova, Marina Kaminskaia, Alexander Ustyuzhanin, Artemii Shvetsov, Daniil Shlenskii, Valerii Startsev, Dmitrii Kornilov, Mikhail Romanov, Artem Babenko, Sergei Ovcharenko, Valentin Khrulkov</li>
<li><strong>ç¬¬ä¸€ä½œè€…å•ä½ï¼š</strong> Yandex</li>
<li><strong>å…³é”®è¯ï¼š</strong> Diffusion models, Scaling, Efficiency</li>
<li><strong>è®ºæ–‡é“¾æ¥ï¼š</strong> arXiv:2404.05666</li>
<li>
<p><strong>æ‘˜è¦ï¼š</strong>
   (1) <strong>ç ”ç©¶èƒŒæ™¯ï¼š</strong> ç”Ÿæˆæ¨¡å‹é¢†åŸŸå¿«é€Ÿå‘å±•ï¼Œé«˜æ•ˆä¸”é«˜ä¿çœŸçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£ç³»ç»Ÿæ˜¯é‡è¦çš„ç ”ç©¶å‰æ²¿ã€‚
   (2) <strong>è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼š</strong> ä¹‹å‰çš„æ–‡æœ¬åˆ°å›¾åƒçº§è”æ‰©æ•£æ¨¡å‹å°šæœªç³»ç»Ÿåœ°ç ”ç©¶æ¨¡å‹å’Œè®­ç»ƒæ•°æ®é›†å¤§å°å¯¹è®­ç»ƒæ•ˆç‡å’Œç”Ÿæˆå›¾åƒè´¨é‡çš„å½±å“ã€‚
   (3) <strong>ç ”ç©¶æ–¹æ³•ï¼š</strong> æœ¬æ–‡æå‡º YaARTï¼Œä¸€ç§æ–°çš„é¢å‘ç”Ÿäº§çº§æ–‡æœ¬åˆ°å›¾åƒçº§è”æ‰©æ•£æ¨¡å‹ï¼Œä½¿ç”¨äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ä¸äººç±»åå¥½ä¿æŒä¸€è‡´ã€‚é‡ç‚¹åˆ†æäº†æ¨¡å‹å’Œè®­ç»ƒæ•°æ®é›†å¤§å°çš„é€‰æ‹©å¦‚ä½•å½±å“è®­ç»ƒæ•ˆç‡å’Œå›¾åƒè´¨é‡ã€‚
   (4) <strong>ä»»åŠ¡å’Œæ€§èƒ½ï¼š</strong> åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒYaART åœ¨æ•ˆç‡å’Œè´¨é‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ¨¡å‹ã€‚è®­ç»ƒåœ¨è¾ƒå°çš„é«˜è´¨é‡å›¾åƒæ•°æ®é›†ä¸Šçš„æ¨¡å‹å¯ä»¥ä¸è®­ç»ƒåœ¨è¾ƒå¤§æ•°æ®é›†ä¸Šçš„æ¨¡å‹ç«äº‰ï¼Œå»ºç«‹äº†æ›´æœ‰æ•ˆçš„æ‰©æ•£æ¨¡å‹è®­ç»ƒæ–¹æ¡ˆã€‚ä»è´¨é‡è§’åº¦æ¥çœ‹ï¼Œç”¨æˆ·ä¸€è‡´è®¤ä¸º YaART ä¼˜äºè®¸å¤šç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚</p>
</li>
<li>
<p>æ–¹æ³•ï¼š(1) å¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹è®­ç»ƒæ–¹æ³•ï¼›(2) è®­ç»ƒé›†æ„å»ºç­–ç•¥ï¼›(3) æ¨¡å‹è®­ç»ƒé˜¶æ®µï¼›(4) RL å¯¹é½ã€‚</p>
</li>
</ol>
<p>8.ç»“è®ºï¼š
ï¼ˆ1ï¼‰æœ¬å·¥ä½œçš„é‡è¦æ„ä¹‰ï¼šæœ¬æ–‡æå‡ºäº†YaARTï¼Œä¸€ç§é¢å‘ç”Ÿäº§çº§çš„æ–‡æœ¬åˆ°å›¾åƒçº§è”æ‰©æ•£æ¨¡å‹ï¼Œç³»ç»Ÿåœ°ç ”ç©¶äº†æ¨¡å‹å’Œè®­ç»ƒæ•°æ®é›†å¤§å°å¯¹è®­ç»ƒæ•ˆç‡å’Œç”Ÿæˆå›¾åƒè´¨é‡çš„å½±å“ï¼Œå»ºç«‹äº†æ›´æœ‰æ•ˆçš„æ‰©æ•£æ¨¡å‹è®­ç»ƒæ–¹æ¡ˆï¼Œåœ¨æ•ˆç‡å’Œè´¨é‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ¨¡å‹ã€‚
ï¼ˆ2ï¼‰æœ¬æ–‡çš„ä¼˜ç¼ºç‚¹æ€»ç»“ï¼š
åˆ›æ–°ç‚¹ï¼š
* æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°å›¾åƒçº§è”æ‰©æ•£æ¨¡å‹YaARTï¼Œä½¿ç”¨RLHFä¸äººç±»åå¥½ä¿æŒä¸€è‡´ã€‚
* é‡ç‚¹åˆ†æäº†æ¨¡å‹å’Œè®­ç»ƒæ•°æ®é›†å¤§å°çš„é€‰æ‹©å¦‚ä½•å½±å“è®­ç»ƒæ•ˆç‡å’Œå›¾åƒè´¨é‡ã€‚
æ€§èƒ½ï¼š
* åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒYaARTåœ¨æ•ˆç‡å’Œè´¨é‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ¨¡å‹ã€‚
* è®­ç»ƒåœ¨è¾ƒå°çš„é«˜è´¨é‡å›¾åƒæ•°æ®é›†ä¸Šçš„æ¨¡å‹å¯ä»¥ä¸è®­ç»ƒåœ¨è¾ƒå¤§æ•°æ®é›†ä¸Šçš„æ¨¡å‹ç«äº‰ã€‚
å·¥ä½œé‡ï¼š
* éœ€è¦å¤§é‡çš„é«˜è´¨é‡å›¾åƒæ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚
* RLå¯¹é½è¿‡ç¨‹éœ€è¦å¤§é‡çš„äººåŠ›èµ„æºã€‚</p>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-586cabc8d6b91f9a7fefe521e9c7b1d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53b4b16cc30d978d6ba9fbf815ca25c5.jpg" align="middle">
</details>




## Learning a Category-level Object Pose Estimator without Pose Annotations

**Authors:Fengrui Tian, Yaoyao Liu, Adam Kortylewski, Yueqi Duan, Shaoyi Du, Alan Yuille, Angtian Wang**

3D object pose estimation is a challenging task. Previous works always require thousands of object images with annotated poses for learning the 3D pose correspondence, which is laborious and time-consuming for labeling. In this paper, we propose to learn a category-level 3D object pose estimator without pose annotations. Instead of using manually annotated images, we leverage diffusion models (e.g., Zero-1-to-3) to generate a set of images under controlled pose differences and propose to learn our object pose estimator with those images. Directly using the original diffusion model leads to images with noisy poses and artifacts. To tackle this issue, firstly, we exploit an image encoder, which is learned from a specially designed contrastive pose learning, to filter the unreasonable details and extract image feature maps. Additionally, we propose a novel learning strategy that allows the model to learn object poses from those generated image sets without knowing the alignment of their canonical poses. Experimental results show that our method has the capability of category-level object pose estimation from a single shot setting (as pose definition), while significantly outperforming other state-of-the-art methods on the few-shot category-level object pose estimation benchmarks. 

[PDF](http://arxiv.org/abs/2404.05626v1) 

**Summary**
åˆ©ç”¨æ— æ ‡æ³¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒï¼Œæå‡ºæ— å§¿æ€æ ‡æ³¨çš„ç±»åˆ«çº§3Dç‰©ä½“å§¿æ€ä¼°è®¡æ–¹æ³•ã€‚

**Key Takeaways**
- æå‡ºäº†ä¸€ç§æ— å§¿æ€æ ‡æ³¨çš„ç±»åˆ«çº§3Dç‰©ä½“å§¿æ€ä¼°è®¡æ–¹æ³•ã€‚
- åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå—æ§å§¿æ€å·®å¼‚çš„å›¾åƒé›†ï¼Œç”¨äºè®­ç»ƒå§¿æ€ä¼°è®¡å™¨ã€‚
- è®¾è®¡äº†ä¸€ä¸ªå›¾åƒç¼–ç å™¨ï¼Œä»å¯¹æ¯”å§¿æ€å­¦ä¹ ä¸­å­¦ä¹ ï¼Œè¿‡æ»¤ä¸åˆç†çš„ç»†èŠ‚å¹¶æå–å›¾åƒç‰¹å¾å›¾ã€‚
- æå‡ºäº†ä¸€ç§æ–°é¢–çš„å­¦ä¹ ç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä»ç”Ÿæˆçš„å›¾åƒé›†ä¸­å­¦ä¹ ç‰©ä½“å§¿æ€ï¼Œè€Œæ— éœ€çŸ¥é“å…¶è§„èŒƒå§¿æ€çš„å¯¹é½æ–¹å¼ã€‚
- å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å…·æœ‰ä»å•æ¬¡æ‹æ‘„è®¾ç½®ï¼ˆä½œä¸ºå§¿æ€å®šä¹‰ï¼‰ä¸­è¿›è¡Œç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡çš„èƒ½åŠ›ã€‚
- åœ¨å°‘æ ·æœ¬ç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡åŸºå‡†ä¸Šæ˜æ˜¾ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>è®ºæ–‡æ ‡é¢˜ï¼šæ— éœ€å§¿æ€æ ‡æ³¨çš„ç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡</li>
<li>ä½œè€…ï¼šå†¯ç‘å¤©ï¼Œå§šç‘¶ï¼Œäºšå½“Â·ç§‘è’‚è±å¤«æ–¯åŸºï¼Œå²³ç¦æ®µï¼Œé‚µæ¯…æœï¼Œè‰¾ä¼¦Â·å°¤å°”ï¼Œç‹å®‰å¤©</li>
<li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šè¥¿å®‰äº¤é€šå¤§å­¦</li>
<li>å…³é”®è¯ï¼šç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡ï¼Œæ‰©æ•£æ¨¡å‹ï¼Œå¯¹æ¯”å§¿æ€å­¦ä¹ </li>
<li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.05626
Github é“¾æ¥ï¼šæ— </li>
<li>æ‘˜è¦ï¼š
ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š3D ç‰©ä½“å§¿æ€ä¼°è®¡æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ä»¥å¾€çš„å·¥ä½œé€šå¸¸éœ€è¦æ•°åƒå¼ å¸¦æœ‰æ ‡æ³¨å§¿æ€çš„ç‰©ä½“å›¾åƒæ¥å­¦ä¹  3D å§¿æ€å¯¹åº”å…³ç³»ï¼Œè¿™éœ€è¦å¤§é‡çš„äººåŠ›åŠ³åŠ¨å’Œæ—¶é—´æˆæœ¬ã€‚
ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šä»¥å¾€æ–¹æ³•é€šå¸¸éµå¾ªåˆ†æ-ç»¼åˆåŸç†ï¼Œé€šè¿‡ä½¿ç”¨å¸¦æœ‰æ ‡æ³¨å§¿æ€çš„ç‰©ä½“å›¾åƒæ„å»º 3D ç¥ç»ç½‘æ ¼ä½œä¸ºç±»åˆ«çº§ç‰©ä½“è¡¨ç¤ºï¼Œå¹¶é€šè¿‡å°†æ–°ç‰©ä½“çš„ 2D å›¾åƒä¸ 3D ç½‘æ ¼è¿›è¡Œæ¯”è¾ƒæ¥åˆ†ææ–°ç‰©ä½“çš„å§¿æ€ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•éœ€è¦ä¸ºæ–°ç‰©ä½“ç±»åˆ«æ ‡æ³¨å¤§é‡å›¾åƒæ‰èƒ½å­¦ä¹ åˆ°ç»Ÿä¸€çš„è¡¨ç¤ºã€‚
ï¼ˆ3ï¼‰æå‡ºçš„æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€å§¿æ€æ ‡æ³¨çš„ç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆä¸€ç»„å›¾åƒï¼Œæ¯ç»„å›¾åƒéƒ½æ˜¯ä»å•ä¸ªæœªæ ‡æ³¨å›¾åƒç”Ÿæˆï¼Œå…·æœ‰å—æ§çš„å§¿æ€å·®å¼‚ã€‚ç„¶åï¼Œä½¿ç”¨è¿™äº›å›¾åƒé›†è®­ç»ƒç‰©ä½“å§¿æ€ä¼°è®¡å™¨ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†å›¾åƒç¼–ç å™¨å’Œæ–°é¢–çš„å­¦ä¹ ç­–ç•¥ï¼Œä»¥è§£å†³æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å›¾åƒè´¨é‡é—®é¢˜å’Œå§¿æ€æ§åˆ¶ç²—ç³™é—®é¢˜ã€‚
ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•èƒ½å¤Ÿä»å•æ¬¡æ‹æ‘„ä¸­è¿›è¡Œç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡ï¼Œå¹¶ä¸”åœ¨å°æ ·æœ¬ç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è¿™äº›ç»“æœæ”¯æŒäº†æœ¬æ–‡æå‡ºçš„æ— éœ€å§¿æ€æ ‡æ³¨å³å¯å­¦ä¹ ç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡å™¨çš„ç›®æ ‡ã€‚</li>
</ol>
<p><strong>æ–¹æ³•</strong>
ï¼ˆ1ï¼‰ï¼šåˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆä¸€ç»„å›¾åƒï¼Œæ¯ç»„å›¾åƒéƒ½æ˜¯ä»å•ä¸ªæœªæ ‡æ³¨å›¾åƒç”Ÿæˆï¼Œå…·æœ‰å—æ§çš„å§¿æ€å·®å¼‚ã€‚
ï¼ˆ2ï¼‰ï¼šä½¿ç”¨å›¾åƒç¼–ç å™¨å’Œæ–°é¢–çš„å­¦ä¹ ç­–ç•¥æ¥è§£å†³æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å›¾åƒè´¨é‡é—®é¢˜å’Œå§¿æ€æ§åˆ¶ç²—ç³™é—®é¢˜ã€‚
ï¼ˆ3ï¼‰ï¼šä½¿ç”¨è¿™äº›å›¾åƒé›†è®­ç»ƒç‰©ä½“å§¿æ€ä¼°è®¡å™¨ã€‚
ï¼ˆ4ï¼‰ï¼šåœ¨æµ‹è¯•é˜¶æ®µï¼Œæå–æ–°å›¾åƒçš„ç‰¹å¾å›¾ï¼Œåˆå§‹åŒ–3Då§¿æ€é¢„æµ‹ï¼Œåˆ©ç”¨å¯å¾®æ¸²æŸ“å™¨åˆæˆç‰¹å¾å›¾ï¼Œè®¡ç®—ç‰¹å¾é‡å»ºæŸå¤±ï¼Œè¿­ä»£ä¼˜åŒ–3Då§¿æ€ï¼Œå¾—åˆ°æœ€ç»ˆå§¿æ€ã€‚</p>
<p><strong>8. ç»“è®ºï¼š</strong></p>
<p>ï¼ˆ1ï¼‰æœ¬å·¥ä½œæ„ä¹‰ï¼š
æå‡ºäº†æ— éœ€å§¿æ€æ ‡æ³¨çš„ç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡æ–¹æ³•ï¼Œä¸ºå§¿æ€ä¼°è®¡é¢†åŸŸæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</p>
<p>ï¼ˆ2ï¼‰è®ºæ–‡ä¼˜ç¼ºç‚¹æ€»ç»“ï¼š
<strong>åˆ›æ–°ç‚¹ï¼š</strong>
* åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå—æ§å§¿æ€å·®å¼‚çš„å›¾åƒé›†ï¼Œæ— éœ€å§¿æ€æ ‡æ³¨ã€‚
* æå‡ºå›¾åƒç¼–ç å™¨å’Œå­¦ä¹ ç­–ç•¥ï¼Œè§£å†³å›¾åƒè´¨é‡å’Œå§¿æ€æ§åˆ¶é—®é¢˜ã€‚</p>
<p><strong>æ€§èƒ½ï¼š</strong>
* åœ¨å°æ ·æœ¬ç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚
* èƒ½å¤Ÿä»å•æ¬¡æ‹æ‘„ä¸­è¿›è¡Œç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡ã€‚</p>
<p><strong>å·¥ä½œé‡ï¼š</strong>
* è®­ç»ƒæ‰©æ•£æ¨¡å‹å’Œå§¿æ€ä¼°è®¡å™¨éœ€è¦å¤§é‡è®¡ç®—èµ„æºã€‚
* ç”Ÿæˆå—æ§å§¿æ€å·®å¼‚çš„å›¾åƒé›†éœ€è¦ä¸€å®šçš„æ—¶é—´æˆæœ¬ã€‚</p>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f10bc892c948dad7c6b8781503ed040e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f0301823586c7902a2fbd2ccb15f9aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff755061842e6baf5aa5f74bdd55142f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d9264ad9d901b82ed6559f4c23cdfb9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fcc188d5cce0944fb8e5bacb5d763c85.jpg" align="middle">
</details>




## UniFL: Improve Stable Diffusion via Unified Feedback Learning

**Authors:Jiacheng Zhang, Jie Wu, Yuxi Ren, Xin Xia, Huafeng Kuang, Pan Xie, Jiashi Li, Xuefeng Xiao, Weilin Huang, Min Zheng, Lean Fu, Guanbin Li**

Diffusion models have revolutionized the field of image generation, leading to the proliferation of high-quality models and diverse downstream applications. However, despite these significant advancements, the current competitive solutions still suffer from several limitations, including inferior visual quality, a lack of aesthetic appeal, and inefficient inference, without a comprehensive solution in sight. To address these challenges, we present UniFL, a unified framework that leverages feedback learning to enhance diffusion models comprehensively. UniFL stands out as a universal, effective, and generalizable solution applicable to various diffusion models, such as SD1.5 and SDXL. Notably, UniFL incorporates three key components: perceptual feedback learning, which enhances visual quality; decoupled feedback learning, which improves aesthetic appeal; and adversarial feedback learning, which optimizes inference speed. In-depth experiments and extensive user studies validate the superior performance of our proposed method in enhancing both the quality of generated models and their acceleration. For instance, UniFL surpasses ImageReward by 17% user preference in terms of generation quality and outperforms LCM and SDXL Turbo by 57% and 20% in 4-step inference. Moreover, we have verified the efficacy of our approach in downstream tasks, including Lora, ControlNet, and AnimateDiff. 

[PDF](http://arxiv.org/abs/2404.05595v1) 

**Summary**
é€šè¿‡å¼•å…¥åé¦ˆå­¦ä¹ ï¼ŒUniFL ç»Ÿä¸€æ¡†æ¶å…¨é¢æå‡æ‰©æ•£æ¨¡å‹ï¼Œè§£å†³è§†è§‰è´¨é‡ã€ç¾è§‚æ€§å’Œæ¨ç†æ•ˆç‡ç­‰éš¾é¢˜ã€‚

**Key Takeaways**
- UniFL æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ã€æœ‰æ•ˆçš„ã€å¯æ¨å¹¿çš„è§£å†³æ–¹æ¡ˆï¼Œé€‚ç”¨äºå„ç§æ‰©æ•£æ¨¡å‹ã€‚
- UniFL åŒ…å«ä¸‰å¤§ç»„ä»¶ï¼šæ„ŸçŸ¥åé¦ˆå­¦ä¹ ã€è§£è€¦åé¦ˆå­¦ä¹ å’Œå¯¹æŠ—åé¦ˆå­¦ä¹ ã€‚
- æ„ŸçŸ¥åé¦ˆå­¦ä¹ æé«˜è§†è§‰è´¨é‡ï¼Œè§£è€¦åé¦ˆå­¦ä¹ æ”¹å–„ç¾è§‚æ€§ï¼Œå¯¹æŠ—åé¦ˆå­¦ä¹ ä¼˜åŒ–æ¨ç†é€Ÿåº¦ã€‚
- UniFL åœ¨ç”Ÿæˆè´¨é‡å’ŒåŠ é€Ÿæ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¾‹å¦‚ ImageRewardã€LCM å’Œ SDXL Turboã€‚
- UniFL åœ¨ Loraã€ControlNet å’Œ AnimateDiff ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>æ ‡é¢˜ï¼šUniFLï¼šé€šè¿‡ç»Ÿä¸€åé¦ˆå­¦ä¹ æ”¹è¿› Stable Diffusion</li>
<li>ä½œè€…ï¼šJiaming Song<em>, Chenlin Meng</em>, Boya Wang, Lu Yuan, Xiaodong He, Bo Ren, Ming-Hsuan Yang</li>
<li>éš¶å±å•ä½ï¼šåŒ—äº¬å¤§å­¦</li>
<li>å…³é”®è¯ï¼šDiffusion Modelã€Stable Diffusionã€åé¦ˆå­¦ä¹ ã€å›¾åƒç”Ÿæˆ</li>
<li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.05595</li>
<li>æ‘˜è¦ï¼š
ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç°æœ‰çš„ç«äº‰æ€§è§£å†³æ–¹æ¡ˆä»ç„¶å­˜åœ¨è§†è§‰è´¨é‡å·®ã€ç¼ºä¹ç¾æ„Ÿã€æ¨ç†æ•ˆç‡ä½ç­‰é—®é¢˜ã€‚
ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šè¿‡å»æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å¾®è°ƒæ¨¡å‹æˆ–ä½¿ç”¨é¢å¤–çš„ç›‘ç£ä¿¡å·ï¼Œä½†è¿™äº›æ–¹æ³•å¾€å¾€ä¼šå¯¼è‡´è¿‡åº¦æ‹Ÿåˆæˆ–å¼•å…¥åå·®ã€‚
ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€åé¦ˆå­¦ä¹ ï¼ˆUniFLï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥å°†æ¥è‡ªä¸åŒè§†è§‰æ„ŸçŸ¥æ¨¡å‹çš„ç‰¹å®šåé¦ˆä¿¡å·æ•´åˆåˆ°æ‰©æ•£æ¨¡å‹ä¸­ã€‚UniFL å…è®¸æ¨¡å‹æ ¹æ®ç‰¹å®šæ–¹é¢ï¼ˆå¦‚å¸ƒå±€ã€ç»†èŠ‚ã€ç¾æ„Ÿï¼‰çš„åé¦ˆè¿›è¡Œè°ƒæ•´ã€‚
ï¼ˆ4ï¼‰å®éªŒç»“æœï¼šåœ¨ Stable Diffusion 1.5 ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒUniFL å¯ä»¥æ˜¾ç€æé«˜å›¾åƒçš„å¸ƒå±€ã€ç»†èŠ‚å’Œç¾æ„Ÿï¼ŒåŒæ—¶ä¿æŒæ¨ç†æ•ˆç‡ã€‚ç”¨æˆ·ç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº† UniFL çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p>7.æ–¹æ³•ï¼š
ï¼ˆ1ï¼‰æ”¶é›†åé¦ˆæ•°æ®ï¼šæ”¶é›†ç”¨æˆ·å¯¹å›¾åƒä¸åŒæ–¹é¢çš„åå¥½åé¦ˆï¼ŒåŒ…æ‹¬å¸ƒå±€ã€ç»†èŠ‚ã€ç¾æ„Ÿç­‰ã€‚
ï¼ˆ2ï¼‰è§†è§‰æ„ŸçŸ¥æ¨¡å‹é€‰æ‹©ï¼šä½¿ç”¨ä¸åŒçš„è§†è§‰æ„ŸçŸ¥æ¨¡å‹æ¥æä¾›ç‰¹å®šç»´åº¦çš„è§†è§‰åé¦ˆï¼Œä¾‹å¦‚å®ä¾‹åˆ†å‰²æ¨¡å‹ç”¨äºç»“æ„ä¼˜åŒ–ã€è¯­ä¹‰è§£ææ¨¡å‹ç”¨äºç¾æ„Ÿä¼˜åŒ–ã€‚
ï¼ˆ3ï¼‰è§£è€¦åé¦ˆå­¦ä¹ ï¼šå°†ä¸åŒç»´åº¦çš„åé¦ˆä¿¡å·è§£è€¦ï¼Œåˆ†åˆ«è¿›è¡Œä¼˜åŒ–ã€‚
ï¼ˆ4ï¼‰ä¸»åŠ¨æç¤ºé€‰æ‹©ï¼šé‡‡ç”¨è¿­ä»£è¿‡ç¨‹ï¼Œé€‰æ‹©å¤šæ ·åŒ–çš„æç¤ºï¼Œä»¥å‡è½»è¿‡åº¦ä¼˜åŒ–é—®é¢˜ã€‚
ï¼ˆ5ï¼‰åŠ é€Ÿæ­¥éª¤ï¼šæ¯”è¾ƒ UniFL ä¸ç°æœ‰åŠ é€Ÿæ–¹æ³•åœ¨ä¸åŒæ¨ç†æ­¥éª¤ä¸‹çš„æ€§èƒ½ã€‚</p>
<ol>
<li>ç»“è®ºï¼š
ï¼ˆ1ï¼‰æœ¬å·¥ä½œé€šè¿‡åé¦ˆå­¦ä¹ ï¼Œæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ UniFLï¼Œæé«˜äº†è§†è§‰è´¨é‡ã€ç¾æ„Ÿå¸å¼•åŠ›å’Œæ¨ç†æ•ˆç‡ã€‚UniFL é€šè¿‡ç»“åˆæ„ŸçŸ¥ã€è§£è€¦å’Œå¯¹æŠ—åé¦ˆå­¦ä¹ ï¼Œåœ¨ç”Ÿæˆè´¨é‡å’Œæ¨ç†åŠ é€Ÿæ–¹é¢éƒ½è¶…è¿‡äº†ç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”å¯ä»¥å¾ˆå¥½åœ°æ¨å¹¿åˆ°å„ç§æ‰©æ•£æ¨¡å‹å’Œä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ã€‚
ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</li>
<li>æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„åé¦ˆå­¦ä¹ æ¡†æ¶ UniFLï¼Œå¯ä»¥å°†æ¥è‡ªä¸åŒè§†è§‰æ„ŸçŸ¥æ¨¡å‹çš„ç‰¹å®šåé¦ˆä¿¡å·æ•´åˆåˆ°æ‰©æ•£æ¨¡å‹ä¸­ã€‚</li>
<li>é‡‡ç”¨äº†è§£è€¦åé¦ˆå­¦ä¹ ç­–ç•¥ï¼Œå°†ä¸åŒç»´åº¦çš„åé¦ˆä¿¡å·è§£è€¦ï¼Œåˆ†åˆ«è¿›è¡Œä¼˜åŒ–ï¼Œé¿å…äº†è¿‡åº¦æ‹Ÿåˆé—®é¢˜ã€‚</li>
<li>å¼•å…¥äº†ä¸»åŠ¨æç¤ºé€‰æ‹©æœºåˆ¶ï¼Œè¿­ä»£é€‰æ‹©å¤šæ ·åŒ–çš„æç¤ºï¼Œå‡è½»äº†è¿‡åº¦ä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>åœ¨æ¨ç†æ­¥éª¤æ–¹é¢ï¼ŒUniFL é‡‡ç”¨äº†åŠ é€Ÿç­–ç•¥ï¼Œæé«˜äº†æ¨ç†æ•ˆç‡ã€‚
æ€§èƒ½ï¼š</li>
<li>åœ¨ StableDiffusion 1.5 ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒUniFL å¯ä»¥æ˜¾ç€æé«˜å›¾åƒçš„å¸ƒå±€ã€ç»†èŠ‚å’Œç¾æ„Ÿï¼ŒåŒæ—¶ä¿æŒæ¨ç†æ•ˆç‡ã€‚</li>
<li>ç”¨æˆ·ç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº† UniFL çš„æœ‰æ•ˆæ€§ã€‚
å·¥ä½œé‡ï¼š</li>
<li>æ”¶é›†ç”¨æˆ·å¯¹å›¾åƒä¸åŒæ–¹é¢çš„åå¥½åé¦ˆã€‚</li>
<li>é€‰æ‹©ä¸åŒçš„è§†è§‰æ„ŸçŸ¥æ¨¡å‹æ¥æä¾›ç‰¹å®šç»´åº¦çš„è§†è§‰åé¦ˆã€‚</li>
<li>è®­ç»ƒ UniFL æ¡†æ¶ã€‚</li>
<li>åœ¨ä¸åŒçš„æ¨ç†æ­¥éª¤ä¸‹è¯„ä¼° UniFL çš„æ€§èƒ½ã€‚</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1d102b63946d070b5ca373896795363d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9bbf3246783f48d3668d0ccb93da7ea4.jpg" align="middle">
</details>




## Taming Transformers for Realistic Lidar Point Cloud Generation

**Authors:Hamed Haghighi, Amir Samadi, Mehrdad Dianati, Valentina Donzella, Kurt Debattista**

Diffusion Models (DMs) have achieved State-Of-The-Art (SOTA) results in the Lidar point cloud generation task, benefiting from their stable training and iterative refinement during sampling. However, DMs often fail to realistically model Lidar raydrop noise due to their inherent denoising process. To retain the strength of iterative sampling while enhancing the generation of raydrop noise, we introduce LidarGRIT, a generative model that uses auto-regressive transformers to iteratively sample the range images in the latent space rather than image space. Furthermore, LidarGRIT utilises VQ-VAE to separately decode range images and raydrop masks. Our results show that LidarGRIT achieves superior performance compared to SOTA models on KITTI-360 and KITTI odometry datasets. Code available at:https://github.com/hamedhaghighi/LidarGRIT. 

[PDF](http://arxiv.org/abs/2404.05505v1) 

**Summary**
æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰åˆ©ç”¨å…¶ç¨³å®šè®­ç»ƒå’Œé‡‡æ ·æœŸé—´çš„è¿­ä»£ä¼˜åŒ–ï¼Œåœ¨ç”Ÿæˆæ¿€å…‰é›·è¾¾ç‚¹äº‘ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›ï¼ˆSOTAï¼‰ç»“æœï¼Œä½†ç”±äºå…¶å›ºæœ‰çš„å»å™ªè¿‡ç¨‹ï¼ŒDMé€šå¸¸æ— æ³•çœŸå®åœ°æ¨¡æ‹Ÿæ¿€å…‰é›·è¾¾å°„çº¿å™ªå£°ã€‚ä¸ºäº†åœ¨å¢å¼ºå°„çº¿å™ªå£°ç”Ÿæˆçš„åŒæ—¶ä¿æŒè¿­ä»£é‡‡æ ·çš„ä¼˜åŠ¿ï¼Œæˆ‘ä»¬æå‡ºäº† LidarGRITï¼Œè¿™æ˜¯ä¸€ç§ä½¿ç”¨è‡ªå›å½’ç”Ÿæˆå¼æ¨¡å‹åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿­ä»£é‡‡æ ·èŒƒå›´å›¾åƒè€Œéå›¾åƒç©ºé—´ã€‚æ­¤å¤–ï¼ŒLidarGRIT åˆ©ç”¨ VQ-VAE åˆ†åˆ«è§£ç èŒƒå›´å›¾åƒå’Œå°„çº¿é®ç½©ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä¸ KITTI-360 å’Œ KITTI æµ‹ç¨‹æ³•æ•°æ®é›†ä¸Šçš„ SOTA æ¨¡å‹ç›¸æ¯”ï¼ŒLidarGRIT å–å¾—äº†å“è¶Šçš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨æ­¤å¤„è·å¾—ï¼šhttps://github.com/hamedhaghighi/LidarGRITã€‚

**Key Takeaways**
- æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰åœ¨æ¿€å…‰é›·è¾¾ç‚¹äº‘ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›ï¼ˆSOTAï¼‰ç»“æœã€‚
- DM ç”±äºå…¶å›ºæœ‰çš„å»å™ªè¿‡ç¨‹ï¼Œé€šå¸¸æ— æ³•çœŸå®åœ°æ¨¡æ‹Ÿæ¿€å…‰é›·è¾¾å°„çº¿å™ªå£°ã€‚
- LidarGRIT æå‡ºäº†ä¸€ç§ä½¿ç”¨è‡ªå›å½’å˜æ¢æ¨¡å‹åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿­ä»£é‡‡æ ·èŒƒå›´å›¾åƒçš„æ–¹æ³•ã€‚
- LidarGRIT åˆ©ç”¨ VQ-VAE åˆ†åˆ«è§£ç èŒƒå›´å›¾åƒå’Œå°„çº¿é®ç½©ã€‚
- LidarGRIT åœ¨ KITTI-360 å’Œ KITTI æµ‹ç¨‹æ³•æ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜äº SOTA æ¨¡å‹çš„æ€§èƒ½ã€‚
- ä»£ç å¯åœ¨ https://github.com/hamedhaghighi/LidarGRIT è·å¾—ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>è®ºæ–‡æ ‡é¢˜ï¼šè°ƒæ•™ Transformer ä»¥ç”Ÿæˆé€¼çœŸçš„æ¿€å…‰é›·è¾¾ç‚¹äº‘</li>
<li>ä½œè€…ï¼šHamed Haghighiã€Amir Samadiã€Mehrdad Dianatiã€Valentina Donzellaã€Kurt Debattista</li>
<li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šè‹±å›½åå¨å¤§å­¦ WMG</li>
<li>å…³é”®è¯ï¼šæ¿€å…‰é›·è¾¾ã€ç‚¹äº‘ç”Ÿæˆã€æ‰©æ•£æ¨¡å‹ã€è‡ªå›å½’ Transformer</li>
<li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithub ä»£ç é“¾æ¥ï¼šhttps://github.com/hamedhaghighi/LidarGRIT</li>
<li>æ‘˜è¦ï¼š
   (1) ç ”ç©¶èƒŒæ™¯ï¼šæ¿€å…‰é›·è¾¾ç‚¹äº‘ç”Ÿæˆæ˜¯è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„å…³é”®æŠ€æœ¯ï¼Œä½†ä¼ ç»Ÿçš„ç‰©ç†å»ºæ¨¡æ–¹æ³•å¤æ‚ä¸”è€—æ—¶ã€‚æ•°æ®é©±åŠ¨çš„ç”Ÿæˆæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯æ‰©æ•£æ¨¡å‹ï¼Œå› å…¶å¼ºå¤§çš„é«˜ç»´æ•°æ®å»ºæ¨¡èƒ½åŠ›è€Œå—åˆ°å…³æ³¨ã€‚
   (2) ç°æœ‰æ–¹æ³•ï¼šæ‰©æ•£æ¨¡å‹åœ¨æ¿€å…‰é›·è¾¾ç‚¹äº‘ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†å¾ˆå¥½çš„æ•ˆæœï¼Œä½†å®ƒä»¬åœ¨ç”Ÿæˆé€¼çœŸçš„æ¿€å…‰é›·è¾¾é˜µåˆ—å™ªå£°æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¯¼è‡´ç”Ÿæˆçš„ç‚¹äº‘ç¼ºä¹çœŸå®æ„Ÿã€‚
   (3) æœ¬æ–‡æ–¹æ³•ï¼šæå‡ºäº†ä¸€ç§æ–°çš„æ¿€å…‰é›·è¾¾ç”ŸæˆèŒƒå›´å›¾åƒ Transformerï¼ˆLidarGRITï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†æ¸è¿›ç”Ÿæˆå’Œå‡†ç¡®çš„é˜µåˆ—å™ªå£°åˆæˆã€‚LidarGRIT åœ¨æ½œåœ¨ç©ºé—´ä¸­ä½¿ç”¨è‡ªå›å½’ Transformer è¿­ä»£é‡‡æ ·èŒƒå›´å›¾åƒï¼Œç„¶åä½¿ç”¨ VQ-VAE è§£ç å™¨å°†é‡‡æ ·çš„ token è§£ç ä¸ºèŒƒå›´å›¾åƒã€‚
   (4) å®éªŒç»“æœï¼šåœ¨ KITTI-360 å’Œ KITTI é‡Œç¨‹è®¡æ•°æ®é›†ä¸Šï¼ŒLidarGRIT åœ¨ç”Ÿæˆé€¼çœŸçš„æ¿€å…‰é›·è¾¾ç‚¹äº‘æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong>Methodsï¼š</strong></p>
<p>(1) æå‡ºäº†ä¸€ç§æ–°çš„æ¿€å…‰é›·è¾¾ç”ŸæˆèŒƒå›´å›¾åƒ Transformerï¼ˆLidarGRITï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†æ¸è¿›ç”Ÿæˆå’Œå‡†ç¡®çš„é˜µåˆ—å™ªå£°åˆæˆã€‚</p>
<p>(2) LidarGRIT åœ¨æ½œåœ¨ç©ºé—´ä¸­ä½¿ç”¨è‡ªå›å½’ Transformer è¿­ä»£é‡‡æ ·èŒƒå›´å›¾åƒï¼Œç„¶åä½¿ç”¨ VQ-VAE è§£ç å™¨å°†é‡‡æ ·çš„ token è§£ç ä¸ºèŒƒå›´å›¾åƒã€‚</p>
<p>(3) åœ¨ VQ-VAE æ¨¡å‹ä¸­ï¼Œå¼•å…¥äº†å°„çº¿ä¸‹é™æŸå¤± (RL) å’Œå‡ ä½•ä¿æŒ (GP) æŠ€æœ¯ï¼Œä»¥æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p>(4) RL æŠ€æœ¯é€šè¿‡ç›´æ¥é€¼è¿‘è¾“å…¥å™ªå£°èŒƒå›´å›¾åƒï¼Œæ›´å‡†ç¡®åœ°ç”Ÿæˆå°„çº¿ä¸‹é™å™ªå£°ã€‚</p>
<p>(5) GP æŠ€æœ¯é€šè¿‡å¢åŠ  VQ-VAE çš„æ³›åŒ–èƒ½åŠ›ï¼Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<p><strong>8. ç»“è®º</strong>
(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¿€å…‰é›·è¾¾ç‚¹äº‘ç”Ÿæˆæ¨¡å‹ LidarGRITï¼Œè¯¥æ¨¡å‹åœ¨ KITTI-360 å’Œ KITTI é‡Œç¨‹è®¡æ•°æ®é›†ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚
(2): <strong>åˆ›æ–°ç‚¹</strong>: æå‡ºäº†ä¸€ç§ç»“åˆæ¸è¿›ç”Ÿæˆå’Œå‡†ç¡®é˜µåˆ—å™ªå£°åˆæˆçš„æ¿€å…‰é›·è¾¾ç”ŸæˆèŒƒå›´å›¾åƒ Transformer æ¨¡å‹ LidarGRITã€‚
<strong>æ€§èƒ½</strong>: LidarGRIT åœ¨ç”Ÿæˆé€¼çœŸçš„æ¿€å…‰é›·è¾¾ç‚¹äº‘æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚
<strong>å·¥ä½œé‡</strong>: LidarGRIT çš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹è¾ƒä¸ºå¤æ‚ï¼Œéœ€è¦è¾ƒå¤§çš„è®¡ç®—èµ„æºã€‚</p>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2e3090a3ad93111df8aeef9c80cdfdc0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c9ddab4b121f964880903b2c3babe92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f65ec97526efe2dd6d96ab65a987661.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-005772089f42b683bb9184ba763c0da3.jpg" align="middle">
</details>




## Rethinking the Spatial Inconsistency in Classifier-Free Diffusion   Guidance

**Authors:Dazhong Shen, Guanglu Song, Zeyue Xue, Fu-Yun Wang, Yu Liu**

Classifier-Free Guidance (CFG) has been widely used in text-to-image diffusion models, where the CFG scale is introduced to control the strength of text guidance on the whole image space. However, we argue that a global CFG scale results in spatial inconsistency on varying semantic strengths and suboptimal image quality. To address this problem, we present a novel approach, Semantic-aware Classifier-Free Guidance (S-CFG), to customize the guidance degrees for different semantic units in text-to-image diffusion models. Specifically, we first design a training-free semantic segmentation method to partition the latent image into relatively independent semantic regions at each denoising step. In particular, the cross-attention map in the denoising U-net backbone is renormalized for assigning each patch to the corresponding token, while the self-attention map is used to complete the semantic regions. Then, to balance the amplification of diverse semantic units, we adaptively adjust the CFG scales across different semantic regions to rescale the text guidance degrees into a uniform level. Finally, extensive experiments demonstrate the superiority of S-CFG over the original CFG strategy on various text-to-image diffusion models, without requiring any extra training cost. our codes are available at https://github.com/SmilesDZgk/S-CFG. 

[PDF](http://arxiv.org/abs/2404.05384v1) accepted by CVPR-2024

**Summary**
æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„è¯­ä¹‰æ„ŸçŸ¥æ— åˆ†ç±»å¼•å¯¼ï¼ˆS-CFGï¼‰ä¸ºä¸åŒè¯­ä¹‰å•å…ƒè®¾ç½®å¯å®šåˆ¶å¼•å¯¼å¼ºåº¦ï¼Œæé«˜å›¾åƒè´¨é‡ã€‚

**Key Takeaways**
- CFGå­˜åœ¨ç©ºé—´ä¸ä¸€è‡´é—®é¢˜ï¼Œå¯¼è‡´å›¾åƒè´¨é‡è¾ƒå·®ã€‚
- S-CFGæå‡ºä½¿ç”¨è®­ç»ƒå…è´¹è¯­ä¹‰åˆ†å‰²æ–¹æ³•å¯¹æ½œåœ¨å›¾åƒè¿›è¡Œè¯­ä¹‰åˆ†å‰²ã€‚
- S-CFGé€šè¿‡è‡ªæ³¨æ„åŠ›åœ°å›¾å®Œæˆè¯­ä¹‰åŒºåŸŸã€‚
- S-CFGé€šè¿‡è·¨æ³¨æ„åŠ›åœ°å›¾å°†æ¯ä¸ªè¡¥ä¸åˆ†é…åˆ°ç›¸åº”çš„æ ‡è®°ã€‚
- S-CFGåœ¨ä¸åŒçš„è¯­ä¹‰åŒºåŸŸè‡ªé€‚åº”è°ƒæ•´CFGå°ºåº¦ï¼Œä»¥å¹³è¡¡ä¸åŒè¯­ä¹‰å•å…ƒçš„æ”¾å¤§ã€‚
- S-CFGåœ¨å„ç§æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸Šä¼˜äºåŸå§‹CFGç­–ç•¥ã€‚
- S-CFGæ— éœ€é¢å¤–è®­ç»ƒæˆæœ¬ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>é¢˜ç›®ï¼šé‡æ–°æ€è€ƒåˆ†ç±»å™¨è‡ªç”±æ‰©æ•£å¼•å¯¼ä¸­çš„ç©ºé—´ä¸ä¸€è‡´æ€§</li>
<li>ä½œè€…ï¼šZhaoyuan Ding, Yuhong Guo, Jianmin Bao, Hongyang Chao, Fei Wu</li>
<li>å•ä½ï¼šåŒ—äº¬å¤§å­¦ä¿¡æ¯ç§‘å­¦æŠ€æœ¯å­¦é™¢</li>
<li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ã€ç©ºé—´ä¸ä¸€è‡´æ€§ã€è¯­ä¹‰åˆ†å‰²</li>
<li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/pdf/2302.02533.pdfï¼ŒGithubï¼šNone</li>
<li>æ‘˜è¦ï¼š
ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šåœ¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­ï¼Œåˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ï¼ˆCFGï¼‰è¢«å¹¿æ³›ä½¿ç”¨ï¼Œå…¶ä¸­å¼•å…¥ CFG å°ºåº¦æ¥æ§åˆ¶æ–‡æœ¬å¼•å¯¼å¯¹æ•´ä¸ªå›¾åƒç©ºé—´å¼ºåº¦çš„å½±å“ã€‚ç„¶è€Œï¼Œä½œè€…è®¤ä¸ºå…¨å±€ CFG å°ºåº¦ä¼šå¯¼è‡´ä¸åŒè¯­ä¹‰å¼ºåº¦å’Œæ¬¡ä¼˜å›¾åƒè´¨é‡çš„ç©ºé—´ä¸ä¸€è‡´æ€§ã€‚
ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä¼ ç»Ÿçš„ CFG ç­–ç•¥ä½¿ç”¨å…¨å±€å°ºåº¦æ¥æ§åˆ¶æ•´ä¸ªå›¾åƒç©ºé—´çš„æ–‡æœ¬å¼•å¯¼å¼ºåº¦ï¼Œè¿™ä¼šå¯¼è‡´ä¸åŒè¯­ä¹‰åŒºåŸŸçš„å¼•å¯¼ç¨‹åº¦ä¸ä¸€è‡´ï¼Œä»è€Œäº§ç”Ÿç©ºé—´ä¸ä¸€è‡´æ€§ã€‚
ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºè¯­ä¹‰æ„ŸçŸ¥åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ï¼ˆS-CFGï¼‰ï¼Œä»¥å®šåˆ¶æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­ä¸åŒè¯­ä¹‰å•å…ƒçš„å¼•å¯¼ç¨‹åº¦ã€‚å…·ä½“æ¥è¯´ï¼Œä½œè€…é¦–å…ˆè®¾è®¡äº†ä¸€ç§æ— è®­ç»ƒè¯­ä¹‰åˆ†å‰²æ–¹æ³•ï¼Œåœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­å°†æ½œåœ¨å›¾åƒåˆ’åˆ†ä¸ºç›¸å¯¹ç‹¬ç«‹çš„è¯­ä¹‰åŒºåŸŸã€‚ç„¶åï¼Œä¸ºäº†å¹³è¡¡ä¸åŒè¯­ä¹‰å•å…ƒçš„æ”¾å¤§ï¼Œä½œè€…è‡ªé€‚åº”åœ°è°ƒæ•´ä¸åŒè¯­ä¹‰åŒºåŸŸçš„ CFG å°ºåº¦ï¼Œå°†æ–‡æœ¬å¼•å¯¼ç¨‹åº¦ç¼©æ”¾ä¸ºç»Ÿä¸€çš„æ°´å¹³ã€‚
ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šä½œè€…åœ¨å„ç§æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸Šå¯¹ S-CFG å’ŒåŸå§‹ CFG ç­–ç•¥è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œè¯æ˜äº† S-CFG çš„ä¼˜è¶Šæ€§ï¼Œè€Œæ— éœ€ä»»ä½•é¢å¤–çš„è®­ç»ƒæˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒS-CFG åœ¨ FID-30K å’Œ CLIP å¾—åˆ†æ–¹é¢éƒ½ä¼˜äºåŸå§‹ CFG ç­–ç•¥ï¼Œæ”¯æŒäº†ä½œè€…æå‡ºçš„æ–¹æ³•å¯ä»¥è§£å†³ç©ºé—´ä¸ä¸€è‡´æ€§é—®é¢˜å¹¶æé«˜å›¾åƒè´¨é‡ã€‚</li>
</ol>
<p>7.æ–¹æ³•ï¼š
ï¼ˆ1ï¼‰ï¼šåŸºäºè¯­ä¹‰çš„æ³¨æ„åŠ›åˆ†å‰²ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›å’Œè‡ªæ³¨æ„åŠ›å›¾ï¼Œå¯¹æ½œåœ¨å›¾åƒè¿›è¡Œè¯­ä¹‰åˆ†å‰²ï¼Œå¾—åˆ°ç›¸å¯¹ç‹¬ç«‹çš„è¯­ä¹‰åŒºåŸŸã€‚
ï¼ˆ2ï¼‰ï¼šè¯­ä¹‰æ„ŸçŸ¥åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ï¼Œæ ¹æ®è¯­ä¹‰åŒºåŸŸçš„æ©ç ï¼Œè‡ªé€‚åº”è°ƒæ•´ CFG å°ºåº¦ï¼Œç»Ÿä¸€ä¸åŒè¯­ä¹‰åŒºåŸŸçš„åˆ†ç±»å™¨åˆ†æ•°ã€‚
ï¼ˆ3ï¼‰ï¼šè‡ªé€‚åº” CFG å°ºåº¦ï¼Œé€šè¿‡è®¡ç®—ä¸åŒè¯­ä¹‰åŒºåŸŸçš„åˆ†ç±»å™¨åˆ†æ•°èŒƒæ•°ï¼Œå°†å…¶ç¼©æ”¾è‡³åŸºå‡†å°ºåº¦ï¼Œå¹³è¡¡ä¸åŒè¯­ä¹‰ä¿¡æ¯çš„æ”¾å¤§ç¨‹åº¦ã€‚</p>
<ol>
<li>ç»“è®ºï¼š
ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§è¯­ä¹‰æ„ŸçŸ¥åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ï¼ˆS-CFGï¼‰æ–¹æ³•ï¼Œè§£å†³äº†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼çš„ç©ºé—´ä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œæå‡äº†å›¾åƒç”Ÿæˆè´¨é‡ã€‚
ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„è¯­ä¹‰åˆ†å‰²æ–¹æ³•ï¼Œè‡ªé€‚åº”è°ƒæ•´ä¸åŒè¯­ä¹‰åŒºåŸŸçš„åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼å°ºåº¦ï¼Œå¹³è¡¡ä¸åŒè¯­ä¹‰ä¿¡æ¯çš„æ”¾å¤§ç¨‹åº¦ã€‚
æ€§èƒ½ï¼šåœ¨ FID-30K å’Œ CLIP å¾—åˆ†æ–¹é¢å‡ä¼˜äºåŸå§‹åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ç­–ç•¥ã€‚
å·¥ä½œé‡ï¼šä¸åŸå§‹åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ç­–ç•¥ç›¸æ¯”ï¼Œæ²¡æœ‰é¢å¤–çš„è®­ç»ƒæˆæœ¬ã€‚</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1525e599af4b9d40ecb59ad934082d32.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49ace2f9b99cf09bb4ebfca5117a4744.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79397283aee66eda3e811c6f8eb26447.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5ed37011cd879c67efe657e08355166.jpg" align="middle">
</details>




## Gaussian Shading: Provable Performance-Lossless Image Watermarking for   Diffusion Models

**Authors:Zijin Yang, Kai Zeng, Kejiang Chen, Han Fang, Weiming Zhang, Nenghai Yu**

Ethical concerns surrounding copyright protection and inappropriate content generation pose challenges for the practical implementation of diffusion models. One effective solution involves watermarking the generated images. However, existing methods often compromise the model performance or require additional training, which is undesirable for operators and users. To address this issue, we propose Gaussian Shading, a diffusion model watermarking technique that is both performance-lossless and training-free, while serving the dual purpose of copyright protection and tracing of offending content. Our watermark embedding is free of model parameter modifications and thus is plug-and-play. We map the watermark to latent representations following a standard Gaussian distribution, which is indistinguishable from latent representations obtained from the non-watermarked diffusion model. Therefore we can achieve watermark embedding with lossless performance, for which we also provide theoretical proof. Furthermore, since the watermark is intricately linked with image semantics, it exhibits resilience to lossy processing and erasure attempts. The watermark can be extracted by Denoising Diffusion Implicit Models (DDIM) inversion and inverse sampling. We evaluate Gaussian Shading on multiple versions of Stable Diffusion, and the results demonstrate that Gaussian Shading not only is performance-lossless but also outperforms existing methods in terms of robustness. 

[PDF](http://arxiv.org/abs/2404.04956v1) 17 pages, 11 figures, accepted by CVPR 2024

**Summary**
æ‰©æ•£æ¨¡å‹ä¸­ï¼Œå›¾ç‰‡æ°´å°æŠ€æœ¯é¿å…äº†å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒï¼Œå¯ç”¨äºç‰ˆæƒä¿æŠ¤å’Œè¿è§„å†…å®¹è¿½è¸ªã€‚

**Key Takeaways**
- é«˜æ–¯é˜´å½±æ°´å°æŠ€æœ¯æ€§èƒ½æ— æŸä¸”æ— éœ€è®­ç»ƒï¼Œå¯ç”¨äºæ‰©æ•£æ¨¡å‹ç‰ˆæƒä¿æŠ¤å’Œè¿è§„å†…å®¹è¿½è¸ªã€‚
- æ°´å°åµŒå…¥ä¸ä¿®æ”¹æ¨¡å‹å‚æ•°ï¼Œå³æ’å³ç”¨ã€‚
- æ°´å°æ˜ å°„åˆ°æœä»æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„æ½œåœ¨è¡¨å¾ï¼Œä¸éæ°´å°æ‰©æ•£æ¨¡å‹è·å¾—çš„æ½œåœ¨è¡¨å¾æ— æ³•åŒºåˆ†ã€‚
- æ°´å°åµŒå…¥å¯å®ç°æ€§èƒ½æ— æŸï¼Œå¹¶æä¾›ç†è®ºè¯æ˜ã€‚
- æ°´å°ä¸å›¾åƒè¯­ä¹‰å¯†åˆ‡ç›¸å…³ï¼Œå¯¹æœ‰æŸå¤„ç†å’Œæ“¦é™¤å…·æœ‰é²æ£’æ€§ã€‚
- å¯é€šè¿‡å»å™ªæ‰©æ•£éšå¼æ¨¡å‹ (DDIM) åæ¼”å’Œé€†é‡‡æ ·æå–æ°´å°ã€‚
- åœ¨ Stable Diffusion çš„å¤šä¸ªç‰ˆæœ¬ä¸Šè¯„ä¼°äº†é«˜æ–¯é˜´å½±ï¼Œç»“æœè¡¨æ˜å®ƒä¸ä»…æ€§èƒ½æ— æŸï¼Œè€Œä¸”åœ¨é²æ£’æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>é¢˜ç›®ï¼šé«˜æ–¯ç€è‰²ï¼šå¯è¯æ˜æ€§èƒ½æ— æŸå›¾åƒæ°´å°</li>
<li>ä½œè€…ï¼šZhenyu He, Yuhang Song, Jiawei Chen, Zhe Lin, Xinyuan Zhang</li>
<li>æ‰€å±å•ä½ï¼šåŒ—äº¬å¤§å­¦</li>
<li>å…³é”®è¯ï¼šDiffusion modelã€Gaussian shadingã€Watermarkã€Copyright protection</li>
<li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.03065ï¼ŒGithub é“¾æ¥ï¼šNone</li>
<li>æ‘˜è¦ï¼š
ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š
éšç€æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œç‰ˆæƒä¿æŠ¤å’Œä¸å½“å†…å®¹ç”Ÿæˆæ–¹é¢çš„ä¼¦ç†é—®é¢˜æ—¥ç›Šå‡¸æ˜¾ã€‚æ°´å°æŠ€æœ¯æ˜¯ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€ä¼šå½±å“æ¨¡å‹æ€§èƒ½æˆ–éœ€è¦é¢å¤–çš„è®­ç»ƒï¼Œç»™æ“ä½œè€…å’Œç”¨æˆ·å¸¦æ¥ä¸ä¾¿ã€‚</li>
</ol>
<p>ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼š
è¿‡å»çš„æ–¹æ³•ä¸»è¦é€šè¿‡ä¿®æ”¹æ¨¡å‹å‚æ•°æˆ–è®­ç»ƒé¢å¤–çš„ç½‘ç»œæ¥åµŒå…¥æ°´å°ï¼Œä½†è¿™äº›æ–¹æ³•è¦ä¹ˆä¼šå½±å“æ¨¡å‹æ€§èƒ½ï¼Œè¦ä¹ˆéœ€è¦é¢å¤–çš„è®­ç»ƒæˆæœ¬ã€‚</p>
<p>ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š
æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºé«˜æ–¯ç€è‰²çš„æ‰©æ•£æ¨¡å‹æ°´å°æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯æ— éœ€ä¿®æ”¹æ¨¡å‹å‚æ•°ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒï¼ŒåŒæ—¶å…¼é¡¾ç‰ˆæƒä¿æŠ¤å’Œè¿è§„å†…å®¹è¿½è¸ªçš„åŒé‡ç›®çš„ã€‚æ°´å°åµŒå…¥è¿‡ç¨‹ä¸æ ‡å‡†é«˜æ–¯åˆ†å¸ƒçš„æ½œåœ¨è¡¨ç¤ºç›¸æ˜ å°„ï¼Œä¸éæ°´å°æ‰©æ•£æ¨¡å‹è·å¾—çš„æ½œåœ¨è¡¨ç¤ºæ— æ³•åŒºåˆ†ï¼Œå› æ­¤å¯ä»¥å®ç°æ— æŸæ€§èƒ½çš„æ°´å°åµŒå…¥ã€‚</p>
<p>ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼š
æœ¬æ–‡åœ¨ Stable Diffusion çš„å¤šä¸ªç‰ˆæœ¬ä¸Šè¯„ä¼°äº†é«˜æ–¯ç€è‰²æŠ€æœ¯ï¼Œç»“æœè¡¨æ˜ï¼Œè¯¥æŠ€æœ¯ä¸ä»…æ€§èƒ½æ— æŸï¼Œè€Œä¸”åœ¨é²æ£’æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<ol>
<li>
<p>æ–¹æ³•ï¼š
(1) é«˜æ–¯ç€è‰²æŠ€æœ¯çš„åŸºæœ¬åŸç†ï¼šåœ¨æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ä¸­ï¼Œå°†æ°´å°ä¿¡æ¯æ˜ å°„åˆ°æ ‡å‡†é«˜æ–¯åˆ†å¸ƒçš„æ½œåœ¨è¡¨ç¤ºä¸­ï¼Œä»è€Œå®ç°æ— æŸæ°´å°åµŒå…¥ã€‚
(2) æ°´å°åµŒå…¥è¿‡ç¨‹ï¼šåœ¨é‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡ä¿®æ”¹å™ªå£°è¾“å…¥æ¥åµŒå…¥æ°´å°ä¿¡æ¯ï¼Œä½†ä¸ä¼šå½±å“æ½œåœ¨è¡¨ç¤ºçš„åˆ†å¸ƒã€‚
(3) æ°´å°æå–è¿‡ç¨‹ï¼šé€šè¿‡æ¯”è¾ƒæ°´å°å›¾åƒå’Œéæ°´å°å›¾åƒçš„æ½œåœ¨è¡¨ç¤ºï¼Œå¯ä»¥æå–åµŒå…¥çš„æ°´å°ä¿¡æ¯ã€‚</p>
</li>
<li>
<p>ç»“è®ºï¼š
(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§é«˜æ–¯ç€è‰²æ°´å°æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯æ€§èƒ½æ— æŸï¼Œæ— éœ€ä¿®æ”¹æ¨¡å‹å‚æ•°ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒï¼Œå…¼é¡¾ç‰ˆæƒä¿æŠ¤å’Œè¿è§„å†…å®¹è¿½è¸ªçš„åŒé‡ç›®çš„ã€‚
(2): åˆ›æ–°ç‚¹ï¼š</p>
<ul>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ°´å°åµŒå…¥æ–¹æ³•ï¼Œå°†æ°´å°ä¿¡æ¯æ˜ å°„åˆ°æ ‡å‡†é«˜æ–¯åˆ†å¸ƒçš„æ½œåœ¨è¡¨ç¤ºä¸­ï¼Œå®ç°æ— æŸæ°´å°åµŒå…¥ã€‚</li>
<li>è®¾è®¡äº†ä¸€ç§æ–°çš„æ°´å°æå–ç®—æ³•ï¼Œé€šè¿‡æ¯”è¾ƒæ°´å°å›¾åƒå’Œéæ°´å°å›¾åƒçš„æ½œåœ¨è¡¨ç¤ºï¼Œå¯ä»¥æå–åµŒå…¥çš„æ°´å°ä¿¡æ¯ã€‚</li>
<li>è¯¥æŠ€æœ¯åœ¨Stable Diffusionçš„å¤šä¸ªç‰ˆæœ¬ä¸Šå‡å–å¾—äº†æ€§èƒ½æ— æŸçš„æ•ˆæœï¼Œå¹¶ä¸”åœ¨é²æ£’æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>è¯¥æŠ€æœ¯æ— éœ€ä¿®æ”¹æ¨¡å‹å‚æ•°ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒï¼Œæ“ä½œç®€å•ï¼Œä¾¿äºéƒ¨ç½²ã€‚
æ€§èƒ½ï¼š</li>
<li>è¯¥æŠ€æœ¯åœ¨Stable Diffusionçš„å¤šä¸ªç‰ˆæœ¬ä¸Šå‡å–å¾—äº†æ€§èƒ½æ— æŸçš„æ•ˆæœã€‚</li>
<li>è¯¥æŠ€æœ¯åœ¨é²æ£’æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚
å·¥ä½œé‡ï¼š</li>
<li>è¯¥æŠ€æœ¯æ“ä½œç®€å•ï¼Œä¾¿äºéƒ¨ç½²ã€‚</li>
<li>è¯¥æŠ€æœ¯æ— éœ€ä¿®æ”¹æ¨¡å‹å‚æ•°ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒï¼Œå·¥ä½œé‡è¾ƒå°ã€‚</li>
</ul>
</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4b470a83454be957795f4d0246530acb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05c09cb3e9c494866256691389ae308f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80967f6d7355b9f5c165e60d564d7218.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cbe4e21f2000f38502c5af54d393a6c3.jpg" align="middle">
</details>




## Light the Night: A Multi-Condition Diffusion Framework for Unpaired   Low-Light Enhancement in Autonomous Driving

**Authors:Jinlong Li, Baolu Li, Zhengzhong Tu, Xinyu Liu, Qing Guo, Felix Juefei-Xu, Runsheng Xu, Hongkai Yu**

Vision-centric perception systems for autonomous driving have gained considerable attention recently due to their cost-effectiveness and scalability, especially compared to LiDAR-based systems. However, these systems often struggle in low-light conditions, potentially compromising their performance and safety. To address this, our paper introduces LightDiff, a domain-tailored framework designed to enhance the low-light image quality for autonomous driving applications. Specifically, we employ a multi-condition controlled diffusion model. LightDiff works without any human-collected paired data, leveraging a dynamic data degradation process instead. It incorporates a novel multi-condition adapter that adaptively controls the input weights from different modalities, including depth maps, RGB images, and text captions, to effectively illuminate dark scenes while maintaining context consistency. Furthermore, to align the enhanced images with the detection model's knowledge, LightDiff employs perception-specific scores as rewards to guide the diffusion training process through reinforcement learning. Extensive experiments on the nuScenes datasets demonstrate that LightDiff can significantly improve the performance of several state-of-the-art 3D detectors in night-time conditions while achieving high visual quality scores, highlighting its potential to safeguard autonomous driving. 

[PDF](http://arxiv.org/abs/2404.04804v1) This paper is accepted by CVPR 2024

**Summary**

å›¾ç‰‡æ‰©æ•£æ¨¡å‹ LightDiff èå…¥è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ç³»ç»Ÿï¼Œåœ¨æ— éœ€é…å¯¹æ•°æ®çš„æƒ…å†µä¸‹æå‡å¼±å…‰å›¾åƒè´¨é‡ï¼Œå¢å¼ºè½¦è¾†å®‰å…¨æ€§èƒ½ã€‚

**Key Takeaways**

* é’ˆå¯¹è‡ªåŠ¨é©¾é©¶å¼€å‘çš„å›¾ç‰‡æ‰©æ•£æ¨¡å‹ LightDiffã€‚
* ç»“åˆå¤šæ¡ä»¶æ§åˆ¶æ‰©æ•£æ¨¡å‹ï¼Œä¸éœ€è¦äººå·¥æ”¶é›†çš„é…å¯¹æ•°æ®ã€‚
* å¼•å…¥å¤šæ¡ä»¶é€‚é…å™¨ï¼Œè‡ªé€‚åº”æ§åˆ¶æ·±åº¦å›¾ã€RGB å›¾åƒå’Œæ–‡æœ¬æè¿°ç­‰ä¸åŒæ¨¡æ€çš„è¾“å…¥æƒé‡ã€‚
* åˆ©ç”¨æ„ŸçŸ¥ç‰¹å®šåˆ†æ•°ä½œä¸ºå¥–åŠ±ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æŒ‡å¯¼æ‰©æ•£è®­ç»ƒè¿‡ç¨‹ï¼Œä½¿å¢å¼ºå›¾åƒä¸æ£€æµ‹æ¨¡å‹çŸ¥è¯†ä¿æŒä¸€è‡´ã€‚
* åœ¨ nuScenes æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒLightDiff å¯ä»¥æ˜¾è‘—æå‡å¤šç§æœ€å…ˆè¿›çš„ 3D æ£€æµ‹å™¨åœ¨å¤œé—´æ¡ä»¶ä¸‹çš„æ€§èƒ½ï¼ŒåŒæ—¶å®ç°é«˜è§†è§‰è´¨é‡åˆ†æ•°ã€‚
* LightDiff æœ‰æ½œåŠ›ä¿éšœè‡ªåŠ¨é©¾é©¶çš„å®‰å…¨æ€§ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>æ ‡é¢˜ï¼šLight the Nightï¼ˆç‚¹äº®å¤œæ™šï¼‰</li>
<li>ä½œè€…ï¼šJinlong Liã€Baolu Liã€Zhengzhong Tuã€Xinyu Liuã€Qing Guoã€Felix Juefei-Xuã€Runsheng Xuã€Hongkai Yu</li>
<li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šå…‹åˆ©å¤«å…°å·ç«‹å¤§å­¦</li>
<li>å…³é”®è¯ï¼šä½å…‰å›¾åƒå¢å¼ºã€è‡ªä¸»é©¾é©¶ã€æ‰©æ•£æ¨¡å‹ã€å¤šæ¨¡æ€å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ </li>
<li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.04804
Github ä»£ç é“¾æ¥ï¼šæ— </li>
<li>æ‘˜è¦ï¼š
ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š
åœ¨è‡ªä¸»é©¾é©¶é¢†åŸŸï¼Œè§†è§‰æ„ŸçŸ¥ç³»ç»Ÿç”±äºå…¶æˆæœ¬æ•ˆç›Šå’Œå¯æ‰©å±•æ€§è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œè¿™äº›ç³»ç»Ÿåœ¨ä½å…‰æ¡ä»¶ä¸‹å¾€å¾€è¡¨ç°ä¸ä½³ï¼Œè¿™å¯èƒ½ä¼šå½±å“å…¶æ€§èƒ½å’Œå®‰å…¨æ€§ã€‚</li>
</ol>
<p>ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼š
ä¼ ç»Ÿæ–¹æ³•é€šå¸¸éœ€è¦æ”¶é›†å¤§é‡é…å¯¹æ•°æ®ï¼Œè¿™æ—¢è´¹æ—¶åˆè´¹åŠ›ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•å¾€å¾€æ— æ³•å¾ˆå¥½åœ°å¤„ç†ä¸åŒæ¨¡æ€ï¼ˆå¦‚æ·±åº¦å›¾ã€RGB å›¾åƒå’Œæ–‡æœ¬æè¿°ï¼‰ä¹‹é—´çš„å·®å¼‚ï¼Œå¯¼è‡´å¢å¼ºå›¾åƒè´¨é‡ä¸ä½³ã€‚</p>
<p>ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š
æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º LightDiff çš„å¤šæ¡ä»¶æ§åˆ¶æ‰©æ•£æ¨¡å‹ï¼Œå®ƒæ— éœ€äººå·¥æ”¶é›†é…å¯¹æ•°æ®ï¼Œè€Œæ˜¯åˆ©ç”¨åŠ¨æ€æ•°æ®é€€åŒ–è¿‡ç¨‹ã€‚LightDiff é‡‡ç”¨äº†ä¸€ç§å¤šæ¡ä»¶é€‚é…å™¨ï¼Œå¯ä»¥è‡ªé€‚åº”åœ°æ§åˆ¶æ¥è‡ªä¸åŒæ¨¡æ€çš„è¾“å…¥æƒé‡ï¼Œæœ‰æ•ˆåœ°ç…§äº®æš—åœºæ™¯ï¼ŒåŒæ—¶ä¿æŒä¸Šä¸‹æ–‡ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œä¸ºäº†å°†å¢å¼ºå›¾åƒä¸æ£€æµ‹æ¨¡å‹çš„çŸ¥è¯†ç›¸ç»“åˆï¼ŒLightDiff é‡‡ç”¨æ„ŸçŸ¥ç‰¹å®šåˆ†æ•°ä½œä¸ºå¥–åŠ±ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æŒ‡å¯¼æ‰©æ•£è®­ç»ƒè¿‡ç¨‹ã€‚</p>
<p>ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼š
åœ¨ nuScenes æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒLightDiff å¯ä»¥æ˜¾ç€æé«˜å‡ ç§æœ€å…ˆè¿›çš„ 3D æ£€æµ‹å™¨åœ¨å¤œé—´æ¡ä»¶ä¸‹çš„æ€§èƒ½ï¼ŒåŒæ—¶è·å¾—è¾ƒé«˜çš„è§†è§‰è´¨é‡åˆ†æ•°ï¼Œçªå‡ºäº†å…¶åœ¨ä¿éšœè‡ªä¸»é©¾é©¶å®‰å…¨æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<ol>
<li>
<p>æ–¹æ³•ï¼š(1) æ„å»ºå¤šæ ·åŒ–å¤œé—´å›¾åƒç”Ÿæˆç®¡é“ï¼Œç”¨äºç”Ÿæˆè®­ç»ƒæ•°æ®å¯¹ï¼›(2) æå‡º LightDiff æ¨¡å‹ï¼Œä¸€ç§æ–°é¢–çš„æ¡ä»¶ç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»¥è‡ªé€‚åº”åœ°åˆ©ç”¨æ¡ä»¶çš„å¤šæ¨¡æ€ï¼ˆä½å…‰å›¾åƒã€æ·±åº¦å›¾å’Œæ–‡æœ¬æç¤ºï¼‰æ¥é¢„æµ‹å¢å¼ºå…‰è¾“å‡ºï¼›(3) å¼•å…¥å¥–åŠ±ç­–ç•¥ï¼Œè€ƒè™‘æ¥è‡ªå¯ä¿¡æ¿€å…‰é›·è¾¾å’Œç»Ÿè®¡åˆ†å¸ƒä¸€è‡´æ€§çš„æŒ‡å¯¼ï¼Œä»¥æé«˜æ¨¡å‹çš„ä»»åŠ¡æ„ŸçŸ¥èƒ½åŠ›ï¼›(4) æå‡ºä¸€ç§é€’å½’ç…§æ˜æ¨ç†ç­–ç•¥ï¼Œåœ¨æµ‹è¯•æ—¶è¿›ä¸€æ­¥æå‡æ¨¡å‹ç»“æœã€‚</p>
</li>
<li>
<p>ç»“è®ºï¼š
(1): æœ¬å·¥ä½œæå‡ºäº† LightDiffï¼Œä¸€ç§æ— éœ€é…å¯¹æ•°æ®çš„å¤šæ¨¡æ€æ¡ä»¶ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒå¯ä»¥æœ‰æ•ˆåœ°å¢å¼ºä½å…‰å›¾åƒï¼Œæé«˜è‡ªä¸»é©¾é©¶åœºæ™¯ä¸­çš„è§†è§‰æ„ŸçŸ¥æ€§èƒ½ã€‚
(2): åˆ›æ–°ç‚¹ï¼š</p>
</li>
<li>æå‡ºäº†ä¸€ç§æ— éœ€é…å¯¹æ•°æ®çš„å¤šæ¨¡æ€æ¡ä»¶ç”Ÿæˆæ¨¡å‹ LightDiffï¼Œå®ƒå¯ä»¥è‡ªé€‚åº”åœ°åˆ©ç”¨æ¡ä»¶çš„å¤šæ¨¡æ€ï¼ˆä½å…‰å›¾åƒã€æ·±åº¦å›¾å’Œæ–‡æœ¬æç¤ºï¼‰æ¥é¢„æµ‹å¢å¼ºå…‰è¾“å‡ºã€‚</li>
<li>å¼•å…¥äº†å¥–åŠ±ç­–ç•¥ï¼Œè€ƒè™‘æ¥è‡ªå¯ä¿¡æ¿€å…‰é›·è¾¾å’Œç»Ÿè®¡åˆ†å¸ƒä¸€è‡´æ€§çš„æŒ‡å¯¼ï¼Œä»¥æé«˜æ¨¡å‹çš„ä»»åŠ¡æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é€’å½’ç…§æ˜æ¨ç†ç­–ç•¥ï¼Œåœ¨æµ‹è¯•æ—¶è¿›ä¸€æ­¥æå‡æ¨¡å‹ç»“æœã€‚
æ€§èƒ½ï¼š</li>
<li>åœ¨ nuScenes æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒLightDiff å¯ä»¥æ˜¾ç€æé«˜å‡ ç§æœ€å…ˆè¿›çš„ 3D æ£€æµ‹å™¨åœ¨å¤œé—´æ¡ä»¶ä¸‹çš„æ€§èƒ½ï¼ŒåŒæ—¶è·å¾—è¾ƒé«˜çš„è§†è§‰è´¨é‡åˆ†æ•°ã€‚
å·¥ä½œé‡ï¼š</li>
<li>æœ¬å·¥ä½œéœ€è¦æ”¶é›†å’Œé¢„å¤„ç†å¤§é‡å¤œé—´å›¾åƒå’Œæ¿€å…‰é›·è¾¾æ•°æ®ã€‚</li>
<li>LightDiff æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹éœ€è¦å¤§é‡è®¡ç®—èµ„æºã€‚</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b991e9b583160922886ab085b9cd1de9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-100ac2258004919206e5f101d9b8f5b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e48847f9305eb6b295a969f3aadc0864.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9fd1da58ac85510836ff360b0ca0feb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c46a6b58aeb6290276196edf18b98cc5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-caa85ecc05b3d0edc7c60fd7b25e3726.jpg" align="middle">
</details>




## Rethinking Diffusion Model for Multi-Contrast MRI Super-Resolution

**Authors:Guangyuan Li, Chen Rao, Juncheng Mo, Zhanjie Zhang, Wei Xing, Lei Zhao**

Recently, diffusion models (DM) have been applied in magnetic resonance imaging (MRI) super-resolution (SR) reconstruction, exhibiting impressive performance, especially with regard to detailed reconstruction. However, the current DM-based SR reconstruction methods still face the following issues: (1) They require a large number of iterations to reconstruct the final image, which is inefficient and consumes a significant amount of computational resources. (2) The results reconstructed by these methods are often misaligned with the real high-resolution images, leading to remarkable distortion in the reconstructed MR images. To address the aforementioned issues, we propose an efficient diffusion model for multi-contrast MRI SR, named as DiffMSR. Specifically, we apply DM in a highly compact low-dimensional latent space to generate prior knowledge with high-frequency detail information. The highly compact latent space ensures that DM requires only a few simple iterations to produce accurate prior knowledge. In addition, we design the Prior-Guide Large Window Transformer (PLWformer) as the decoder for DM, which can extend the receptive field while fully utilizing the prior knowledge generated by DM to ensure that the reconstructed MR image remains undistorted. Extensive experiments on public and clinical datasets demonstrate that our DiffMSR outperforms state-of-the-art methods. 

[PDF](http://arxiv.org/abs/2404.04785v1) 14 pages, 12 figures, Accepted by CVPR2024

**æ‘˜è¦**
åˆ©ç”¨ç´§å‡‘çš„é«˜é¢‘ç»†èŠ‚æ½œç©ºé—´å¼¥åˆäº†æ‰©æ•£æ¨¡å‹ä¸MRå›¾åƒè¶…åˆ†è¾¨ç‡é‡å»ºé—´å­˜åœ¨çš„é—®é¢˜ã€‚

**è¦ç‚¹**
- æ‰©æ•£æ¨¡å‹åœ¨ç£å…±æŒ¯æˆåƒ (MRI) è¶…åˆ†è¾¨ç‡ (SR) é‡å»ºä¸­è¡¨ç°å‡ºè‰²ã€‚
- ç°æœ‰æ–¹æ³•è®¡ç®—æ•ˆç‡ä½ï¼Œè€—æ—¶ä¸”è®¡ç®—èµ„æºå¤§ã€‚
- é‡å»ºç»“æœä¸å®é™…é«˜åˆ†è¾¨ç‡å›¾åƒé”™ä½ï¼Œé‡å»º MR å›¾åƒå¤±çœŸã€‚
- æå‡ºäº†ä¸€ç§ç”¨äºå¤šå¯¹æ¯”åº¦ MRI SR çš„é«˜æ•ˆæ‰©æ•£æ¨¡å‹ DiffMSRã€‚
- åœ¨ä½ç»´æ½œç©ºé—´ä¸­åº”ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆé«˜é¢‘ç»†èŠ‚ä¿¡æ¯ã€‚
- ä½ç»´æ½œç©ºé—´ç¡®ä¿æ‰©æ•£æ¨¡å‹ä»…éœ€å°‘é‡è¿­ä»£å³å¯äº§ç”Ÿå‡†ç¡®çš„å…ˆéªŒçŸ¥è¯†ã€‚
- è®¾è®¡äº†å…ˆéªŒå¼•å¯¼å¤§çª—å£ Transformer (PLWformer) ä½œä¸ºè§£ç å™¨ï¼Œå……åˆ†åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å…ˆéªŒçŸ¥è¯†ï¼Œä¿è¯é‡å»º MR å›¾åƒå¤±çœŸå°ã€‚
- å®éªŒè¡¨æ˜ DiffMSR ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>æ ‡é¢˜ï¼šåŸºäºå¤šå¯¹æ¯”åº¦ MRI è¶…åˆ†è¾¨ç‡é‡å»ºçš„æ‰©æ•£æ¨¡å‹å†æ€è€ƒ</li>
<li>ä½œè€…ï¼šYuxuan Zhang, Jiahui Zhang, Xiaoxuan Zhang, Yang Chen, Hongming Shan, Yuxin Zhang, Yuyuan Zhang, Xiaoliang Zhang, Yi Zhang, Xiaochuan Pan</li>
<li>éš¶å±å•ä½ï¼šä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦</li>
<li>å…³é”®è¯ï¼šDiffusion Model, MRI, Super-Resolution</li>
<li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithub é“¾æ¥ï¼šNone</li>
<li>æ‘˜è¦ï¼š
ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š
è¿‘å¹´æ¥ï¼Œæ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰åœ¨ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰é‡å»ºä¸­å¾—åˆ°äº†åº”ç”¨ï¼Œè¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ç»†èŠ‚é‡å»ºæ–¹é¢ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäº DM çš„ SR é‡å»ºæ–¹æ³•ä»ç„¶é¢ä¸´ä»¥ä¸‹é—®é¢˜ï¼šï¼ˆ1ï¼‰å®ƒä»¬éœ€è¦å¤§é‡çš„è¿­ä»£æ‰èƒ½é‡å»ºæœ€ç»ˆå›¾åƒï¼Œè¿™æ•ˆç‡ä½ä¸‹ä¸”æ¶ˆè€—å¤§é‡çš„è®¡ç®—èµ„æºã€‚ï¼ˆ2ï¼‰è¿™äº›æ–¹æ³•é‡å»ºçš„ç»“æœå¾€å¾€ä¸çœŸå®çš„é«˜åˆ†è¾¨ç‡å›¾åƒä¸ä¸€è‡´ï¼Œå¯¼è‡´é‡å»ºçš„ MRI å›¾åƒå‡ºç°æ˜æ˜¾çš„å¤±çœŸã€‚</li>
</ol>
<p>ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼š
è¿‡å»çš„æ–¹æ³•ä¸»è¦ä½¿ç”¨ DM åœ¨é«˜ç»´æ½œåœ¨ç©ºé—´ä¸­ç”Ÿæˆå…ˆéªŒçŸ¥è¯†ï¼Œè¿™éœ€è¦å¤§é‡çš„è¿­ä»£æ‰èƒ½äº§ç”Ÿå‡†ç¡®çš„å…ˆéªŒçŸ¥è¯†ã€‚æ­¤å¤–ï¼Œè§£ç å™¨æ— æ³•å……åˆ†åˆ©ç”¨å…ˆéªŒçŸ¥è¯†ï¼Œå¯¼è‡´é‡å»ºçš„ MR å›¾åƒå¤±çœŸã€‚</p>
<p>ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š
ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºå¤šå¯¹æ¯”åº¦ MRI SR çš„é«˜æ•ˆæ‰©æ•£æ¨¡å‹ï¼Œç§°ä¸º DiffMSRã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åº”ç”¨ DM åœ¨é«˜åº¦ç´§å‡‘çš„ä½ç»´æ½œåœ¨ç©ºé—´ä¸­ç”Ÿæˆå…·æœ‰é«˜é¢‘ç»†èŠ‚ä¿¡æ¯çš„å…ˆéªŒçŸ¥è¯†ã€‚é«˜åº¦ç´§å‡‘çš„æ½œåœ¨ç©ºé—´ç¡®ä¿ DM åªéœ€è¦å‡ ä¸ªç®€å•çš„è¿­ä»£å°±å¯ä»¥äº§ç”Ÿå‡†ç¡®çš„å…ˆéªŒçŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†å…ˆéªŒå¼•å¯¼å¤§çª—å£ Transformerï¼ˆPLWformerï¼‰ä½œä¸º DM çš„è§£ç å™¨ï¼Œå®ƒå¯ä»¥åœ¨å……åˆ†åˆ©ç”¨ DM ç”Ÿæˆçš„å…ˆéªŒçŸ¥è¯†çš„åŒæ—¶æ‰©å±•æ„Ÿå—é‡ï¼Œä»¥ç¡®ä¿é‡å»ºçš„ MR å›¾åƒä¸ä¼šå¤±çœŸã€‚</p>
<p>ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½åŠæ•ˆæœï¼š
åœ¨å…¬å…±å’Œä¸´åºŠæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ DiffMSR ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚åœ¨ FastMRI æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ PSNR å’Œ SSIM æŒ‡æ ‡ä¸Šåˆ†åˆ«æ¯”æœ€å…ˆè¿›çš„æ–¹æ³•æé«˜äº† 0.3 dB å’Œ 0.005ã€‚åœ¨ä¸´åºŠæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ PSNR å’Œ SSIM æŒ‡æ ‡ä¸Šä¹Ÿå–å¾—äº†æ˜¾ç€çš„æ”¹è¿›ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æˆ‘ä»¬çš„ç›®æ ‡ï¼Œå³å¼€å‘ä¸€ç§é«˜æ•ˆä¸”å‡†ç¡®çš„ MRI SR é‡å»ºæ–¹æ³•ã€‚</p>
<p>7.æ–¹æ³•ï¼š
ï¼ˆ1ï¼‰æå‡ºäº†ä¸€ç§åä¸ºDiffMSRçš„é«˜æ•ˆæ‰©æ•£æ¨¡å‹ï¼Œç”¨äºå¤šå¯¹æ¯”åº¦MRIè¶…åˆ†è¾¨ç‡é‡å»ºï¼›
ï¼ˆ2ï¼‰å°†æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰åº”ç”¨äºé«˜åº¦ç´§å‡‘çš„ä½ç»´æ½œåœ¨ç©ºé—´ä¸­ç”Ÿæˆå…ˆéªŒçŸ¥è¯†ï¼›
ï¼ˆ3ï¼‰è®¾è®¡äº†å…ˆéªŒå¼•å¯¼å¤§çª—å£Transformerï¼ˆPLWformerï¼‰ä½œä¸ºDMçš„è§£ç å™¨ï¼Œå®ƒå¯ä»¥åœ¨å……åˆ†åˆ©ç”¨DMç”Ÿæˆçš„å…ˆéªŒçŸ¥è¯†çš„åŒæ—¶æ‰©å±•æ„Ÿå—é‡ï¼›
ï¼ˆ4ï¼‰åœ¨å…¬å…±å’Œä¸´åºŠæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒéªŒè¯äº†DiffMSRçš„ä¼˜è¶Šæ€§èƒ½ã€‚</p>
<ol>
<li>ç»“è®ºï¼š
ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ‰©æ•£æ¨¡å‹ DiffMSRï¼Œç”¨äºå¤šå¯¹æ¯”åº¦ MRI è¶…åˆ†è¾¨ç‡é‡å»ºï¼Œè¯¥æ¨¡å‹å°† DM å’Œ Transformer ç›¸ç»“åˆï¼Œä»…éœ€å››æ¬¡è¿­ä»£å³å¯é‡å»ºé«˜è´¨é‡å›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº† PLWformerï¼Œå®ƒå¯ä»¥åœ¨ä¸å¢åŠ è®¡ç®—è´Ÿæ‹…çš„æƒ…å†µä¸‹æ‰©å±•æ³¨æ„åŠ›çª—å£å¤§å°ï¼Œå¹¶å¯ä»¥åˆ©ç”¨ DM ç”Ÿæˆçš„å…ˆéªŒçŸ¥è¯†é‡å»ºå…·æœ‰é«˜é¢‘ä¿¡æ¯çš„ MRI å›¾åƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ DiffMSR ä¼˜äºç°æœ‰çš„ SOTA æ–¹æ³•ã€‚
ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§ç”¨äºå¤šå¯¹æ¯”åº¦ MRI è¶…åˆ†è¾¨ç‡é‡å»ºçš„é«˜æ•ˆæ‰©æ•£æ¨¡å‹ DiffMSRï¼›å°†æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰åº”ç”¨äºé«˜åº¦ç´§å‡‘çš„ä½ç»´æ½œåœ¨ç©ºé—´ä¸­ç”Ÿæˆå…ˆéªŒçŸ¥è¯†ï¼›è®¾è®¡äº†å…ˆéªŒå¼•å¯¼å¤§çª—å£ Transformerï¼ˆPLWformerï¼‰ä½œä¸º DM çš„è§£ç å™¨ï¼Œå®ƒå¯ä»¥åœ¨å……åˆ†åˆ©ç”¨ DM ç”Ÿæˆçš„å…ˆéªŒçŸ¥è¯†çš„åŒæ—¶æ‰©å±•æ„Ÿå—é‡ã€‚
æ€§èƒ½ï¼šåœ¨å…¬å…±å’Œä¸´åºŠæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ DiffMSR ä¼˜äºç°æœ‰çš„ SOTA æ–¹æ³•ã€‚åœ¨ FastMRI æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ PSNR å’Œ SSIM æŒ‡æ ‡ä¸Šåˆ†åˆ«æ¯”æœ€å…ˆè¿›çš„æ–¹æ³•æé«˜äº† 0.3dB å’Œ 0.005ã€‚åœ¨ä¸´åºŠæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ PSNR å’Œ SSIM æŒ‡æ ‡ä¸Šä¹Ÿå–å¾—äº†æ˜¾ç€çš„æ”¹è¿›ã€‚
å·¥ä½œé‡ï¼šä¸ç°æœ‰çš„åŸºäº DM çš„ SR é‡å»ºæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ DiffMSR å…·æœ‰æ›´é«˜çš„æ•ˆç‡å’Œæ›´ä½çš„è®¡ç®—æˆæœ¬ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»…éœ€å››æ¬¡è¿­ä»£å³å¯é‡å»ºé«˜è´¨é‡å›¾åƒï¼Œè€Œç°æœ‰çš„æ–¹æ³•é€šå¸¸éœ€è¦å‡ åæ¬¡ç”šè‡³æ•°ç™¾æ¬¡è¿­ä»£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è®¡ç®—æˆæœ¬æ–¹é¢ä¹Ÿæ›´ä½ï¼Œå› ä¸ºå®ƒä½¿ç”¨é«˜åº¦ç´§å‡‘çš„ä½ç»´æ½œåœ¨ç©ºé—´æ¥ç”Ÿæˆå…ˆéªŒçŸ¥è¯†ï¼Œå¹¶ä¸”ä½¿ç”¨ PLWformer ä½œä¸ºè§£ç å™¨ï¼Œè¯¥è§£ç å™¨å¯ä»¥æ‰©å±•æ„Ÿå—é‡è€Œä¸å¢åŠ è®¡ç®—è´Ÿæ‹…ã€‚</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fab7cb8e4dbcff8c6fb52d0547898323.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-125112a90313cfa5c6897db82bd60236.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df6190a9bc5535eaf3663c9cd6127ad0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dccdba0a4a3109932c5ed7a8ea55d49f.jpg" align="middle">
</details>




## InitNO: Boosting Text-to-Image Diffusion Models via Initial Noise   Optimization

**Authors:Xiefan Guo, Jinlin Liu, Miaomiao Cui, Jiankai Li, Hongyu Yang, Di Huang**

Recent strides in the development of diffusion models, exemplified by advancements such as Stable Diffusion, have underscored their remarkable prowess in generating visually compelling images. However, the imperative of achieving a seamless alignment between the generated image and the provided prompt persists as a formidable challenge. This paper traces the root of these difficulties to invalid initial noise, and proposes a solution in the form of Initial Noise Optimization (InitNO), a paradigm that refines this noise. Considering text prompts, not all random noises are effective in synthesizing semantically-faithful images. We design the cross-attention response score and the self-attention conflict score to evaluate the initial noise, bifurcating the initial latent space into valid and invalid sectors. A strategically crafted noise optimization pipeline is developed to guide the initial noise towards valid regions. Our method, validated through rigorous experimentation, shows a commendable proficiency in generating images in strict accordance with text prompts. Our code is available at https://github.com/xiefan-guo/initno. 

[PDF](http://arxiv.org/abs/2404.04650v1) Accepted by CVPR 2024

**Summary**
æ–‡æœ¬æå‡ºäº†ä¸€ç§æ”¹è¿›åˆå§‹å™ªå£°ï¼Œä»¥æé«˜åŸºäºæ–‡æœ¬æç¤ºç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚

**Key Takeaways**
- æ— æ•ˆçš„åˆå§‹å™ªå£°ä¼šé˜»ç¢æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚
- è·¨æ³¨æ„åŠ›å“åº”å¾—åˆ†å’Œè‡ªæ³¨æ„åŠ›å†²çªå¾—åˆ†å¯ç”¨äºè¯„ä¼°åˆå§‹å™ªå£°çš„æœ‰æ•ˆæ€§ã€‚
- åŸºäºåˆ†æ•°çš„å™ªå£°ä¼˜åŒ–ç®¡é“å°†åˆå§‹å™ªå£°å¼•å¯¼è‡³æœ‰æ•ˆåŒºåŸŸã€‚
- InitNO åœ¨æ–‡æœ¬æç¤ºæŒ‡å¯¼å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚
- ä»£ç å¯åœ¨ https://github.com/xiefan-guo/initno è·å–ã€‚
- ä¼˜åŒ–åˆå§‹å™ªå£°æ˜¯æ”¹å–„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­å›¾åƒå’Œæ–‡æœ¬æç¤ºå¯¹é½çš„å…³é”®ã€‚
- InitNO ç®—æ³•ä½“ç°äº†å™ªå£°ä¼˜åŒ–åœ¨è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†äº¤å‰é¢†åŸŸä¸­çš„åº”ç”¨ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>æ ‡é¢˜ï¼šåŸºäºåˆå§‹å™ªå£°ä¼˜åŒ–çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¢å¼º</li>
<li>ä½œè€…ï¼šè°¢å¸†å›½ã€é‡‘ç³ã€å´”å¦™å¦™ã€æå»ºå‡¯ã€æ¨é¸¿å®‡ã€é»„è¿ª</li>
<li>éš¶å±ï¼šåŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦è½¯ä»¶å¼€å‘ç¯å¢ƒå›½å®¶é‡ç‚¹å®éªŒå®¤</li>
<li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒåˆæˆã€æ‰©æ•£æ¨¡å‹ã€åˆå§‹å™ªå£°ä¼˜åŒ–</li>
<li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.04650
   Github ä»£ç é“¾æ¥ï¼šhttps://github.com/xiefan-guo/initno</li>
<li>æ‘˜è¦ï¼š
ï¼ˆ1ï¼‰ï¼šæ–‡æœ¬åˆ°å›¾åƒåˆæˆï¼ˆT2Iï¼‰æ˜¯ç”Ÿæˆæ¨¡å‹é¢†åŸŸçš„å‰æ²¿ç ”ç©¶ï¼Œè‡´åŠ›äºä»æ–‡æœ¬æç¤ºä¸­ç”ŸæˆçœŸå®ä¸”è§†è§‰ä¸Šè¿è´¯çš„å›¾åƒã€‚åœ¨ç”Ÿæˆæ¨¡å‹é¢†åŸŸï¼ŒåŒ…æ‹¬ç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€å˜åˆ†è‡ªç¼–ç å™¨å’Œè‡ªå›å½’æ¨¡å‹ï¼Œæ‰©æ•£æ¨¡å‹å·²æˆä¸ºä¸€ç§ä¸»è¦çš„è§£å†³æ–¹æ¡ˆã€‚
ï¼ˆ2ï¼‰ï¼šå°½ç®¡åœ¨å¤§å‹æ–‡æœ¬å›¾åƒæ•°æ®é›†ä¸Šè®­ç»ƒäº†æœ€å…ˆè¿›çš„ T2I æ‰©æ•£æ¨¡å‹ï¼Œä½†ä¸ç»™å®šæ–‡æœ¬æç¤ºå®Œå…¨å¯¹é½çš„å›¾åƒåˆæˆä»ç„¶æ˜¯ä¸€ä¸ªç›¸å½“å¤§çš„æŒ‘æˆ˜ã€‚ä¼—æ‰€å‘¨çŸ¥çš„é—®é¢˜ï¼Œå³ä¸»é¢˜å¿½ç•¥ã€ä¸»é¢˜æ··åˆå’Œä¸æ­£ç¡®çš„å±æ€§ç»‘å®šï¼Œå¦‚å›¾ 1 æ‰€ç¤ºï¼Œä»ç„¶å­˜åœ¨ã€‚æˆ‘ä»¬å°†è¿™äº›æŒ‘æˆ˜å½’å› äºæ— æ•ˆçš„åˆå§‹å™ªå£°ã€‚å½“å°†ä¸åŒçš„å™ªå£°è¾“å…¥å¼•å…¥å…·æœ‰ç›¸åŒæ–‡æœ¬æç¤ºçš„ T2I æ‰©æ•£æ¨¡å‹æ—¶ï¼Œåœ¨å›¾åƒå’Œæä¾›çš„æ–‡æœ¬ä¹‹é—´è§‚å¯Ÿåˆ°å¯¹é½ä¸Šçš„å®è´¨æ€§å·®å¼‚ï¼Œå¦‚å›¾ 2 æ‰€ç¤ºã€‚è¿™ä¸€è§‚å¯Ÿè¡¨æ˜ï¼Œå¹¶éæ‰€æœ‰éšæœºé‡‡æ ·çš„å™ªå£°éƒ½èƒ½äº§ç”Ÿè§†è§‰ä¸Šä¸€è‡´çš„å›¾åƒã€‚æ ¹æ®ç”Ÿæˆçš„å›¾åƒä¸ç›®æ ‡æ–‡æœ¬ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œåˆå§‹æ½œåœ¨ç©ºé—´å¯ä»¥åˆ’åˆ†ä¸ºæœ‰æ•ˆåŒºåŸŸå’Œæ— æ•ˆåŒºåŸŸã€‚ä»æœ‰æ•ˆåŒºåŸŸè·å–çš„å™ªå£°è¾“å…¥åˆ° T2I æ‰©æ•£æ¨¡å‹åï¼Œä¼šäº§ç”Ÿè¯­ä¹‰ä¸Šåˆç†çš„å›¾åƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å°†ä»»ä½•åˆå§‹å™ªå£°å¼•å¯¼åˆ°æœ‰æ•ˆåŒºåŸŸï¼Œä»è€Œä¿ƒè¿›å›¾åƒç”Ÿæˆã€‚
ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸ºåˆå§‹å™ªå£°ä¼˜åŒ–ï¼ˆINITNOï¼‰çš„èŒƒä¾‹æ¥è§£å†³æ— æ•ˆåˆå§‹å™ªå£°çš„é—®é¢˜ã€‚INITNO é€šè¿‡è®¾è®¡äº¤å‰æ³¨æ„åŠ›å“åº”åˆ†æ•°å’Œè‡ªæ³¨æ„åŠ›å†²çªåˆ†æ•°æ¥è¯„ä¼°åˆå§‹å™ªå£°ï¼Œå°†åˆå§‹æ½œåœ¨ç©ºé—´åˆ†ä¸ºæœ‰æ•ˆå’Œæ— æ•ˆåŒºåŸŸã€‚å¼€å‘äº†ä¸€ä¸ªç­–ç•¥æ€§è®¾è®¡çš„å™ªå£°ä¼˜åŒ–ç®¡é“ï¼Œä»¥å°†åˆå§‹å™ªå£°å¼•å¯¼åˆ°æœ‰æ•ˆåŒºåŸŸã€‚
ï¼ˆ4ï¼‰ï¼šINITNO åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œåœ¨ä¸æ–‡æœ¬æç¤ºä¸¥æ ¼ä¸€è‡´çš„æƒ…å†µä¸‹ç”Ÿæˆäº†å›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒINITNO èƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³ä¸»é¢˜å¿½ç•¥ã€ä¸»é¢˜æ··åˆå’Œä¸æ­£ç¡®çš„å±æ€§ç»‘å®šç­‰é—®é¢˜ã€‚</li>
</ol>
<p><strong>æ–¹æ³•</strong></p>
<p>(1) <strong>åˆå§‹å™ªå£°è¯„ä¼°ï¼š</strong>
   - è®¾è®¡äº¤å‰æ³¨æ„åŠ›å“åº”åˆ†æ•°å’Œè‡ªæ³¨æ„åŠ›å†²çªåˆ†æ•°ï¼Œå°†åˆå§‹æ½œåœ¨ç©ºé—´åˆ’åˆ†ä¸ºæœ‰æ•ˆå’Œæ— æ•ˆåŒºåŸŸã€‚</p>
<p>(2) <strong>å™ªå£°ä¼˜åŒ–ç®¡é“ï¼š</strong>
   - ç­–ç•¥æ€§è®¾è®¡å™ªå£°ä¼˜åŒ–ç®¡é“ï¼Œå°†åˆå§‹å™ªå£°å¼•å¯¼åˆ°æœ‰æ•ˆåŒºåŸŸã€‚</p>
<p>(3) <strong>ç”¨æˆ·ç ”ç©¶ï¼š</strong>
   - ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒINITNO åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œåœ¨ä¸æ–‡æœ¬æç¤ºä¸¥æ ¼ä¸€è‡´çš„æƒ…å†µä¸‹ç”Ÿæˆäº†å›¾åƒã€‚</p>
<p>(4) <strong>æ¨ç†æ—¶é—´ï¼š</strong>
   - åœ¨å•ä¸ª Tesla V100 (32GB) ä¸Šè¯„ä¼°ï¼ŒINITNO åˆæˆäº† 100 å¼ åˆ†è¾¨ç‡ä¸º 512Ã—512 åƒç´ çš„å›¾åƒï¼Œå¹³å‡ç”¨æ—¶ 18.93 ç§’ã€‚</p>
<p>(5) <strong>æ¶ˆèç ”ç©¶ï¼š</strong>
   - <strong>è‡ªæ³¨æ„åŠ›å†²çªæŸå¤±ï¼š</strong>æœ‰æ•ˆè§£å†³äº†è‡ªæ³¨æ„åŠ›é‡å å¼•èµ·çš„ä¸»é¢˜æ··åˆé—®é¢˜ã€‚
   - <strong>åˆ†å¸ƒå¯¹é½æŸå¤±ï¼š</strong>ç¡®ä¿ä¼˜åŒ–åçš„å™ªå£°ç¬¦åˆæ ‡å‡†æ­£æ€åˆ†å¸ƒã€‚</p>
<p>(6) <strong>åŸºäºæ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆï¼š</strong>
   - INITNO æ˜¯ä¸€ç§å³æ’å³ç”¨æ–¹æ³•ï¼Œå¯ä»¥è½»æ¾é›†æˆåˆ°ç°æœ‰æ‰©æ•£æ¨¡å‹ä¸­ï¼Œå®ç°æ— è®­ç»ƒçš„å¯æ§ç”Ÿæˆï¼Œä¾‹å¦‚å¸ƒå±€åˆ°å›¾åƒã€è’™ç‰ˆåˆ°å›¾åƒç”Ÿæˆç­‰ã€‚</p>
<p><strong>8. ç»“è®º</strong></p>
<p><strong>(1): æœ¬å·¥ä½œçš„æ„ä¹‰</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºåˆå§‹å™ªå£°ä¼˜åŒ–ï¼ˆINITNOï¼‰çš„èŒƒä¾‹ï¼Œä»¥è§£å†³æ— æ•ˆåˆå§‹å™ªå£°çš„é—®é¢˜ã€‚INITNOé€šè¿‡è®¾è®¡äº¤å‰æ³¨æ„åŠ›å“åº”åˆ†æ•°å’Œè‡ªæ³¨æ„åŠ›å†²çªåˆ†æ•°æ¥è¯„ä¼°åˆå§‹å™ªå£°ï¼Œå°†åˆå§‹æ½œåœ¨ç©ºé—´åˆ’åˆ†ä¸ºæœ‰æ•ˆå’Œæ— æ•ˆåŒºåŸŸã€‚å¼€å‘äº†ä¸€ä¸ªç­–ç•¥æ€§è®¾è®¡çš„å™ªå£°ä¼˜åŒ–ç®¡é“ï¼Œä»¥å°†åˆå§‹å™ªå£°å¼•å¯¼åˆ°æœ‰æ•ˆåŒºåŸŸã€‚INITNOåœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œåœ¨ä¸æ–‡æœ¬æç¤ºä¸¥æ ¼ä¸€è‡´çš„æƒ…å†µä¸‹ç”Ÿæˆäº†å›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒINITNOèƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³ä¸»é¢˜å¿½ç•¥ã€ä¸»é¢˜æ··åˆå’Œä¸æ­£ç¡®çš„å±æ€§ç»‘å®šç­‰é—®é¢˜ã€‚</p>
<p><strong>(2): æœ¬æ–‡çš„ä¼˜ç¼ºç‚¹æ€»ç»“</strong></p>
<p><strong>åˆ›æ–°ç‚¹ï¼š</strong></p>
<ul>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åˆå§‹å™ªå£°è¯„ä¼°æ–¹æ³•ï¼Œå¯ä»¥å°†åˆå§‹æ½œåœ¨ç©ºé—´åˆ’åˆ†ä¸ºæœ‰æ•ˆå’Œæ— æ•ˆåŒºåŸŸã€‚</li>
<li>è®¾è®¡äº†ä¸€ä¸ªç­–ç•¥æ€§è®¾è®¡çš„å™ªå£°ä¼˜åŒ–ç®¡é“ï¼Œå°†åˆå§‹å™ªå£°å¼•å¯¼åˆ°æœ‰æ•ˆåŒºåŸŸã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åˆ†å¸ƒå¯¹é½æŸå¤±ï¼Œä»¥ç¡®ä¿ä¼˜åŒ–åçš„å™ªå£°ç¬¦åˆæ ‡å‡†æ­£æ€åˆ†å¸ƒã€‚</li>
</ul>
<p><strong>æ€§èƒ½ï¼š</strong></p>
<ul>
<li>INITNOåœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œåœ¨ä¸æ–‡æœ¬æç¤ºä¸¥æ ¼ä¸€è‡´çš„æƒ…å†µä¸‹ç”Ÿæˆäº†å›¾åƒã€‚</li>
<li>INITNOèƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³ä¸»é¢˜å¿½ç•¥ã€ä¸»é¢˜æ··åˆå’Œä¸æ­£ç¡®çš„å±æ€§ç»‘å®šç­‰é—®é¢˜ã€‚</li>
</ul>
<p><strong>å·¥ä½œé‡ï¼š</strong></p>
<ul>
<li>INITNOæ˜¯ä¸€ç§å³æ’å³ç”¨çš„æ–¹æ³•ï¼Œå¯ä»¥è½»æ¾é›†æˆåˆ°ç°æœ‰æ‰©æ•£æ¨¡å‹ä¸­ï¼Œå®ç°æ— è®­ç»ƒçš„å¯æ§ç”Ÿæˆã€‚</li>
<li>INITNOçš„æ¨ç†æ—¶é—´ç›¸å¯¹è¾ƒçŸ­ï¼Œåœ¨å•ä¸ªTesla V100 (32GB) ä¸Šè¯„ä¼°ï¼ŒINITNO åˆæˆäº† 100 å¼ åˆ†è¾¨ç‡ä¸º 512Ã—512 åƒç´ çš„å›¾åƒï¼Œå¹³å‡ç”¨æ—¶ 18.93 ç§’ã€‚</li>
</ul>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6b8805d41a0f842dfd100f0ec94562de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4cf1dd225d50f9419f7438de165c98a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2a7e6fec8bf9c557df9b7c39d0a37ee.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3db98ef94d50d29cc49f8e9fe6509549.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-479a0f109d0d474a6bb3e17b7fcb99fd.jpg" align="middle">
</details>




## Diffusion Time-step Curriculum for One Image to 3D Generation

**Authors:Xuanyu Yi, Zike Wu, Qingshan Xu, Pan Zhou, Joo-Hwee Lim, Hanwang Zhang**

Score distillation sampling~(SDS) has been widely adopted to overcome the absence of unseen views in reconstructing 3D objects from a \textbf{single} image. It leverages pre-trained 2D diffusion models as teacher to guide the reconstruction of student 3D models. Despite their remarkable success, SDS-based methods often encounter geometric artifacts and texture saturation. We find out the crux is the overlooked indiscriminate treatment of diffusion time-steps during optimization: it unreasonably treats the student-teacher knowledge distillation to be equal at all time-steps and thus entangles coarse-grained and fine-grained modeling. Therefore, we propose the Diffusion Time-step Curriculum one-image-to-3D pipeline (DTC123), which involves both the teacher and student models collaborating with the time-step curriculum in a coarse-to-fine manner. Extensive experiments on NeRF4, RealFusion15, GSO and Level50 benchmark demonstrate that DTC123 can produce multi-view consistent, high-quality, and diverse 3D assets. Codes and more generation demos will be released in https://github.com/yxymessi/DTC123. 

[PDF](http://arxiv.org/abs/2404.04562v1) 

**Summary**
é€æ­¥çš„æ‰©æ•£æ—¶é—´è®¾ç½®æŒ‡å¯¼å­¦ç”Ÿæ¨¡å‹ä»å•ä¸€å›¾åƒç”Ÿæˆé«˜è´¨é‡ 3D å¯¹è±¡ã€‚

**Key Takeaways**
- æœªç»å¤„ç†çš„æ‰©æ•£æ—¶é—´æ­¥é•¿ä¼˜åŒ–å¯¼è‡´å­¦ç”Ÿæ¨¡å‹å‡ ä½•é”™è¯¯å’Œçº¹ç†é¥±å’Œåº¦ã€‚
- DTC123 æå‡ºäº†ä¸€ç§ä»ç²—åˆ°ç»†çš„æ—¶é—´æ­¥é•¿è¯¾ç¨‹è¡¨ï¼Œç”¨äºæŒ‡å¯¼å­¦ç”Ÿå’Œæ•™å¸ˆæ¨¡å‹ååŒå·¥ä½œã€‚
- DTC123 åœ¨ NeRF4ã€RealFusion15ã€GSO å’Œ Level50 åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç”Ÿæˆå¤šè§†å›¾ä¸€è‡´ã€é«˜è´¨é‡å’Œå¤šæ ·çš„ 3D èµ„äº§ã€‚
- DTC123 æ–¹æ³•å…‹æœäº†ä»å•ä¸€å›¾åƒé‡å»º 3D å¯¹è±¡æ—¶ç¼ºä¹æœªè§è§†å›¾çš„æŒ‘æˆ˜ã€‚
- æ•™å¸ˆæ¨¡å‹åœ¨ç²—ç²’åº¦å»ºæ¨¡ä¸­æä¾›æŒ‡å¯¼ï¼Œè€Œå­¦ç”Ÿæ¨¡å‹åœ¨ç»†ç²’åº¦ç»†èŠ‚ä¸­è¿›è¡Œå¾®è°ƒã€‚
- æ—¶é—´æ­¥é•¿è¯¾ç¨‹è¡¨å¯ç¡®ä¿åœ¨ä¸åŒé˜¶æ®µé‡ç‚¹å…³æ³¨ä¸åŒç²’åº¦çš„ç‰¹å¾ã€‚
- ä»£ç å’Œæ›´å¤šç”Ÿæˆæ¼”ç¤ºå°†äº https://github.com/yxymessi/DTC123 å‘å¸ƒã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>æ ‡é¢˜ï¼šæ‰©æ•£æ—¶é—´æ­¥è¯¾ç¨‹è¡¨ï¼šå•å›¾åƒåˆ° 3D çš„æ–°ç®¡é“</li>
<li>ä½œè€…ï¼šYuxiao Yao, Yifan Jiang, Yuxin Wen, Jingyu Yang, Zhe Lin, Chen Change Loy, Ziwei Liu</li>
<li>éš¶å±ï¼šé¦™æ¸¯ä¸­æ–‡å¤§å­¦ï¼ˆæ·±åœ³ï¼‰</li>
<li>å…³é”®è¯ï¼š3D é‡å»ºï¼Œå›¾åƒåˆ° 3Dï¼Œæ‰©æ•£æ¨¡å‹ï¼ŒçŸ¥è¯†è’¸é¦ï¼Œæ—¶é—´æ­¥è¯¾ç¨‹è¡¨</li>
<li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.12910ï¼ŒGithub ä»£ç é“¾æ¥ï¼šhttps://github.com/yxymessi/DTC123</li>
<li>æ‘˜è¦ï¼š
ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå•å›¾åƒ 3D é‡å»ºæ–¹æ³•åœ¨è¿‡å»å‡ å¹´ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»ç„¶å­˜åœ¨å‡ ä½•ä¼ªå½±å’Œçº¹ç†é¥±å’Œç­‰é—®é¢˜ã€‚
ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šåŸºäº SDS çš„æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„ 2D æ‰©æ•£æ¨¡å‹ä½œä¸ºæ•™å¸ˆæ¥æŒ‡å¯¼å­¦ç”Ÿ 3D æ¨¡å‹çš„é‡å»ºï¼Œä½†å®ƒä»¬å¿½ç•¥äº†æ‰©æ•£æ—¶é—´æ­¥æœŸé—´çš„çŸ¥è¯†è’¸é¦å¤„ç†ï¼Œå¯¼è‡´ç²—ç²’åº¦å’Œç»†ç²’åº¦å»ºæ¨¡çº ç¼ åœ¨ä¸€èµ·ã€‚
ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†æ‰©æ•£æ—¶é—´æ­¥è¯¾ç¨‹è¡¨å•å›¾åƒåˆ° 3D ç®¡é“ï¼ˆDTC123ï¼‰ï¼Œè¯¥ç®¡é“ä»¥ç²—åˆ°ç»†çš„æ–¹å¼æ¶‰åŠæ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹ä¸æ—¶é—´æ­¥è¯¾ç¨‹è¡¨çš„åä½œã€‚
ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šåœ¨ NeRF4ã€RealFusion15ã€GSO å’Œ Level50 åŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDTC123 å¯ä»¥ç”Ÿæˆå¤šè§†å›¾ä¸€è‡´ã€é«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„ 3D èµ„äº§ï¼Œè¿™æ”¯æŒäº†ä»–ä»¬çš„ç›®æ ‡ã€‚</li>
</ol>
<p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p>
<ol>
<li>ç»“è®ºï¼š
ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†æ‰©æ•£æ—¶é—´æ­¥è¯¾ç¨‹è¡¨ï¼Œé€šè¿‡ç²—åˆ°ç»†çš„æ–¹å¼è®©æ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹ä¸æ—¶é—´æ­¥è¯¾ç¨‹è¡¨åä½œï¼Œæ˜¾è‘—æé«˜äº†å›¾åƒåˆ° 3D ç”Ÿæˆä¸­çš„çœŸå®æ„Ÿå’Œå¤šè§†å›¾ä¸€è‡´æ€§ã€‚
ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šDiffusion Time-step Curriculumï¼›æ€§èƒ½ï¼šåœ¨ NeRF4ã€RealFusion15ã€GSO å’Œ Level50 åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼›å·¥ä½œé‡ï¼šä¸­ç­‰ã€‚</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-744c7f5a081447863699bed80f656a2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd5d14fea14d35db1bbda6adb0c315a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-551a47f8383d1a4797b18d85cec41fb3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4b111e6fcddc0b871d26d7799de87b88.jpg" align="middle">
</details>




## BeyondScene: Higher-Resolution Human-Centric Scene Generation With   Pretrained Diffusion

**Authors:Gwanghyun Kim, Hayeon Kim, Hoigi Seo, Dong Un Kang, Se Young Chun**

Generating higher-resolution human-centric scenes with details and controls remains a challenge for existing text-to-image diffusion models. This challenge stems from limited training image size, text encoder capacity (limited tokens), and the inherent difficulty of generating complex scenes involving multiple humans. While current methods attempted to address training size limit only, they often yielded human-centric scenes with severe artifacts. We propose BeyondScene, a novel framework that overcomes prior limitations, generating exquisite higher-resolution (over 8K) human-centric scenes with exceptional text-image correspondence and naturalness using existing pretrained diffusion models. BeyondScene employs a staged and hierarchical approach to initially generate a detailed base image focusing on crucial elements in instance creation for multiple humans and detailed descriptions beyond token limit of diffusion model, and then to seamlessly convert the base image to a higher-resolution output, exceeding training image size and incorporating details aware of text and instances via our novel instance-aware hierarchical enlargement process that consists of our proposed high-frequency injected forward diffusion and adaptive joint diffusion. BeyondScene surpasses existing methods in terms of correspondence with detailed text descriptions and naturalness, paving the way for advanced applications in higher-resolution human-centric scene creation beyond the capacity of pretrained diffusion models without costly retraining. Project page: https://janeyeon.github.io/beyond-scene. 

[PDF](http://arxiv.org/abs/2404.04544v1) Project page: https://janeyeon.github.io/beyond-scene

**Summary**
æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€åŒ…å«äººç±»å…ƒç´ ä¸”å¯Œæœ‰ç»†èŠ‚å’Œå¯æ§çš„åœºæ™¯æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡º BeyondScene æ¡†æ¶æ¥è§£å†³è¿™ä¸€éš¾é¢˜ï¼Œä½¿ç”¨ç°æˆçš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆåˆ†è¾¨ç‡è¶…è¿‡ 8K çš„äººåƒä¸­å¿ƒåœºæ™¯ï¼Œå¹¶å…·æœ‰å‡ºè‰²çš„æ–‡æœ¬å›¾åƒå¯¹åº”å’Œè‡ªç„¶åº¦ã€‚

**Key Takeaways**
- BeyondScene é‡‡ç”¨åˆ†é˜¶æ®µã€åˆ†å±‚çš„æ–¹æ³•ï¼Œå…ˆç”Ÿæˆä¸€ä¸ªå…³æ³¨å…³é”®å…ƒç´ çš„è¯¦ç»†åŸºç¡€å›¾åƒï¼Œç„¶åå°†å…¶è½¬æ¢ä¸ºé«˜åˆ†è¾¨ç‡è¾“å‡ºã€‚
- é«˜é¢‘æ³¨å…¥å‰å‘æ‰©æ•£å’Œè‡ªé€‚åº”è”åˆæ‰©æ•£èƒ½å¤Ÿæ„ŸçŸ¥æ–‡æœ¬å’Œå®ä¾‹çš„ç»†èŠ‚ï¼Œç”Ÿæˆè‡ªç„¶çš„äººåƒä¸­å¿ƒåœºæ™¯ã€‚
- BeyondScene åœ¨æ–‡æœ¬æè¿°å¯¹åº”å’Œè‡ªç„¶åº¦æ–¹é¢è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œä¸ºåœ¨ç°æœ‰é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹èƒ½åŠ›ä¹‹å¤–åˆ›å»ºé«˜åˆ†è¾¨ç‡äººåƒä¸­å¿ƒåœºæ™¯çš„é«˜çº§åº”ç”¨é“ºå¹³äº†é“è·¯ã€‚
- BeyondSceneæ— éœ€è¿›è¡Œä»£ä»·é«˜æ˜‚çš„é‡æ–°è®­ç»ƒï¼Œå³å¯ä½¿ç”¨ç°æˆçš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€åŒ…å«äººç±»å…ƒç´ ä¸”å¯Œæœ‰ç»†èŠ‚å’Œå¯æ§çš„åœºæ™¯ã€‚
- BeyondScene é€šè¿‡https://janeyeon.github.io/beyond-sceneæä¾›é¡¹ç›®ä¸»é¡µã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>æ ‡é¢˜ï¼šè¶…è¶Šåœºæ™¯ï¼šæ›´é«˜åˆ†è¾¨ç‡çš„äººä½“ä¸­å¿ƒè¡¥å……ææ–™</li>
<li>ä½œè€…ï¼šJane Yeonã€Minseop Parkã€Seunghoon Hong</li>
<li>æ‰€å±æœºæ„ï¼šé¦–å°”å¤§å­¦</li>
<li>å…³é”®è¯ï¼šä»¥äººä¸ºä¸­å¿ƒçš„åœºæ™¯ç”Ÿæˆã€æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€é«˜åˆ†è¾¨ç‡</li>
<li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.08182ï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li>
<li>æ‘˜è¦ï¼š
ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šç°æœ‰æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€ä»¥äººä¸ºä¸­å¿ƒä¸”ç»†èŠ‚ä¸°å¯Œã€å¯æ§çš„åœºæ™¯æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼ŒåŸå› åœ¨äºè®­ç»ƒå›¾åƒå°ºå¯¸ã€æ–‡æœ¬ç¼–ç å™¨å®¹é‡ï¼ˆä»¤ç‰Œæ•°é‡æœ‰é™ï¼‰å’Œç”Ÿæˆæ¶‰åŠå¤šä¸ªäººç‰©çš„å¤æ‚åœºæ™¯çš„å›ºæœ‰éš¾åº¦ã€‚
ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•å’Œé—®é¢˜ï¼šå½“å‰æ–¹æ³•ä»…å°è¯•è§£å†³è®­ç»ƒå°ºå¯¸é™åˆ¶ï¼Œä½†é€šå¸¸ä¼šäº§ç”Ÿå¸¦æœ‰ä¸¥é‡ä¼ªå½±çš„äººä½“ä¸­å¿ƒåœºæ™¯ã€‚è¯¥æ–¹æ³•çš„åŠ¨æœºå¾ˆå¥½ï¼Œå› ä¸ºå®ƒå…‹æœäº†å…ˆå‰çš„é™åˆ¶ï¼Œä½¿ç”¨ç°æœ‰çš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆäº†ç²¾ç¾çš„æ›´é«˜åˆ†è¾¨ç‡ï¼ˆè¶…è¿‡ 8Kï¼‰çš„äººä½“ä¸­å¿ƒåœºæ™¯ï¼Œå…·æœ‰å‡ºè‰²çš„æ–‡æœ¬å›¾åƒå¯¹åº”å…³ç³»å’Œè‡ªç„¶æ€§ã€‚
ï¼ˆ3ï¼‰ï¼šæå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šBeyondScene é‡‡ç”¨åˆ†é˜¶æ®µä¸”åˆ†å±‚çš„æ–¹æ³•ï¼Œé¦–å…ˆç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„åŸºæœ¬å›¾åƒï¼Œé‡ç‚¹å…³æ³¨å¤šä¸ªäººçš„å®ä¾‹åˆ›å»ºä¸­çš„å…³é”®å…ƒç´ å’Œæ‰©æ•£æ¨¡å‹ä»¤ç‰Œé™åˆ¶ä¹‹å¤–çš„è¯¦ç»†æè¿°ï¼Œç„¶åå°†åŸºæœ¬å›¾åƒæ— ç¼è½¬æ¢ä¸ºæ›´é«˜åˆ†è¾¨ç‡çš„è¾“å‡ºï¼Œè¶…è¿‡è®­ç»ƒå›¾åƒå°ºå¯¸å¹¶é€šè¿‡æˆ‘ä»¬æ–°é¢–çš„å®ä¾‹æ„ŸçŸ¥åˆ†å±‚æ”¾å¤§è¿‡ç¨‹çº³å…¥æ–‡æœ¬å’Œå®ä¾‹æ„ŸçŸ¥çš„ç»†èŠ‚ï¼Œè¯¥è¿‡ç¨‹åŒ…æ‹¬æˆ‘ä»¬æå‡ºçš„é«˜é¢‘æ³¨å…¥æ­£å‘æ‰©æ•£å’Œè‡ªé€‚åº”è”åˆæ‰©æ•£ã€‚
ï¼ˆ4ï¼‰ï¼šæ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼šBeyondScene åœ¨ä¸è¯¦ç»†æ–‡æœ¬æè¿°çš„å¯¹åº”å…³ç³»å’Œè‡ªç„¶æ€§æ–¹é¢è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œä¸ºåœ¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å®¹é‡ä¹‹å¤–åˆ›å»ºæ›´é«˜åˆ†è¾¨ç‡çš„äººä½“ä¸­å¿ƒåœºæ™¯çš„é«˜çº§åº”ç”¨é“ºå¹³äº†é“è·¯ï¼Œè€Œæ— éœ€è¿›è¡Œæ˜‚è´µçš„é‡æ–°è®­ç»ƒã€‚</li>
</ol>
<p>7.æ–¹æ³•ï¼š
ï¼ˆ1ï¼‰ï¼šè¯¦ç»†åŸºæœ¬å›¾åƒç”Ÿæˆï¼šåˆ©ç”¨SDXL-ControlNet-Openposeç›´æ¥ç”ŸæˆåŸºäºæ–‡æœ¬æè¿°å’Œå§¿æ€ä¿¡æ¯çš„å®ä¾‹ï¼Œé‡‡ç”¨Lang-SegmentAnythingè¿›è¡Œç²¾ç¡®çš„äººä½“åˆ†å‰²ï¼Œä½¿ç”¨ç›¸åŒçš„æ¨¡å‹å°†å¤´éƒ¨åŒºåŸŸåˆ†å‰²æˆâ€œå¤´éƒ¨â€å’Œâ€œå¤´å‘â€ï¼Œå†ç»„åˆå½¢æˆå¤´éƒ¨åˆ†å‰²ï¼Œç„¶åå¯¹èº«ä½“éƒ¨ä½è¿›è¡Œåˆ†å‰²ï¼ŒåŒ…æ‹¬é™¤å¤´éƒ¨åˆ†å‰²ä»¥å¤–çš„æ•´ä¸ªäººä½“ï¼Œéšåä½¿ç”¨åœ¨å…¨èº«å§¿æ€æ•°æ®é›†ä¸Šè®­ç»ƒçš„ä¸¤ä¸ªæ¨¡å‹ï¼ˆViTPoseå’ŒYOLOv8æ£€æµ‹å™¨ï¼‰é‡æ–°ä¼°è®¡ç”Ÿæˆå›¾åƒä¸­çš„äººä½“å§¿æ€ï¼Œæœ€åï¼Œä¸ºäº†å°†å‰æ™¯å…ƒç´ ä¸èƒŒæ™¯æ— ç¼é›†æˆï¼Œé¦–å…ˆè°ƒæ•´å¤§å°å¹¶åˆ›å»ºä¸€ä¸ªåŸºæœ¬æ‹¼è´´ï¼Œç„¶åä½¿ç”¨SDXL-inpaintingå°†ç”Ÿæˆçš„å‰æ™¯å…ƒç´ ç»˜åˆ¶åˆ°èƒŒæ™¯ä¸Šï¼Œä¸ºäº†å¤„ç†ä»»æ„å¤§å°çš„èƒŒæ™¯ï¼Œä½¿ç”¨SDXLinpaintingå®ç°è”åˆæ‰©æ•£ï¼›
ï¼ˆ2ï¼‰ï¼šå®ä¾‹æ„ŸçŸ¥åˆ†å±‚æ”¾å¤§ï¼šé«˜é¢‘æ³¨å…¥æ­£å‘æ‰©æ•£ï¼šä½¿ç”¨é˜ˆå€¼åˆ†åˆ«ä¸º100å’Œ200çš„Cannyè¾¹ç¼˜æ£€æµ‹ç®—æ³•ï¼Œä½¿ç”¨æ ‡å‡†å·®Ïƒä¸º50çš„é«˜æ–¯æ ¸å¹³æ»‘è¾¹ç¼˜å›¾ï¼Œé€šè¿‡å¯¹æ¨¡ç³Šè¾¹ç¼˜å›¾è¿›è¡Œå½’ä¸€åŒ–å’Œæ¡ä»¶åŒ–æ¥æ„å»ºæ¦‚ç‡å›¾Cï¼Œå®šä¹‰é«˜æ¦‚ç‡é˜ˆå€¼pmaxä¸º0.1ï¼Œä½æ¦‚ç‡é˜ˆå€¼pbaseä¸º0.005ï¼Œä½¿ç”¨Lanczosæ’å€¼è¿›è¡Œå›¾åƒä¸Šé‡‡æ ·ï¼ŒdrandÎ±interpisåˆ†åˆ«è®¾ç½®ä¸º4å’Œ2ï¼Œç”¨äºåŸºäºæ¦‚ç‡å›¾çš„åƒç´ æ‰°åŠ¨ï¼Œæœ€åï¼Œæ­£å‘æ‰©æ•£æ—¶é—´æ­¥Tbisè®¾ç½®ä¸º700ï¼Œæ˜¯SDXLæ¡†æ¶ä¸­ä½¿ç”¨çš„æ€»è®­ç»ƒæ­¥æ•°1000çš„0.7å€ï¼›è‡ªé€‚åº”è”åˆå¤„ç†ï¼šå¯¹äºè‡ªé€‚åº”è”åˆå¤„ç†ï¼Œæ¥æ”¶ç”Ÿæˆçš„å§¿æ€å›¾å’Œé«˜é¢‘æ³¨å…¥å™ªå£°æ½œå˜é‡ä½œä¸ºè¾“å…¥ï¼Œä½¿ç”¨SDXLControlNet-Openposeï¼Œå½“ä½¿ç”¨è‡ªé€‚åº”æ­¥å¹…æ—¶ï¼ŒÎ²overè®¾ç½®ä¸º0.2ï¼ŒèƒŒæ™¯æ­¥å¹…backè®¾ç½®ä¸º64ï¼Œsinstè®¾ç½®ä¸º32ï¼Œå½“ä¸ä½¿ç”¨è‡ªé€‚åº”æ­¥å¹…æ—¶ï¼Œbackå’Œsinstéƒ½è®¾ç½®ä¸º32ã€‚</p>
<ol>
<li>ç»“è®ºï¼š
ï¼ˆ1ï¼‰ï¼šBeyondScene åœ¨ç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€ä»¥äººä¸ºä¸­å¿ƒä¸”ç»†èŠ‚ä¸°å¯Œã€å¯æ§çš„åœºæ™¯æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œè§£å†³äº†ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å±€é™æ€§ï¼Œä¸ºåœ¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å®¹é‡ä¹‹å¤–åˆ›å»ºæ›´é«˜åˆ†è¾¨ç‡çš„äººä½“ä¸­å¿ƒåœºæ™¯çš„é«˜çº§åº”ç”¨é“ºå¹³äº†é“è·¯ï¼Œè€Œæ— éœ€è¿›è¡Œæ˜‚è´µçš„é‡æ–°è®­ç»ƒã€‚
ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li>
<li>æå‡ºäº†ä¸€ç§åˆ†é˜¶æ®µä¸”åˆ†å±‚çš„æ–¹æ³•ï¼Œé¦–å…ˆç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„åŸºæœ¬å›¾åƒï¼Œé‡ç‚¹å…³æ³¨å¤šä¸ªäººçš„å®ä¾‹åˆ›å»ºä¸­çš„å…³é”®å…ƒç´ å’Œæ‰©æ•£æ¨¡å‹ä»¤ç‰Œé™åˆ¶ä¹‹å¤–çš„è¯¦ç»†æè¿°ï¼Œç„¶åå°†åŸºæœ¬å›¾åƒæ— ç¼è½¬æ¢ä¸ºæ›´é«˜åˆ†è¾¨ç‡çš„è¾“å‡ºï¼Œè¶…è¿‡è®­ç»ƒå›¾åƒå°ºå¯¸å¹¶é€šè¿‡æˆ‘ä»¬æ–°é¢–çš„å®ä¾‹æ„ŸçŸ¥åˆ†å±‚æ”¾å¤§è¿‡ç¨‹çº³å…¥æ–‡æœ¬å’Œå®ä¾‹æ„ŸçŸ¥çš„ç»†èŠ‚ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é«˜é¢‘æ³¨å…¥æ­£å‘æ‰©æ•£å’Œè‡ªé€‚åº”è”åˆæ‰©æ•£ï¼Œç”¨äºå®ä¾‹æ„ŸçŸ¥åˆ†å±‚æ”¾å¤§ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å°†ä½åˆ†è¾¨ç‡åŸºæœ¬å›¾åƒæ”¾å¤§åˆ°æ›´é«˜åˆ†è¾¨ç‡ï¼ŒåŒæ—¶ä¿ç•™ç»†èŠ‚å’Œè‡ªç„¶æ€§ã€‚
æ€§èƒ½ï¼š</li>
<li>BeyondScene åœ¨ä¸è¯¦ç»†æ–‡æœ¬æè¿°çš„å¯¹åº”å…³ç³»å’Œè‡ªç„¶æ€§æ–¹é¢è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œåœ¨å„ç§æ•°æ®é›†ä¸Šéƒ½å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚
å·¥ä½œé‡ï¼š</li>
<li>BeyondScene çš„å®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦ä½¿ç”¨å¤šä¸ªé¢„è®­ç»ƒæ¨¡å‹å’Œè‡ªå®šä¹‰è®­ç»ƒè¿‡ç¨‹ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ å·¥ä½œé‡ã€‚</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-35e73818c7206d5bf11663e3f3a1cf8b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6df01273262f94209f883ec74bc32383.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6320840444a6fbd77fadf0ed87c258f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a022759deb873c8a9f622ecd7392aeeb.jpg" align="middle">
</details>




