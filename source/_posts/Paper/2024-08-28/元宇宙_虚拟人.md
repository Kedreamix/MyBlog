
---
title: å…ƒå®‡å®™/è™šæ‹Ÿäºº
date: 2024-08-28 07:42:13
author: Kedreamix
cover: https://picx.zhimg.com/v2-fb38b82805491f3b8b63bc866361c519.jpg
categories: Paper
tags:
    - å…ƒå®‡å®™/è™šæ‹Ÿäºº
description: å…ƒå®‡å®™/è™šæ‹Ÿäºº æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-08-28  Hyperdimensional Computing Empowered Federated Foundation Model over   Wireless Networks for Metaverse  
keywords: å…ƒå®‡å®™/è™šæ‹Ÿäºº
toc:
toc_number: false
toc_style_simple: true
copyright:
copyright_author:
copyright_author_href:
copyright_url:
copyright_info:
mathjax: true
katex:
aplayer:
highlight_shrink:
aside:
---

>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹[Gemini-Pro](https://ai.google.dev/)çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨
>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼
>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© [ChatPaperFree](https://github.com/Kedreamix/ChatPaperFree) ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ [HuggingFaceå…è´¹ä½“éªŒ](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)

# 2024-08-28 æ›´æ–°


## Hyperdimensional Computing Empowered Federated Foundation Model over   Wireless Networks for Metaverse

**Authors:Yahao Ding, Wen Shang, Minrui Xu, Zhaohui Yang, Ye Hu, Dusit Niyato, Mohammad Shikh-Bahaei**

The Metaverse, a burgeoning collective virtual space merging augmented reality and persistent virtual worlds, necessitates advanced artificial intelligence (AI) and communication technologies to support immersive and interactive experiences. Federated learning (FL) has emerged as a promising technique for collaboratively training AI models while preserving data privacy. However, FL faces challenges such as high communication overhead and substantial computational demands, particularly for neural network (NN) models. To address these issues, we propose an integrated federated split learning and hyperdimensional computing (FSL-HDC) framework for emerging foundation models. This novel approach reduces communication costs, computation load, and privacy risks, making it particularly suitable for resource-constrained edge devices in the Metaverse, ensuring real-time responsive interactions. Additionally, we introduce an optimization algorithm that concurrently optimizes transmission power and bandwidth to minimize the maximum transmission time among all users to the server. The simulation results based on the MNIST dataset indicate that FSL-HDC achieves an accuracy rate of approximately 87.5%, which is slightly lower than that of FL-HDC. However, FSL-HDC exhibits a significantly faster convergence speed, approximately 3.733x that of FSL-NN, and demonstrates robustness to non-IID data distributions. Moreover, our proposed optimization algorithm can reduce the maximum transmission time by up to 64% compared with the baseline. 

[PDF](http://arxiv.org/abs/2408.14416v1) 

**Summary**
å…ƒå®‡å®™è™šæ‹Ÿäººéœ€æ±‚é«˜çº§AIä¸é€šä¿¡æŠ€æœ¯ï¼ŒFSL-HDCæ¡†æ¶é™ä½æˆæœ¬ï¼Œä¼˜åŒ–ä¼ è¾“æ—¶é—´ã€‚

**Key Takeaways**
1. å…ƒå®‡å®™èåˆARä¸è™šæ‹Ÿä¸–ç•Œï¼Œéœ€AIä¸é€šä¿¡æŠ€æœ¯ã€‚
2. è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰ä¿è¯æ•°æ®éšç§ï¼Œä½†é¢ä¸´é€šä¿¡ä¸è®¡ç®—æŒ‘æˆ˜ã€‚
3. æå‡ºFSL-HDCæ¡†æ¶ï¼Œé™ä½é€šä¿¡æˆæœ¬ã€è®¡ç®—è´Ÿè½½ä¸éšç§é£é™©ã€‚
4. é€‚ç”¨äºèµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ï¼Œç¡®ä¿å®æ—¶äº’åŠ¨ã€‚
5. ä¼˜åŒ–ç®—æ³•æœ€å°åŒ–æœ€å¤§ä¼ è¾“æ—¶é—´ã€‚
6. FSL-HDCå‡†ç¡®ç‡ç•¥ä½äºFL-HDCï¼Œä½†æ”¶æ•›é€Ÿåº¦æ›´å¿«ã€‚
7. ä¼˜åŒ–ç®—æ³•å¯é™ä½æœ€å¤§ä¼ è¾“æ—¶é—´64%ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**



1. Title: è¶…ç»´è®¡ç®—èµ‹èƒ½çš„æ— çº¿ç½‘ç»œè”é‚¦åŸºç¡€æ¨¡å‹ç”¨äºå…ƒå®‡å®™
2. Authors: Yahao Ding, Wen Shang, Minrui Xu, Zhaohui Yang, Ye Hu, Dusit Niyato, Mohammad Shikh-Bahaei
3. Affiliation: King's College London, Nanyang Technological University, Zhejiang University, University of Miami
4. Keywords: è”é‚¦å­¦ä¹  (Federated Learning), è¶…ç»´è®¡ç®— (Hyperdimensional Computing), èµ„æºåˆ†é…
5. Urls: arXiv:2408.14416v1 [cs.LG] 26 Aug 2024
6. Summary:

   - (1): è¯¥æ–‡çš„ç ”ç©¶èƒŒæ™¯æ˜¯å…ƒå®‡å®™çš„å‘å±•éœ€è¦å…ˆè¿›çš„äººå·¥æ™ºèƒ½å’Œé€šä¿¡æŠ€æœ¯æ¥æ”¯æŒæ²‰æµ¸å¼å’Œäº¤äº’å¼ä½“éªŒã€‚è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰ä½œä¸ºä¸€ç§ä¿æŠ¤æ•°æ®éšç§çš„åŒæ—¶åä½œè®­ç»ƒAIæ¨¡å‹çš„æŠ€æœ¯ï¼Œé¢ä¸´ç€é€šä¿¡å¼€é”€é«˜å’Œè®¡ç®—éœ€æ±‚å¤§çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ç¥ç»ç½‘ç»œï¼ˆNNï¼‰æ¨¡å‹ä¸­ã€‚
 
   - (2): è¿‡å»çš„æ–¹æ³•åŒ…æ‹¬è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰å’Œåˆ†å‰²å­¦ä¹ ï¼ˆSLï¼‰ã€‚FLé¢ä¸´é€šä¿¡å¼€é”€é«˜å’Œè®¡ç®—éœ€æ±‚å¤§çš„é—®é¢˜ï¼Œè€ŒSLé€šè¿‡åœ¨å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨ä¹‹é—´åˆ†é…è®¡ç®—ä»»åŠ¡æ¥ç¼“è§£è¿™äº›é—®é¢˜ï¼Œä½†ä»å­˜åœ¨éšç§å’Œè®¡ç®—èµ„æºåˆ©ç”¨çš„å±€é™æ€§ã€‚æœ¬æ–‡æå‡ºçš„æ–¹æ³•ç»“åˆäº†FLå’ŒSLçš„ä¼˜ç‚¹ï¼Œå¹¶é€šè¿‡è¶…ç»´è®¡ç®—ï¼ˆHDCï¼‰è¿›ä¸€æ­¥æé«˜äº†æ•ˆç‡å’Œéšç§ä¿æŠ¤ï¼Œå…·æœ‰å¾ˆå¥½çš„åŠ¨æœºã€‚
 
   - (3): æœ¬æ–‡æå‡ºçš„æ–¹æ³•æ˜¯è”é‚¦åˆ†å‰²å­¦ä¹ ï¼ˆFSLï¼‰ä¸è¶…ç»´è®¡ç®—ï¼ˆHDCï¼‰çš„é›†æˆæ¡†æ¶ï¼ˆFSL-HDCï¼‰ã€‚è¯¥æ–¹æ³•å°†æ¨¡å‹åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼Œä¸€éƒ¨åˆ†åœ¨å®¢æˆ·ç«¯å¤„ç†ï¼Œå¦ä¸€éƒ¨åˆ†åœ¨æœåŠ¡å™¨ç«¯å¤„ç†ï¼ŒåŒæ—¶ä½¿ç”¨HDCæ¥é™ä½è®¡ç®—å¤æ‚åº¦å’Œèƒ½é‡æ¶ˆè€—ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§ä¼˜åŒ–ç®—æ³•ï¼ŒåŒæ—¶ä¼˜åŒ–ä¼ è¾“åŠŸç‡å’Œå¸¦å®½ï¼Œä»¥æœ€å°åŒ–ç”¨æˆ·åˆ°æœåŠ¡å™¨çš„æœ€å¤§ä¼ è¾“æ—¶é—´ã€‚
 
   - (4): åœ¨MNISTæ•°æ®é›†ä¸Šçš„ä»¿çœŸç»“æœè¡¨æ˜ï¼ŒFSL-HDCçš„å‡†ç¡®ç‡çº¦ä¸º87.5%ï¼Œç•¥ä½äºFL-HDCï¼Œä½†æ”¶æ•›é€Ÿåº¦æé«˜äº†çº¦3.733å€ï¼Œå¯¹éç‹¬ç«‹åŒåˆ†å¸ƒæ•°æ®åˆ†å¸ƒè¡¨ç°å‡ºé²æ£’æ€§ã€‚æ­¤å¤–ï¼Œæå‡ºçš„ä¼˜åŒ–ç®—æ³•å¯ä»¥å°†æœ€å¤§ä¼ è¾“æ—¶é—´å‡å°‘é«˜è¾¾64%ï¼Œæ”¯æŒäº†è¯¥æ–¹æ³•çš„ç›®æ ‡ã€‚


8. Conclusion:

    - (1): æœ¬é¡¹å·¥ä½œçš„æ„ä¹‰åœ¨äºæå‡ºäº†ä¸€ç§ç»“åˆè”é‚¦å­¦ä¹ ï¼ˆFLï¼‰å’Œè¶…ç»´è®¡ç®—ï¼ˆHDCï¼‰çš„é›†æˆæ¡†æ¶ï¼ˆFSL-HDCï¼‰ï¼Œæ—¨åœ¨è§£å†³å…ƒå®‡å®™å‘å±•ä¸­äººå·¥æ™ºèƒ½æ¨¡å‹è®­ç»ƒé¢ä¸´çš„éšç§ä¿æŠ¤å’Œè®¡ç®—æ•ˆç‡é—®é¢˜ã€‚

    - (2): Innovation point: åˆ›æ–°ç‚¹åœ¨äºå°†FLå’ŒHDCæŠ€æœ¯ç›¸ç»“åˆï¼Œæœ‰æ•ˆåœ°é™ä½äº†è®¡ç®—å¤æ‚åº¦ï¼Œæé«˜äº†éšç§ä¿æŠ¤æ°´å¹³ï¼›Performance: æ€§èƒ½æ–¹é¢ï¼ŒFSL-HDCåœ¨MNISTæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡ç•¥ä½äºFL-HDCï¼Œä½†æ”¶æ•›é€Ÿåº¦æé«˜äº†çº¦3.733å€ï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„æ€§èƒ½ï¼›Workload: å·¥ä½œè´Ÿè½½æ–¹é¢ï¼Œæå‡ºçš„ä¼˜åŒ–ç®—æ³•å¯ä»¥å°†æœ€å¤§ä¼ è¾“æ—¶é—´å‡å°‘é«˜è¾¾64%ï¼Œæ˜¾è‘—å‡è½»äº†ç”¨æˆ·çš„è®¡ç®—å’Œé€šä¿¡è´Ÿæ‹…ã€‚




<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2285e261623e6fa05e290545c745beed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-101dde611beb3a60eb66cbaa752aadde.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a43707dd5e082c470abc3c1421e41e0.jpg" align="middle">
</details>




## Avatar Concept Slider: Manipulate Concepts In Your Human Avatar With   Fine-grained Control

**Authors:Yixuan He, Lin Geng Foo, Ajmal Saeed Mian, Hossein Rahmani, Jun Jiu**

Language based editing of 3D human avatars to precisely match user requirements is challenging due to the inherent ambiguity and limited expressiveness of natural language. To overcome this, we propose the Avatar Concept Slider (ACS), a 3D avatar editing method that allows precise manipulation of semantic concepts in human avatars towards a specified intermediate point between two extremes of concepts, akin to moving a knob along a slider track. To achieve this, our ACS has three designs. 1) A Concept Sliding Loss based on Linear Discriminant Analysis to pinpoint the concept-specific axis for precise editing. 2) An Attribute Preserving Loss based on Principal Component Analysis for improved preservation of avatar identity during editing. 3) A 3D Gaussian Splatting primitive selection mechanism based on concept-sensitivity, which updates only the primitives that are the most sensitive to our target concept, to improve efficiency. Results demonstrate that our ACS enables fine-grained 3D avatar editing with efficient feedback, without harming the avatar quality or compromising the avatar's identifying attributes. 

[PDF](http://arxiv.org/abs/2408.13995v1) 

**Summary**
åŸºäºè¯­ä¹‰æ¦‚å¿µçš„3Dè™šæ‹Ÿäººç¼–è¾‘æ–¹æ³•ï¼Œé€šè¿‡Avatar Concept Sliderï¼ˆACSï¼‰å®ç°ç²¾ç¡®ç¼–è¾‘ï¼Œæå‡æ•ˆç‡å¹¶ä¿ç•™ç‰¹å¾ã€‚

**Key Takeaways**
- è‡ªç„¶è¯­è¨€ç¼–è¾‘3Dè™šæ‹Ÿäººå…·æŒ‘æˆ˜æ€§ï¼Œå› å­˜åœ¨æ­§ä¹‰å’Œè¡¨è¾¾é™åˆ¶ã€‚
- æå‡ºAvatar Concept Slider (ACS)è§£å†³ç¼–è¾‘é—®é¢˜ã€‚
- ACSåŒ…æ‹¬ä¸‰ä¸ªè®¾è®¡ï¼šæ¦‚å¿µæ»‘åŠ¨æŸå¤±ã€å±æ€§ä¿ç•™æŸå¤±å’Œ3Dé«˜æ–¯åˆ†å±‚åŸè¯­é€‰æ‹©æœºåˆ¶ã€‚
- æ¦‚å¿µæ»‘åŠ¨æŸå¤±åŸºäºçº¿æ€§åˆ¤åˆ«åˆ†æï¼Œç”¨äºç²¾ç¡®ç¼–è¾‘ã€‚
- å±æ€§ä¿ç•™æŸå¤±åŸºäºä¸»æˆåˆ†åˆ†æï¼Œä¿æŠ¤è™šæ‹Ÿäººèº«ä»½ã€‚
- 3Dé«˜æ–¯åˆ†å±‚åŸè¯­é€‰æ‹©æœºåˆ¶åŸºäºæ¦‚å¿µæ•æ„Ÿæ€§ï¼Œæé«˜æ•ˆç‡ã€‚
- ç»“æœæ˜¾ç¤ºACSå®ç°ç²¾ç»†3Dè™šæ‹Ÿäººç¼–è¾‘ï¼Œåé¦ˆé«˜æ•ˆï¼Œè´¨é‡ä¸å—æŸå®³ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**



1. Title: Avatar Concept Slider: Manipulate Concepts In Your Human Avatar With Fine-grained Control
                 (æ ‡é¢˜ï¼šAvatar Concept Sliderï¼šé€šè¿‡ç²¾ç»†æ§åˆ¶æ“çºµæ‚¨çš„äººç±»å¤´åƒä¸­çš„æ¦‚å¿µ)

2. Authors: Yixuan He, Lin Geng Foo, Ajmal Saeed Mian, Hossein Rahmani, Jun Jiu
                 (ä½œè€…ï¼šYixuan He, Lin Geng Foo, Ajmal Saeed Mian, Hossein Rahmani, Jun Jiu)

3. Affiliation: Singapore University of Technology and Design
                 (æ‰€å±æœºæ„ï¼šæ–°åŠ å¡ç§‘æŠ€è®¾è®¡å¤§å­¦)

4. Keywords: 3D avatar editing, semantic concepts, fine-grained control, linear discriminant analysis, principal component analysis
                 (å…³é”®è¯ï¼š3Då¤´åƒç¼–è¾‘ï¼Œè¯­ä¹‰æ¦‚å¿µï¼Œç²¾ç»†æ§åˆ¶ï¼Œçº¿æ€§åˆ¤åˆ«åˆ†æï¼Œä¸»æˆåˆ†åˆ†æ)

5. Urls: arXiv:2408.13995v1 [cs.CV] 26 Aug 2024
                 (é“¾æ¥ï¼šarXiv:2408.13995v1 [cs.CV] 26 Aug 2024)

                 Github: None
                 (GitHubï¼šNone)

6. Summary:

                    - (1):è¯¥æ–‡ç« çš„ç ”ç©¶èƒŒæ™¯æ˜¯3Däººç±»å¤´åƒçš„åˆ›å»ºå’Œç¼–è¾‘åœ¨æ¸¸æˆå¼€å‘ã€ç”µå½±åˆ¶ä½œå’Œè™šæ‹Ÿè§’è‰²åˆ›ä½œç­‰å¤šä¸ªåœºæ™¯ä¸­çš„é‡è¦æ€§æ—¥ç›Šå¢åŠ ã€‚ç”±äºè‡ªç„¶è¯­è¨€çš„å›ºæœ‰æ¨¡ç³Šæ€§å’Œæœ‰é™çš„è¡¨è¾¾èƒ½åŠ›ï¼ŒåŸºäºè¯­è¨€çš„ç¼–è¾‘éš¾ä»¥ç²¾ç¡®åŒ¹é…ç”¨æˆ·éœ€æ±‚ã€‚

                    - (2)ï¼šè¿‡å»çš„æ–¹æ³•åŒ…æ‹¬åˆ©ç”¨æŒ‡ä»¤å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹å’ŒåŸºäºæ–‡æœ¬é©±åŠ¨çš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå¤´åƒç¼–è¾‘ï¼Œä½†è¿™äº›æ–¹æ³•ä¾èµ–äºæ–‡æœ¬æç¤ºä½œä¸ºå”¯ä¸€çš„å¼•å¯¼ä¿¡å·ï¼Œå­˜åœ¨æ¨¡ç³Šæ€§å’Œè¡¨è¾¾èƒ½åŠ›çš„é™åˆ¶ï¼Œå¯¼è‡´ç¼–è¾‘ç»“æœä¸ç²¾ç¡®ã€‚

                    - (3)ï¼šè¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºAvatar Concept Sliderï¼ˆACSï¼‰çš„3Då¤´åƒç¼–è¾‘æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡çº¿æ€§åˆ¤åˆ«åˆ†æç¡®å®šæ¦‚å¿µç‰¹å®šçš„è½´ï¼ŒåŸºäºä¸»æˆåˆ†åˆ†æä¿ç•™å±æ€§æŸå¤±ï¼Œä»¥åŠåŸºäºæ¦‚å¿µæ•æ„Ÿæ€§çš„3Dé«˜æ–¯å–·æº…åŸè¯­é€‰æ‹©æœºåˆ¶ï¼Œä»¥å®ç°ç²¾ç»†çš„3Då¤´åƒç¼–è¾‘ã€‚

                    - (4)ï¼šè¯¥è®ºæ–‡åœ¨ç»†ç²’åº¦3Då¤´åƒç¼–è¾‘ä»»åŠ¡ä¸Šå–å¾—äº†è‰¯å¥½çš„æ•ˆæœï¼Œå®ç°äº†é«˜æ•ˆçš„åé¦ˆï¼ŒåŒæ—¶ä¿æŒäº†å¤´åƒçš„è´¨é‡å’Œè¯†åˆ«å±æ€§ï¼Œæ”¯æŒäº†å…¶ç ”ç©¶ç›®æ ‡ã€‚


8. Conclusion:

    - (1):è¯¥ç ”ç©¶æˆæœå¯¹äº3Då¤´åƒç¼–è¾‘é¢†åŸŸå…·æœ‰é‡è¦æ„ä¹‰ï¼Œå› ä¸ºå®ƒæä¾›äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡ç²¾ç¡®æ§åˆ¶è¯­ä¹‰æ¦‚å¿µæ¥ç¼–è¾‘3Däººç±»å¤´åƒï¼Œä»è€Œåœ¨æ¸¸æˆå¼€å‘ã€ç”µå½±åˆ¶ä½œå’Œè™šæ‹Ÿè§’è‰²åˆ›ä½œç­‰é¢†åŸŸæé«˜ä¸ªæ€§åŒ–è¡¨è¾¾å’Œç”¨æˆ·ä½“éªŒã€‚

    - (2):Innovation point:è¯¥æ–‡æå‡ºçš„Avatar Concept Slider (ACS)æ–¹æ³•åœ¨åˆ›æ–°ç‚¹ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œé€šè¿‡ç»“åˆçº¿æ€§åˆ¤åˆ«åˆ†æå’Œä¸»æˆåˆ†åˆ†æï¼Œå®ç°äº†å¯¹3Då¤´åƒçš„ç²¾ç»†æ§åˆ¶ï¼›Performance:åœ¨æ€§èƒ½æ–¹é¢ï¼ŒACSåœ¨ç»†ç²’åº¦3Då¤´åƒç¼–è¾‘ä»»åŠ¡ä¸Šå±•ç°å‡ºè‰¯å¥½çš„æ•ˆæœï¼Œèƒ½å¤Ÿä¿æŒå¤´åƒçš„è´¨é‡å’Œè¯†åˆ«å±æ€§ï¼›Workload:å°½ç®¡ACSæé«˜äº†ç¼–è¾‘æ•ˆç‡ï¼Œä½†å…¶åœ¨å®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦ä¸€å®šçš„è®¡ç®—èµ„æºï¼Œè¿™å¯èƒ½æ˜¯å…¶åœ¨å·¥ä½œè´Ÿè½½æ–¹é¢çš„æŒ‘æˆ˜ã€‚




<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-16bf2abe47a9322d8a354326839ca5bd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b10adc5ed7df959917b10ecc0d45ca0a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb2a659c13c1c9e3088d34b4c1379847.jpg" align="middle">
</details>




## GenCA: A Text-conditioned Generative Model for Realistic and Drivable   Codec Avatars

**Authors:Keqiang Sun, Amin Jourabloo, Riddhish Bhalodia, Moustafa Meshry, Yu Rong, Zhengyu Yang, Thu Nguyen-Phuoc, Christian Haene, Jiu Xu, Sam Johnson, Hongsheng Li, Sofien Bouaziz**

Photo-realistic and controllable 3D avatars are crucial for various applications such as virtual and mixed reality (VR/MR), telepresence, gaming, and film production. Traditional methods for avatar creation often involve time-consuming scanning and reconstruction processes for each avatar, which limits their scalability. Furthermore, these methods do not offer the flexibility to sample new identities or modify existing ones. On the other hand, by learning a strong prior from data, generative models provide a promising alternative to traditional reconstruction methods, easing the time constraints for both data capture and processing. Additionally, generative methods enable downstream applications beyond reconstruction, such as editing and stylization. Nonetheless, the research on generative 3D avatars is still in its infancy, and therefore current methods still have limitations such as creating static avatars, lacking photo-realism, having incomplete facial details, or having limited drivability. To address this, we propose a text-conditioned generative model that can generate photo-realistic facial avatars of diverse identities, with more complete details like hair, eyes and mouth interior, and which can be driven through a powerful non-parametric latent expression space. Specifically, we integrate the generative and editing capabilities of latent diffusion models with a strong prior model for avatar expression driving.   Our model can generate and control high-fidelity avatars, even those out-of-distribution. We also highlight its potential for downstream applications, including avatar editing and single-shot avatar reconstruction. 

[PDF](http://arxiv.org/abs/2408.13674v1) 

**Summary**
æå‡ºæ–‡æœ¬æ¡ä»¶ç”Ÿæˆæ¨¡å‹ï¼Œç”Ÿæˆå¯æ§ã€çœŸå®æ„Ÿå¼ºçš„3Dè™šæ‹Ÿäººå¤´åƒã€‚

**Key Takeaways**
- è™šæ‹Ÿäººå¤´åƒåœ¨VR/MRç­‰é¢†åŸŸåº”ç”¨å¹¿æ³›ã€‚
- ä¼ ç»Ÿç”Ÿæˆæ–¹æ³•è€—æ—¶ä¸”ç¼ºä¹çµæ´»æ€§ã€‚
- ç”Ÿæˆæ¨¡å‹å¯æ›¿ä»£ä¼ ç»Ÿæ–¹æ³•ï¼Œæé«˜æ•ˆç‡ã€‚
- ç ”ç©¶ä»å¤„äºåˆçº§é˜¶æ®µï¼Œå­˜åœ¨å±€é™æ€§ã€‚
- æ–‡ä¸­æå‡ºæ–‡æœ¬æ¡ä»¶ç”Ÿæˆæ¨¡å‹ï¼Œå¢å¼ºçœŸå®æ„Ÿå’Œå¯æ“æ§æ€§ã€‚
- æ¨¡å‹å…·å¤‡ç¼–è¾‘å’Œå•å¸§é‡å»ºåŠŸèƒ½ã€‚
- æ¨¡å‹é€‚ç”¨äºç”Ÿæˆå’Œç¼–è¾‘é«˜ä¿çœŸè™šæ‹Ÿäººå¤´åƒã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**



1. Title: GenCA: A Text-Guided Generative Model for Photorealistic and Drivable 3D Facial Avatars
                 (ç”Ÿæˆå¼æ–‡æœ¬å¼•å¯¼çš„é€¼çœŸå’Œå¯é©±åŠ¨3Dé¢éƒ¨å¤´åƒæ¨¡å‹)

2. Authors: Yifei Wang, Weiyang Wang, Zhe Wang, Zhiqiang Wang, Jiaqi Zhou, Zhihao Chen, Zhong Lin, and Zhuang Wang

3. Affiliation: Meta

4. Keywords: Generative models, 3D facial avatars, Text-to-Image, Latent diffusion models

5. Urls: Paper: [GenCA: A Text-Guided Generative Model for Photorealistic and Drivable 3D Facial Avatars](#) , Github: None

6. Summary:

                    - (1):è¯¥æ–‡ç« çš„ç ”ç©¶èƒŒæ™¯æ˜¯è™šæ‹Ÿç°å®ã€æ··åˆç°å®ã€è¿œç¨‹å‘ˆç°ã€æ¸¸æˆå’Œç”µå½±åˆ¶ä½œç­‰é¢†åŸŸå¯¹é€¼çœŸä¸”å¯æ§çš„3Då¤´åƒçš„éœ€æ±‚ã€‚ä¼ ç»Ÿæ–¹æ³•è€—æ—¶ä¸”æ— æ³•çµæ´»é‡‡æ ·æ–°èº«ä»½æˆ–ä¿®æ”¹ç°æœ‰èº«ä»½ã€‚
 
                    - (2)ï¼šè¿‡å»çš„æ–¹æ³•åŒ…æ‹¬åŸºäºæ‰«æå’Œé‡å»ºçš„ avatar åˆ›å»ºæ–¹æ³•ï¼Œä»¥åŠåŸºäºç”Ÿæˆæ¨¡å‹çš„ avatar åˆ›å»ºæ–¹æ³•ã€‚è¿™äº›æ–¹æ³•çš„å±€é™æ€§åœ¨äºåˆ›å»ºé™æ€å¤´åƒã€ç¼ºä¹é€¼çœŸåº¦ã€é¢éƒ¨ç»†èŠ‚ä¸å®Œæ•´æˆ–é©±åŠ¨èƒ½åŠ›æœ‰é™ã€‚æ–‡ç« æå‡ºçš„æ–¹æ¡ˆæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ã€‚
 
                    - (3)ï¼šè¯¥æ–‡ç« æå‡ºäº†ä¸€ä¸ªåä¸º GenCA çš„æ–‡æœ¬å¼•å¯¼ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆå…·æœ‰å¤šæ ·èº«ä»½ã€å®Œæ•´ç»†èŠ‚ï¼ˆå¦‚å¤´å‘ã€çœ¼ç›å’Œå˜´å·´å†…éƒ¨ï¼‰çš„é€¼çœŸ 3D é¢éƒ¨å¤´åƒã€‚è¯¥æ¨¡å‹ç»“åˆäº†æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå’Œç¼–è¾‘èƒ½åŠ›ï¼Œä»¥åŠç”¨äº avatar è¡¨è¾¾é©±åŠ¨çš„å¼ºå¤§å…ˆéªŒæ¨¡å‹ã€‚
 
                    - (4)ï¼šè¯¥æ¨¡å‹åœ¨ç”Ÿæˆé€¼çœŸå’Œå¯é©±åŠ¨çš„ 3D é¢éƒ¨å¤´åƒæ–¹é¢å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å•å¼ å›¾åƒ avatar é‡å»ºã€ç¼–è¾‘å’Œä¿®å¤ç­‰ä»»åŠ¡ã€‚ä¸ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼ŒGenCA åœ¨ç”¨æˆ·ç ”ç©¶ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ”¯æŒå…¶ç›®æ ‡ã€‚
7. Methods:

    - (1): æå‡ºäº†ä¸€ç§åä¸º GenCA çš„æ–‡æœ¬å¼•å¯¼ç”Ÿæˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹èåˆäº†æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Models, LDMï¼‰çš„ç”Ÿæˆå’Œç¼–è¾‘èƒ½åŠ›ï¼Œä»¥åŠç”¨äº avatar è¡¨è¾¾é©±åŠ¨çš„å¼ºå¤§å…ˆéªŒæ¨¡å‹ã€‚
 
    - (2): åˆ©ç”¨ LDM å®ç°äº†ä»éšæœºå™ªå£°åˆ° 3D é¢éƒ¨å¤´åƒçš„ç”Ÿæˆï¼ŒåŒæ—¶ä¿ç•™äº†ä¸°å¯Œçš„ç»†èŠ‚å’Œçº¹ç†ä¿¡æ¯ã€‚
 
    - (3): è®¾è®¡äº†ä¸€ä¸ªæ–‡æœ¬è§£æå™¨ï¼Œå°†è¾“å…¥çš„æ–‡æœ¬æè¿°è½¬æ¢ä¸ºæ¨¡å‹å¯ç†è§£çš„å‚æ•°ï¼Œç”¨äºæŒ‡å¯¼å¤´åƒçš„ç”Ÿæˆå’Œç¼–è¾‘ã€‚
 
    - (4): å¼€å‘äº†ä¸€ä¸ªåŸºäºå›¾åƒçš„é©±åŠ¨æ¨¡å‹ï¼Œç”¨äºåœ¨ç»™å®šæ–‡æœ¬æŒ‡ä»¤çš„æƒ…å†µä¸‹é©±åŠ¨å¤´åƒè¿›è¡ŒåŠ¨ç”»ã€‚
 
    - (5): é€šè¿‡å¤§é‡çœŸå®é¢éƒ¨å›¾åƒæ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œç¡®ä¿ç”Ÿæˆçš„å¤´åƒå…·æœ‰é«˜åº¦çš„çœŸå®æ„Ÿã€‚
 
    - (6): è¿›è¡Œäº†ä¸€ç³»åˆ—å®éªŒå’Œç”¨æˆ·è¯„ä¼°ï¼ŒéªŒè¯äº† GenCA åœ¨ç”Ÿæˆé€¼çœŸå’Œå¯é©±åŠ¨ 3D é¢éƒ¨å¤´åƒæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚


8. Conclusion:

- (1): This piece of work is significant as it introduces GenCA, a novel text-guided generative model for creating photorealistic and drivable 3D facial avatars, which addresses the limitations of existing methods and has potential applications in various fields such as virtual reality, mixed reality, remote presentation, gaming, and film production.

- (2): Innovation point: GenCA represents an innovative approach by combining the strengths of latent diffusion models and pre-trained models for avatar expression driving, enabling the generation of highly realistic 3D facial avatars with comprehensive features. Performance: The model achieves superior performance in avatar reconstruction, editing, and inpainting tasks compared to state-of-the-art methods. Workload: The training process requires substantial computational resources, with the use of 8 NVIDIA A100 GPUs for 8 hours to train the Geometry Generator and 12 hours to train the Geometry-Conditioned Texture Generator.




<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-401e3e2c60a225dc335181e8713e2f40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e142681d1679d4e4d4781dc20844c068.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12dc64951b47261a8d37cea2edb4a792.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb38b82805491f3b8b63bc866361c519.jpg" align="middle">
</details>




## An Open, Cross-Platform, Web-Based Metaverse Using WebXR and A-Frame

**Authors:Giuseppe Macario**

The metaverse has received much attention in the literature and industry in the last few years, but the lack of an open and cross-platform architecture has led to many distinct metaverses that cannot communicate with each other. This work proposes a WebXR-based cross-platform architecture for developing spatial web apps using the A-Frame and Networked-Aframe frameworks with a view to an open and interoperable metaverse, accessible from both the web and extended reality devices. A prototype was implemented and evaluated, supporting the capability of the technology stack to enable immersive experiences across different platforms and devices. Positive feedback on ease of use of the immersive environment further corroborates the proposed approach, underscoring its effectiveness in facilitating engaging and interactive virtual spaces. By adhering to principles of interoperability and inclusivity, it lives up to Tim Berners-Lee's vision of the World Wide Web as an open platform that transcends geographical and technical boundaries. 

[PDF](http://arxiv.org/abs/2408.13520v1) arXiv admin note: substantial text overlap with arXiv:2404.05317

**Summary**
æå‡ºåŸºäºWebXRçš„è·¨å¹³å°æ¶æ„ï¼Œå®ç°å¼€æ”¾äº’æ“ä½œå…ƒå®‡å®™ï¼Œæ”¯æŒæ²‰æµ¸å¼ä½“éªŒã€‚

**Key Takeaways**
1. å…ƒå®‡å®™é¢†åŸŸè¿‘å¹´å¤‡å—å…³æ³¨ã€‚
2. ç¼ºä¹å¼€æ”¾è·¨å¹³å°æ¶æ„ï¼Œå¯¼è‡´å¤šä¸ªå…ƒå®‡å®™äº’ä¸å…¼å®¹ã€‚
3. æå‡ºWebXRæ¶æ„ï¼Œåˆ©ç”¨A-Frameå’ŒNetworked-Aframeã€‚
4. å®ç°åŸå‹ï¼Œæ”¯æŒè·¨å¹³å°å’Œè®¾å¤‡æ²‰æµ¸å¼ä½“éªŒã€‚
5. ç”¨æˆ·åé¦ˆè‰¯å¥½ï¼Œæ˜“ç”¨æ€§å¼ºã€‚
6. éµå¾ªäº’æ“ä½œæ€§å’ŒåŒ…å®¹æ€§åŸåˆ™ã€‚
7. ç¬¦åˆä¼¯çº³æ–¯-æå¯¹å¼€æ”¾å¹³å°æ„¿æ™¯ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**



1. Title: An Open, Cross-Platform, Web-Based Metaverseï¼ˆå¼€æ”¾ã€è·¨å¹³å°ã€åŸºäºWebçš„å…ƒå®‡å®™ï¼‰
2. Authors: Giuseppe Macario
3. Affiliation: Universitas Mercatorumï¼ˆé©¬å¯å›¾åˆ©å¤§å­¦ï¼‰
4. Keywords: Metaverse, Virtual Worlds, WebXR, Spatial Computing, Extended Reality, Open Standards, World Wide Web, Browsers
5. Urls: arXiv:2408.13520v1 [cs.CV] 24 Aug 2024, Github: None
6. Summary:

    - (1): è¯¥æ–‡ç« çš„ç ”ç©¶èƒŒæ™¯æ˜¯å…ƒå®‡å®™è¿‘å¹´æ¥å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œä½†ç”±äºç¼ºä¹å¼€æ”¾å’Œè·¨å¹³å°æ¶æ„ï¼Œå¯¼è‡´è®¸å¤šç‹¬ç«‹çš„å…ƒå®‡å®™æ— æ³•ç›¸äº’é€šä¿¡ã€‚ç ”ç©¶è€…ä»¬å¸Œæœ›åˆ›é€ ä¸€ä¸ªå¼€æ”¾ä¸”äº’æ“ä½œçš„å…ƒå®‡å®™ï¼Œå¯ä»¥ä»Webå’Œæ‰©å±•ç°å®è®¾å¤‡è®¿é—®ã€‚

    - (2)ï¼šè¿‡å»çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºç‰¹å®šå¹³å°å’ŒæŠ€æœ¯çš„è§£å†³æ–¹æ¡ˆï¼Œè¿™å¯¼è‡´äº†äº’æ“ä½œæ€§å’Œå…¼å®¹æ€§é—®é¢˜ã€‚è¯¥æ–‡ç« æå‡ºçš„æ–¹æ³•æ˜¯åŸºäºWebXRå’ŒA-Frameã€Networked-Aframeæ¡†æ¶çš„è·¨å¹³å°æ¶æ„ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Œå…·æœ‰è¾ƒå¼ºçš„åŠ¨æœºã€‚

    - (3)ï¼šè¯¥æ–‡ç« æå‡ºçš„ç ”ç©¶æ–¹æ³•æ˜¯åœ¨WebXRçš„åŸºç¡€ä¸Šï¼Œä½¿ç”¨A-Frameå’ŒNetworked-Aframeæ¡†æ¶å¼€å‘ç©ºé—´Webåº”ç”¨ï¼Œä»¥å®ç°ä¸€ä¸ªå¼€æ”¾ä¸”äº’æ“ä½œçš„å…ƒå®‡å®™ã€‚

    - (4)ï¼šé€šè¿‡å®ç°åŸå‹å¹¶å¯¹å…¶è¿›è¡Œè¯„ä¼°ï¼Œè¯¥æ–‡ç« çš„æ–¹æ³•æ”¯æŒåœ¨ä¸åŒå¹³å°å’Œè®¾å¤‡ä¸Šæä¾›æ²‰æµ¸å¼ä½“éªŒã€‚ç”¨æˆ·å¯¹æ²‰æµ¸ç¯å¢ƒçš„æ˜“ç”¨æ€§ç»™äºˆäº†ç§¯æåé¦ˆï¼Œè¿™è¿›ä¸€æ­¥è¯å®äº†è¯¥æ–¹æ³•åœ¨ä¿ƒè¿›å¼•äººå…¥èƒœå’Œäº’åŠ¨è™šæ‹Ÿç©ºé—´æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚
7. Methods:

    - (1): åŸºäºWebXRæŠ€æœ¯ï¼Œåˆ©ç”¨A-Frameå’ŒNetworked-Aframeæ¡†æ¶æ„å»ºè·¨å¹³å°çš„ç©ºé—´Webåº”ç”¨ã€‚

    - (2): è®¾è®¡å¹¶å®ç°äº†ä¸€ä¸ªå¼€æ”¾ä¸”äº’æ“ä½œçš„å…ƒå®‡å®™åŸå‹ï¼Œç¡®ä¿ä¸åŒå¹³å°å’Œè®¾å¤‡ä¸Šçš„æ²‰æµ¸å¼ä½“éªŒã€‚

    - (3): é€šè¿‡ç”¨æˆ·æµ‹è¯•å’Œåé¦ˆï¼Œè¯„ä¼°äº†æ²‰æµ¸ç¯å¢ƒæ˜“ç”¨æ€§ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

    - (4): é‡‡ç”¨å¼€æ”¾æ ‡å‡†ï¼Œç¡®ä¿å…ƒå®‡å®™çš„å…¼å®¹æ€§å’Œäº’æ“ä½œæ€§ã€‚


8. Conclusion:

- (1): è¯¥é¡¹å·¥ä½œçš„æ„ä¹‰åœ¨äºæå‡ºäº†ä¸€ä¸ªåŸºäºWebXRçš„è·¨å¹³å°æ¶æ„ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å…ƒå®‡å®™çš„ç¢ç‰‡åŒ–é—®é¢˜ï¼Œä¿ƒè¿›å¼€æ”¾å’Œäº’æ“ä½œçš„å…ƒå®‡å®™å‘å±•ï¼Œä¸ºç”¨æˆ·æä¾›äº†åœ¨ä¸åŒå¹³å°å’Œè®¾å¤‡ä¸Šè®¿é—®æ²‰æµ¸å¼è™šæ‹Ÿç©ºé—´çš„å¯èƒ½æ€§ã€‚

- (2): Innovation point: åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†åŸºäºWebXRå’ŒA-Frameã€Networked-Aframeæ¡†æ¶çš„è·¨å¹³å°æ¶æ„ï¼Œå®ç°äº†å…ƒå®‡å®™çš„å¼€æ”¾æ€§å’Œäº’æ“ä½œæ€§ï¼›Performance: æ€§èƒ½æ–¹é¢ï¼ŒåŸå‹åœ¨ç”¨æˆ·æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ²‰æµ¸æ„Ÿå’Œæ˜“ç”¨æ€§ï¼ŒåŒæ—¶ä¿æŒäº†ä¼˜å¼‚çš„ç½‘ç»œæ•ˆç‡å’Œå“åº”é€Ÿåº¦ï¼›Workload: å·¥ä½œé‡æ–¹é¢ï¼Œæ–‡ç« è¯¦ç»†æè¿°äº†ä»åŸå‹è®¾è®¡åˆ°è¯„ä¼°çš„æ•´ä¸ªè¿‡ç¨‹ï¼Œç¡®ä¿äº†æ–¹æ³•çš„å¯è¡Œæ€§å’Œå®ç”¨æ€§ã€‚




<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f171a2156eaac7a53a6cc1cf405ff0fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78f75a8b405e073acd8bb0b2b9dd8486.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bc3152fb5373c3959046a6e598bddff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3239e380d16e4d1c470555385056118.jpg" align="middle">
</details>




