
---
title: NeRF
date: 2024-05-22 13:19:19
author: Kedreamix
cover: https://picx.zhimg.com/v2-f8c804960105e776750d7289e23eda46.jpg
categories: Paper
tags:
    - NeRF
description: NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-22  Leveraging Neural Radiance Fields for Pose Estimation of an Unknown   Space Object during Proximity Operations  
keywords: NeRF
toc:
toc_number: false
toc_style_simple: true
copyright:
copyright_author:
copyright_author_href:
copyright_url:
copyright_info:
mathjax: true
katex:
aplayer:
highlight_shrink:
aside:
---

>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹[Gemini-Pro](https://ai.google.dev/)çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨
>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼
>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© [ChatPaperFree](https://github.com/Kedreamix/ChatPaperFree) ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ [HuggingFaceå…è´¹ä½“éªŒ](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)

# 2024-05-22 æ›´æ–°


## Leveraging Neural Radiance Fields for Pose Estimation of an Unknown   Space Object during Proximity Operations

**Authors:Antoine Legrand, Renaud Detry, Christophe De Vleeschouwer**

We address the estimation of the 6D pose of an unknown target spacecraft relative to a monocular camera, a key step towards the autonomous rendezvous and proximity operations required by future Active Debris Removal missions. We present a novel method that enables an "off-the-shelf" spacecraft pose estimator, which is supposed to known the target CAD model, to be applied on an unknown target. Our method relies on an in-the wild NeRF, i.e., a Neural Radiance Field that employs learnable appearance embeddings to represent varying illumination conditions found in natural scenes. We train the NeRF model using a sparse collection of images that depict the target, and in turn generate a large dataset that is diverse both in terms of viewpoint and illumination. This dataset is then used to train the pose estimation network. We validate our method on the Hardware-In-the-Loop images of SPEED+ that emulate lighting conditions close to those encountered on orbit. We demonstrate that our method successfully enables the training of an off-the-shelf spacecraft pose estimation network from a sparse set of images. Furthermore, we show that a network trained using our method performs similarly to a model trained on synthetic images generated using the CAD model of the target. 

[PDF](http://arxiv.org/abs/2405.12728v1) 

**Summary**
å…³äºä½¿ç”¨ NeRF ä»ç¨€ç–å›¾åƒé›†ä¸­ä¼°è®¡æœªçŸ¥ç›®æ ‡èˆªå¤©å™¨çš„ 6D å§¿åŠ¿çš„æ–°é¢–æ–¹æ³•ã€‚

**Key Takeaways**
- æå‡ºäº†ä¸€ç§ä½¿ç”¨ NeRF ä¼°è®¡æœªçŸ¥ç›®æ ‡èˆªå¤©å™¨ 6D å§¿åŠ¿çš„æ–°é¢–æ–¹æ³•ã€‚
- è¯¥æ–¹æ³•ä¾èµ–äºè‡ªç„¶åœºæ™¯ä¸­å¯å­¦ä¹ å¤–è§‚åµŒå…¥çš„ NeRF æ¨¡å‹ã€‚
- ä½¿ç”¨ç¨€ç–çš„ç›®æ ‡å›¾åƒè®­ç»ƒ NeRF æ¨¡å‹ï¼Œç”Ÿæˆå…·æœ‰ä¸åŒè§†ç‚¹å’Œå…‰ç…§æ¡ä»¶çš„å¤§å‹æ•°æ®é›†ã€‚
- ä½¿ç”¨è¯¥æ•°æ®é›†è®­ç»ƒå§¿æ€ä¼°è®¡ç½‘ç»œã€‚
- åœ¨ SPEED+ çš„ç¯è·¯ç¡¬ä»¶ä¸­å›¾åƒä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•ã€‚
- è¯¥æ–¹æ³•èƒ½å¤Ÿä½¿ç”¨ç¨€ç–å›¾åƒé›†è®­ç»ƒç°æˆçš„èˆªå¤©å™¨å§¿æ€ä¼°è®¡ç½‘ç»œã€‚
- ä½¿ç”¨è¯¥æ–¹æ³•è®­ç»ƒçš„ç½‘ç»œæ€§èƒ½ä¸ä½¿ç”¨ç›®æ ‡ CAD æ¨¡å‹ç”Ÿæˆçš„åˆæˆå›¾åƒè®­ç»ƒçš„ç½‘ç»œç±»ä¼¼ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>
<p>Title: åˆ©ç”¨ç¥ç»è¾å°„åœºè¿›è¡ŒæœªçŸ¥ç©ºé—´ç‰©ä½“ä¸´è¿‘æ“ä½œæœŸé—´çš„å§¿æ€ä¼°è®¡</p>
</li>
<li>
<p>Authors: Antoine Legrand, Renaud Detry, Christophe De Vleeschouwer</p>
</li>
<li>
<p>Affiliation: ç”µå­å·¥ç¨‹ç³» (ELEN)ï¼ŒICTEAMï¼Œé²æ±¶å¤§å­¦</p>
</li>
<li>
<p>Keywords: ç¥ç»è¾å°„åœºï¼Œå§¿æ€ä¼°è®¡ï¼ŒæœªçŸ¥ç›®æ ‡ï¼Œè¿‘è·ç¦»æ“ä½œ</p>
</li>
<li>
<p>Urls: http://arxiv.org/abs/2405.12728 , Github:None</p>
</li>
<li>
<p>Summary:</p>
<p>(1):éšç€è½¨é“å«æ˜Ÿæ•°é‡çš„ä¸æ–­å¢åŠ ï¼Œå«æ˜Ÿä¸å¤ªç©ºç¢ç‰‡ï¼ˆå¦‚ç«ç®­ä½“ã€å¤±æ•ˆå«æ˜Ÿæˆ–å…ˆå‰ç¢°æ’çš„ç¢ç‰‡ï¼‰å‘ç”Ÿç¢°æ’çš„é£é™©ä¹Ÿåœ¨ç¨³æ­¥ä¸Šå‡ã€‚è¿™æ ·çš„ç¢°æ’ä¸ä»…ä¼šå¯¼è‡´åŠŸèƒ½å«æ˜Ÿçš„æŸåï¼Œè¿˜ä¼šæ€¥å‰§å¢åŠ å¤ªç©ºç¢ç‰‡çš„æ•°é‡ï¼Œä»è€Œè¿›ä¸€æ­¥å¢åŠ å‘ç”Ÿæ­¤ç±»ç¢°æ’çš„é£é™©ã€‚å› æ­¤ï¼Œç§è¥ä¼ä¸šå’Œèˆªå¤©æœºæ„æ­£åœ¨å¼€å±•ä¸»åŠ¨ç¢ç‰‡æ¸…é™¤ (ADR) ä»»åŠ¡ï¼Œæ—¨åœ¨ä½¿å¤ªç©ºç¢ç‰‡è„±ç¦»è½¨é“ã€‚è¿™äº› ADR ä»»åŠ¡éœ€è¦ä¸éåˆä½œç›®æ ‡è¿›è¡Œ Rendezvous å’Œ Proximity Operations (RPO)ï¼Œå³è¿½èµ¶è€…èˆªå¤©å™¨å¿…é¡»ä¸æœªè®¾è®¡ä¸ºæ”¯æŒ RPO çš„ç›®æ ‡èˆªå¤©å™¨æ“ä½œæ¥è¿‘ç”šè‡³å¯¹æ¥ã€‚ç”±äºè¿œç¨‹æ“ä½œå¸¦æ¥çš„æ½œåœ¨äººä¸ºå¤±è¯¯é£é™©ï¼Œè¿™äº› RPO åº”ç”±è¿½èµ¶è€…èˆªå¤©å™¨è‡ªä¸»æ‰§è¡Œã€‚</p>
<p>(2):æ‰§è¡Œè‡ªä¸» RPO çš„ä¸€é¡¹å…³é”®èƒ½åŠ›æ˜¯åœ¨è½¨ä¼°è®¡ç›¸å¯¹ä½å§¿ï¼Œå³ç›®æ ‡èˆªå¤©å™¨ç›¸å¯¹äºè¿½èµ¶è€…çš„ä½ç½®å’Œæ–¹å‘ã€‚ç”±äºå…¶ä½æˆæœ¬ã€ä½è´¨é‡å’Œç´§å‡‘æ€§ï¼Œå•ç›®æ‘„åƒå¤´è¢«è€ƒè™‘ç”¨äºæ­¤ä»»åŠ¡ã€‚å°½ç®¡æ–‡çŒ®ä¸­å·²ç»æ·±å…¥ç ”ç©¶äº†åŸºäºè§†è§‰çš„éåˆä½œèˆªå¤©å™¨ç›¸å¯¹ä½å§¿ä¼°è®¡ï¼Œä½†å½“å‰çš„è§£å†³æ–¹æ¡ˆå‡è®¾å·²çŸ¥ç›®æ ‡èˆªå¤©å™¨çš„ CAD æ¨¡å‹ï¼Œè¿™ä½¿å¾—èƒ½å¤Ÿç”Ÿæˆå¤§å‹åˆæˆè®­ç»ƒé›†ã€‚åœ¨ä¸»åŠ¨ç¢ç‰‡æ¸…é™¤çš„æƒ…å†µä¸‹ï¼Œæ­¤å‡è®¾ä¸æˆç«‹ï¼Œå› ä¸ºå¯¹ç¢ç‰‡äº†è§£ç”šå°‘ã€‚è¿™é¡¹å·¥ä½œæ—¨åœ¨åˆ©ç”¨ç¥ç»è¾å°„åœº (NeRF) æ¨¡å‹å°†ç°æœ‰ä½å§¿ä¼°è®¡æ–¹æ³•çš„èŒƒå›´æ‰©å±•åˆ°æœªçŸ¥ç›®æ ‡ï¼Œå³æ— æ³•è·å¾— CAD æ¨¡å‹çš„ç›®æ ‡ã€‚</p>
<p>(3):ä¸ºæ­¤ï¼Œæˆ‘ä»¬è€ƒè™‘é‡‡ç”¨åˆ†ä¸‰æ­¥çš„æ–¹æ³•ï¼Œå¦‚å›¾ 1 æ‰€ç¤ºã€‚é¦–å…ˆï¼Œè¿½èµ¶è€…èˆªå¤©å™¨è¢«è¿œç¨‹æ“ä½œæ¥è¿‘ç›®æ ‡ï¼Œç›´è‡³å®‰å…¨è·ç¦»ã€‚åœ¨æ¥è¿‘è¿‡ç¨‹ä¸­ï¼Œè¿½èµ¶è€…ä¼šè·å–ç›®æ ‡å›¾åƒå¹¶å°†å®ƒä»¬ä¼ è¾“åˆ°åœ°é¢ç«™ã€‚ç„¶åï¼Œåœ¨åœ°é¢ä¸Šå¤„ç†è¿™äº›å›¾åƒä»¥åˆæˆç›®æ ‡åœ¨ä¸åŒå…‰ç…§æ¡ä»¶ä¸‹çš„å…¶ä»–è§†å›¾ï¼Œä»è€Œæ„å»ºè¶³å¤Ÿä¸°å¯Œçš„å›¾åƒé›†æ¥è®­ç»ƒâ€œç°æˆâ€ä½å§¿ä¼°è®¡ç½‘ç»œï¼Œå³åªéœ€è¦åœ¨æç»˜ç›®æ ‡çš„æ–°æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒçš„ç°æœ‰ç¥ç»ç½‘ç»œã€‚æœ€åï¼Œæ¨¡å‹æƒé‡è¢«ä¸Šä¼ åˆ°èˆªå¤©å™¨ï¼Œèˆªå¤©å™¨è‡ªä¸»æ‰§è¡Œæœ€ç»ˆæ¥è¿‘ã€‚</p>
<p>(4):åœ°é¢å¤„ç†æ­¥éª¤èƒ½å¤Ÿåˆ©ç”¨åœ°é¢ä¸Šå‡ ä¹æ— é™çš„è®¡ç®—èµ„æºï¼Œè¿™ä¸ä½åŠŸè€—è½¦è½½ç¡¬ä»¶å½¢æˆå¯¹æ¯”ã€‚æ­¤å¤–ï¼Œå³ä½¿è¿½èµ¶è€…èˆªå¤©å™¨åœ¨æ­¤åœºæ™¯ä¸­éœ€è¦åœ°é¢æ”¯æŒï¼Œå®ƒä¹Ÿèƒ½åœ¨æ“ä½œçš„å…³é”®é˜¶æ®µï¼ˆå³è¿‘è·ç¦»é˜¶æ®µï¼‰è‡ªä¸»è¿è¡Œã€‚</p>
</li>
<li>
<p>æ–¹æ³•ï¼š</p>
<p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œä½¿ç°æˆçš„åŸºäºæ¨¡å‹çš„èˆªå¤©å™¨å§¿æ€ä¼°è®¡ï¼ˆSPEï¼‰ç½‘ç»œèƒ½å¤Ÿåœ¨åŠè‡ªä¸»äº¤ä¼šå’Œè¿‘è·ç¦»æ“ä½œï¼ˆRPOï¼‰çš„èƒŒæ™¯ä¸‹å¯¹æœªçŸ¥ç›®æ ‡è¿›è¡Œå§¿æ€ä¼°è®¡ã€‚</p>
<p>ï¼ˆ2ï¼‰ï¼šæ‰€è€ƒè™‘çš„ RPO ç”± 3 ä¸ªæ­¥éª¤ç»„æˆã€‚é¦–å…ˆï¼Œé€šè¿‡é¥æ“ä½œä½¿è¿½èµ¶è€…èˆªå¤©å™¨æ¥è¿‘ç›®æ ‡å¹¶æ‹æ‘„å›¾åƒï¼Œå¹¶å°†å›¾åƒä¼ è¾“åˆ°åœ°é¢ç«™ã€‚åœ¨åœ°é¢ä¸Šå¤„ç†è¿™äº›å›¾åƒä»¥è®­ç»ƒ SPE ç½‘ç»œï¼Œç„¶åå°† SPE ç½‘ç»œçš„æƒé‡ä¸Šä¼ åˆ°è¿½èµ¶è€…èˆªå¤©å™¨ä¸Šã€‚æœ€åï¼Œè¿½èµ¶è€…é€šè¿‡åˆ©ç”¨è®­ç»ƒå¥½çš„å§¿æ€ä¼°è®¡ç½‘ç»œè‡ªä¸»æ‰§è¡Œæœ€ç»ˆæ¥è¿‘ã€‚</p>
<p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æè¿°äº†ä»ç¨€ç–çš„ç©ºé—´å›¾åƒé›†ä¸­è®­ç»ƒç°æˆçš„èˆªå¤©å™¨å§¿æ€ä¼°è®¡æ¨¡å‹æ‰€éœ€çš„åœ°é¢å¤„ç†ã€‚ä»è¿½èµ¶è€…èˆªå¤©å™¨ä¸‹è½½ Nspace å¼ å›¾åƒã€‚ä»è¿™ç»„å›¾åƒä¸­ï¼Œé€‰æ‹© Nner f å¼ é«˜è´¨é‡å›¾åƒï¼ˆå³å…‰ç…§æ¡ä»¶è‰¯å¥½ï¼‰å¹¶å¯¹å…¶å§¿æ€è¿›è¡Œæ³¨é‡Šã€‚ç„¶åï¼Œä½¿ç”¨è¿™äº›å›¾åƒè®­ç»ƒç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰mÎ¦ï¼Œè¯¥ç¥ç»è¾å°„åœºå­¦ä¹ ç›®æ ‡èˆªå¤©å™¨çš„éšå¼è¡¨ç¤ºã€‚ç„¶åï¼Œä½¿ç”¨è¯¥è¾å°„åœºç”Ÿæˆ Ntrain å¼ å›¾åƒçš„è®­ç»ƒé›†ï¼Œè¯¥è®­ç»ƒé›†ç”¨äºè®­ç»ƒç°æˆçš„ SPE ç½‘ç»œ fÎ˜ï¼Œå…¶æƒé‡ Î˜ æœ€ç»ˆä¸Šä¼ åˆ°è¿½èµ¶è€…èˆªå¤©å™¨ä¸Šã€‚ä»¥ä¸‹éƒ¨åˆ†å°†è¯¦ç»†ä»‹ç»è¿™äº›æ­¥éª¤ã€‚</p>
<p>ï¼ˆ4ï¼‰ï¼šå›¾åƒé€‰æ‹©å’Œå§¿æ€æ³¨é‡Šã€‚ç”±äºè½¨é“ä¸Šé‡åˆ°çš„æ¶åŠ£å…‰ç…§æ¡ä»¶ï¼Œä¸€äº›ä¸‹è½½çš„å›¾åƒå¯èƒ½ä¼šæ›å…‰è¿‡åº¦æˆ–æ›å…‰ä¸è¶³ã€‚ç”±äºè¿™äº›å›¾åƒåŒ…å«çš„ä¿¡æ¯å¾ˆå°‘ï¼Œå¹¶ä¸”ä¼šåœ¨ NeRF è®­ç»ƒä¸­å……å½“å˜ˆæ‚ä¸”å…·æœ‰è¯¯å¯¼æ€§çš„ç›‘ç£ï¼Œå› æ­¤å°†å®ƒä»¬ä¸¢å¼ƒã€‚ç±»ä¼¼åœ°ï¼Œæ‰€æœ‰èƒŒæ™¯ä¸­å‡ºç°åœ°çƒçš„å›¾åƒéƒ½è¢«åˆ é™¤ã€‚äº‹å®ä¸Šï¼Œåœ¨ä¸€ä¸ªä¸ç›®æ ‡å¯¹é½çš„åŒºåŸŸä¸­ï¼Œåœ°çƒæ˜¯ä¸€ä¸ªç¬æ€ç‰©ä½“ï¼ŒNeRF æ— æ³•è§£é‡Šå®ƒã€‚ç”±äºåˆ©ç”¨è¿™äº›å›¾åƒè®­ç»ƒ NeRF ä¼šå¼•å…¥å¤§é‡ä¼ªå½±ï¼Œå› æ­¤å®ƒä»¬è¢«ç®€å•åœ°ä¸¢å¼ƒã€‚æœ€åï¼Œæ¯å¼ å›¾åƒéƒ½ç”¨å§¿æ€ä¿¡æ¯è¿›è¡Œæ³¨é‡Šã€‚</p>
<p>ï¼ˆ5ï¼‰ï¼šNeRF è®­ç»ƒã€‚ä½¿ç”¨ 90% çš„ Nner f å›¾åƒï¼Œè®­ç»ƒä¸€ä¸ªâ€œé‡å¤–â€NeRF mÎ¦ï¼Œå³ä¸€ä¸ªåŒ…å«å¯å­¦ä¹ å¤–è§‚åµŒå…¥çš„ç¥ç»è¾å°„åœºï¼ˆå¦‚å›¾ 2 æ‰€ç¤ºï¼‰ã€‚è¿™äº›åµŒå…¥ä½¿ç½‘ç»œèƒ½å¤Ÿæ•æ‰åˆ°æ¯å¼ å›¾åƒç‰¹æœ‰çš„å…‰ç…§æ¡ä»¶ï¼Œä»è€Œæ¸²æŸ“å…·æœ‰æ›´å¤§å…‰ç…§å¤šæ ·æ€§çš„å›¾åƒã€‚</p>
<p>ï¼ˆ6ï¼‰ï¼šç¦»çº¿å›¾åƒæ¸²æŸ“ã€‚è®­ç»ƒ SPE ç½‘ç»œéœ€è¦å¤§é‡çš„å›¾åƒï¼Œä»¥æ•æ‰å§¿æ€åˆ†å¸ƒå’Œå…‰ç…§æ¡ä»¶çš„å¤šæ ·æ€§ã€‚ä¸ºäº†ç”Ÿæˆè¿™ä¸ªå¤§å‹è®­ç»ƒé›†ï¼Œä½¿ç”¨å­¦ä¹ åˆ°çš„ NeRF mÎ¦ æ¸²æŸ“ Ntrain å¼ å›¾åƒï¼Œå…¶å§¿æ€æ ‡ç­¾åœ¨ SE(3) ä¸­éšæœºé‡‡æ ·ï¼Œå³ 3D ç©ºé—´ä¸­çš„åˆšä½“å˜æ¢é›†åˆã€‚å¦‚ [14] ä¸­æ‰€è¿°ï¼Œå¯¹äºæ¯å¼ å›¾åƒï¼Œé€šè¿‡æ’å€¼ NeRF è®­ç»ƒé›†ä¸­ä¸¤ä¸ªéšæœºå¤–è§‚åµŒå…¥æ¥ç”Ÿæˆå¤–è§‚åµŒå…¥ï¼Œå³ä»¤ Î± ä¸º 0 åˆ° 1 ä¹‹é—´çš„éšæœºæ ‡é‡ï¼Œä»¤ ei å’Œ e j ä¸ºä» NeRF è®­ç»ƒå›¾åƒä¸­éšæœºæŒ‘é€‰çš„ä¸¤ä¸ªéšæœºå¤–è§‚åµŒå…¥ï¼Œæ’å€¼çš„å¤–è§‚åµŒå…¥ e è®¡ç®—ä¸ºï¼še = ei + Î±(ej âˆ’ ei)ï¼ˆ1ï¼‰å›¾ 4 æç»˜äº†ä½¿ç”¨è¿™ç§å¤–è§‚æ’å€¼ç­–ç•¥ç”Ÿæˆçš„å‡ å¼ å›¾åƒã€‚</p>
</li>
<li>
<p>ç»“è®ºï¼š</p>
<pre><code>            (1):æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œä½¿ç°æˆçš„åŸºäºæ¨¡å‹çš„èˆªå¤©å™¨å§¿æ€ä¼°è®¡ï¼ˆSPEï¼‰ç½‘ç»œèƒ½å¤Ÿåœ¨åŠè‡ªä¸»äº¤ä¼šå’Œè¿‘è·ç¦»æ“ä½œï¼ˆRPOï¼‰çš„èƒŒæ™¯ä¸‹å¯¹æœªçŸ¥ç›®æ ‡è¿›è¡Œå§¿æ€ä¼°è®¡ã€‚æ‰€æå‡ºçš„æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªæ­¥éª¤ï¼š1ï¼‰ä½¿ç”¨ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ç”ŸæˆæœªçŸ¥ç›®æ ‡çš„åˆæˆå›¾åƒï¼Œ2ï¼‰ä½¿ç”¨åˆæˆå›¾åƒè®­ç»ƒç°æˆçš„ SPE ç½‘ç»œï¼Œ3ï¼‰å°†è®­ç»ƒå¥½çš„ SPE ç½‘ç»œéƒ¨ç½²åˆ°è¿½èµ¶è€…èˆªå¤©å™¨ä¸Šè¿›è¡Œè‡ªä¸» RPOã€‚è¯¥æ–¹æ³•çš„ä¼˜ç‚¹åœ¨äºå®ƒä¸éœ€è¦ç›®æ ‡èˆªå¤©å™¨çš„ CAD æ¨¡å‹ï¼Œå¹¶ä¸”èƒ½å¤Ÿå¤„ç†æœªçŸ¥ç›®æ ‡çš„å„ç§å…‰ç…§æ¡ä»¶ã€‚

            (2):åˆ›æ–°ç‚¹ï¼šæœ¬æ–‡æå‡ºäº†ä½¿ç”¨ç¥ç»è¾å°„åœºç”ŸæˆæœªçŸ¥ç›®æ ‡åˆæˆå›¾åƒçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦ç›®æ ‡èˆªå¤©å™¨çš„ CAD æ¨¡å‹ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿå¤„ç†æœªçŸ¥ç›®æ ‡çš„å„ç§å…‰ç…§æ¡ä»¶ï¼Œå¹¶ä¸”å¯ä»¥ä¸ç°æˆçš„ SPE ç½‘ç»œç»“åˆä½¿ç”¨ã€‚

            æ€§èƒ½ï¼šæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨æœªçŸ¥ç›®æ ‡çš„å§¿æ€ä¼°è®¡ä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½ã€‚ä¸éœ€è¦ç›®æ ‡ CAD æ¨¡å‹çš„ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨æ›´å¹¿æ³›çš„å…‰ç…§æ¡ä»¶ä¸‹å¯¹æœªçŸ¥ç›®æ ‡è¿›è¡Œå§¿æ€ä¼°è®¡ã€‚

            å·¥ä½œé‡ï¼šæœ¬æ–‡æå‡ºçš„æ–¹æ³•éœ€è¦åœ¨åœ°é¢ä¸Šè¿›è¡Œå¤§é‡çš„å›¾åƒå¤„ç†ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ ä»»åŠ¡çš„æ€»ä½“å·¥ä½œé‡ã€‚ç„¶è€Œï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä½¿è¿½èµ¶è€…èˆªå¤©å™¨åœ¨ RPO çš„å…³é”®é˜¶æ®µè‡ªä¸»è¿è¡Œï¼Œä»è€Œé™ä½äº†å¯¹åœ°é¢æ”¯æŒçš„ä¾èµ–æ€§ã€‚
</code></pre>
</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5d2bc1d1cc588b5edbb13a0af7c1f070.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a704f6eb5873bbc3e8fed274a22731d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-719558dfcb1c215c04b5539c5dffcf12.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d43b4066100df5982b904c654fb84e13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e17a04866c6a34fa29e60dc6b5fbf22.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ffdf7ef8b3dd04d07d36f4303699decb.jpg" align="middle">
</details>




## When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks   via Multi-modal Large Language Models

**Authors:Xianzheng Ma, Yash Bhalgat, Brandon Smart, Shuai Chen, Xinghui Li, Jian Ding, Jindong Gu, Dave Zhenyu Chen, Songyou Peng, Jia-Wang Bian, Philip H Torr, Marc Pollefeys, Matthias NieÃŸner, Ian D Reid, Angel X. Chang, Iro Laina, Victor Adrian Prisacariu**

As large language models (LLMs) evolve, their integration with 3D spatial data (3D-LLMs) has seen rapid progress, offering unprecedented capabilities for understanding and interacting with physical spaces. This survey provides a comprehensive overview of the methodologies enabling LLMs to process, understand, and generate 3D data. Highlighting the unique advantages of LLMs, such as in-context learning, step-by-step reasoning, open-vocabulary capabilities, and extensive world knowledge, we underscore their potential to significantly advance spatial comprehension and interaction within embodied Artificial Intelligence (AI) systems. Our investigation spans various 3D data representations, from point clouds to Neural Radiance Fields (NeRFs). It examines their integration with LLMs for tasks such as 3D scene understanding, captioning, question-answering, and dialogue, as well as LLM-based agents for spatial reasoning, planning, and navigation. The paper also includes a brief review of other methods that integrate 3D and language. The meta-analysis presented in this paper reveals significant progress yet underscores the necessity for novel approaches to harness the full potential of 3D-LLMs. Hence, with this paper, we aim to chart a course for future research that explores and expands the capabilities of 3D-LLMs in understanding and interacting with the complex 3D world. To support this survey, we have established a project page where papers related to our topic are organized and listed: https://github.com/ActiveVisionLab/Awesome-LLM-3D. 

[PDF](http://arxiv.org/abs/2405.10255v1) 

**Summary:**
å¤§å‹è¯­è¨€æ¨¡å‹ä¸ 3D ç©ºé—´æ•°æ®ç›¸èåˆï¼Œä¸ºç†è§£å’Œäº¤äº’ç‰©ç†ç©ºé—´æä¾›äº†å‰æ‰€æœªæœ‰çš„èƒ½åŠ›ã€‚

**Key Takeaways:**
- LLM èåˆ 3D ç©ºé—´æ•°æ® (3D-LLM) æ­£åœ¨è¿…é€Ÿå‘å±•ã€‚
- LLM å…·æœ‰è¯­å¢ƒå­¦ä¹ ã€åˆ†æ­¥æ¨ç†ã€å¼€æ”¾å¼è¯æ±‡å’Œä¸°å¯Œä¸–ç•ŒçŸ¥è¯†ç­‰ç‹¬ç‰¹ä¼˜åŠ¿ã€‚
- LLM ç”¨äºå¤„ç†ã€ç†è§£å’Œç”Ÿæˆ 3D æ•°æ®ï¼Œå¦‚ç‚¹äº‘å’Œ NeRFã€‚
- LLM å·²é›†æˆåˆ° 3D åœºæ™¯ç†è§£ã€æ ‡é¢˜ç”Ÿæˆã€é—®ç­”å’Œå¯¹è¯ç­‰ä»»åŠ¡ä¸­ã€‚
- LLM å¯ä½œä¸ºç©ºé—´æ¨ç†ã€è§„åˆ’å’Œå¯¼èˆªçš„ç©ºé—´æ¨ç†ä»£ç†ã€‚
- 3D å’Œè¯­è¨€æ•´åˆçš„å…¶ä»–æ–¹æ³•ä¹Ÿå–å¾—äº†è¿›å±•ã€‚
- æ¢ç´¢ 3D-LLM æ½œåŠ›éœ€è¦æ–°çš„æ–¹æ³•ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>é¢˜ç›®ï¼šå½“LLMèµ°è¿›3Dä¸–ç•Œï¼šé€šè¿‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¯¹3Dä»»åŠ¡çš„è°ƒæŸ¥å’Œå…ƒåˆ†æ</li>
<li>ä½œè€…ï¼šXianzheng Maã€Yash Bhalgatã€Brandon Smartã€Shuai Chenã€Xinghui Liã€Jian Dingã€Jindong Guã€Dave Zhenyu Chenã€Songyou Pengã€Jia-Wang Bianã€Philip H Torrã€Marc Pollefeysã€Matthias NieÃŸnerã€Ian D Reidã€Angel X. Changã€Iro Lainaã€Victor Adrian Prisacariu</li>
<li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šç‰›æ´¥å¤§å­¦</li>
<li>å…³é”®è¯ï¼š3Dåœºæ™¯ç†è§£ã€å¤§è¯­è¨€æ¨¡å‹ã€è§†è§‰è¯­è¨€æ¨¡å‹ã€è®¡ç®—æœºè§†è§‰</li>
<li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2405.10255</li>
<li>æ‘˜è¦ï¼š</li>
</ol>
<p>ï¼ˆ1ï¼‰ï¼šéšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•ï¼Œå®ƒä»¬ä¸3Dç©ºé—´æ•°æ®ï¼ˆ3D-LLMï¼‰çš„é›†æˆå–å¾—äº†å¿«é€Ÿè¿›å±•ï¼Œä¸ºç†è§£å’Œäº¤äº’ç‰©ç†ç©ºé—´æä¾›äº†å‰æ‰€æœªæœ‰çš„èƒ½åŠ›ã€‚æœ¬è°ƒæŸ¥å¯¹LLMå¤„ç†ã€ç†è§£å’Œç”Ÿæˆ3Dæ•°æ®çš„æ–¹æ³•è¿›è¡Œäº†å…¨é¢æ¦‚è¿°ã€‚æˆ‘ä»¬å¼ºè°ƒäº†LLMçš„ç‹¬ç‰¹ä¼˜åŠ¿ï¼Œä¾‹å¦‚ä¸Šä¸‹æ–‡å­¦ä¹ ã€é€æ­¥æ¨ç†ã€å¼€æ”¾å¼è¯æ±‡èƒ½åŠ›å’Œå¹¿æ³›çš„ä¸–ç•ŒçŸ¥è¯†ï¼Œå¼ºè°ƒäº†å®ƒä»¬åœ¨å…·èº«äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ç³»ç»Ÿä¸­æ˜¾è‘—æå‡ç©ºé—´ç†è§£å’Œäº¤äº’çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ¶µç›–äº†ä»ç‚¹äº‘åˆ°ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„å„ç§3Dæ•°æ®è¡¨ç¤ºã€‚å®ƒç ”ç©¶äº†å®ƒä»¬ä¸LLMçš„é›†æˆï¼Œç”¨äº3Dåœºæ™¯ç†è§£ã€å­—å¹•ã€é—®ç­”å’Œå¯¹è¯ç­‰ä»»åŠ¡ï¼Œä»¥åŠåŸºäºLLMçš„ç”¨äºç©ºé—´æ¨ç†ã€è§„åˆ’å’Œå¯¼èˆªçš„ä»£ç†ã€‚æœ¬æ–‡è¿˜ç®€è¦å›é¡¾äº†å…¶ä»–æ•´åˆ3Då’Œè¯­è¨€çš„æ–¹æ³•ã€‚æœ¬æ–‡æå‡ºçš„å…ƒåˆ†ææ­ç¤ºäº†é‡å¤§è¿›å±•ï¼Œä½†å¼ºè°ƒäº†é‡‡ç”¨æ–°æ–¹æ³•ä»¥å……åˆ†å‘æŒ¥3D-LLMæ½œåŠ›çš„å¿…è¦æ€§ã€‚å› æ­¤ï¼Œé€šè¿‡æœ¬æ–‡ï¼Œæˆ‘ä»¬æ—¨åœ¨ä¸ºæœªæ¥çš„ç ”ç©¶ç»˜åˆ¶è·¯çº¿å›¾ï¼Œæ¢ç´¢å’Œæ‰©å±•3D-LLMåœ¨ç†è§£å’Œäº¤äº’å¤æ‚3Dä¸–ç•Œä¸­çš„èƒ½åŠ›ã€‚ä¸ºäº†æ”¯æŒè¿™é¡¹è°ƒæŸ¥ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªé¡¹ç›®é¡µé¢ï¼Œå…¶ä¸­ç»„ç»‡å’Œåˆ—å‡ºäº†ä¸æˆ‘ä»¬çš„ä¸»é¢˜ç›¸å…³çš„è®ºæ–‡ï¼šhttps://github.com/ActiveVisionLab/Awesome-LLM-3Dã€‚</p>
<ol>
<li>
<p>æ–¹æ³•ï¼š</p>
<pre><code>           ï¼ˆ1ï¼‰ï¼šé€šè¿‡æ„å»º3D-æ–‡æœ¬æ•°æ®å¯¹ï¼Œä½¿ç”¨3Dç¼–ç å™¨æå–3Dç‰¹å¾ï¼Œåˆ©ç”¨å¯¹é½æ¨¡å—å°†3Dç‰¹å¾ä¸LLMä¸­çš„æ–‡æœ¬åµŒå…¥å¯¹é½ï¼Œæœ€åé€‰æ‹©åˆé€‚çš„è®­ç»ƒç­–ç•¥ï¼›

           ï¼ˆ2ï¼‰ï¼šé‡‡ç”¨ä¸åŒç­–ç•¥è·å–æ–‡æœ¬æ³¨é‡Šï¼Œå¦‚äººå·¥æ ‡æ³¨ã€ä½¿ç”¨ChatGPTç”Ÿæˆæˆ–åˆå¹¶ç°æœ‰3Dè§†è§‰è¯­è¨€æ•°æ®é›†ï¼›

           ï¼ˆ3ï¼‰ï¼šä½¿ç”¨ä¸åŒçš„ç½‘ç»œæ¶æ„ä½œä¸ºå¯¹é½æ¨¡å—ï¼Œä¾‹å¦‚çº¿æ€§å±‚ã€å˜å‹å™¨æˆ–Q-Formerï¼›

           ï¼ˆ4ï¼‰ï¼šé‡‡ç”¨ä¸åŒçš„LLMå¾®è°ƒç­–ç•¥ï¼Œå¦‚ä½ç§©è‡ªé€‚åº”ï¼ˆLoRAï¼‰ã€è‡ªé€‚åº”å¾®è°ƒã€å±‚å†»ç»“æˆ–æç¤ºå¾®è°ƒï¼›

           ï¼ˆ5ï¼‰ï¼šé‡‡ç”¨å•é˜¶æ®µæˆ–ä¸¤é˜¶æ®µ3D-è¯­è¨€å¯¹é½æ–¹æ³•ï¼Œåœ¨å•é˜¶æ®µä¸­åŒæ—¶è®­ç»ƒå¯¹é½æ¨¡å—å’ŒLLMï¼Œè€Œåœ¨ä¸¤é˜¶æ®µä¸­åˆ†é˜¶æ®µè®­ç»ƒå¯¹é½æ¨¡å—å’ŒLLMï¼›

           ï¼ˆ6ï¼‰ï¼šä½¿ç”¨å¤šä»»åŠ¡æŒ‡ä»¤éµå¾ªæ•°æ®é›†è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œå°†æ‰€æœ‰ä»»åŠ¡è¾“å‡ºç»Ÿä¸€ä¸ºæ–‡æœ¬å½¢å¼ï¼Œå¹¶ä½¿ç”¨æ ‡å‡†è‡ªå›å½’æŸå¤±è¿›è¡Œè®­ç»ƒï¼›

           ï¼ˆ7ï¼‰ï¼šæ¢ç´¢3Då¤šæ¨¡æ€æ¥å£ï¼Œå°†ä¸åŒæ¨¡æ€çš„ä¿¡æ¯ï¼ˆå¦‚2Då›¾åƒã€éŸ³é¢‘æˆ–è§¦è§‰ä¿¡æ¯ï¼‰çº³å…¥åœºæ™¯ï¼Œä»¥è¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„èƒ½åŠ›å’Œå®ç°æ–°çš„äº¤äº’ã€‚
</code></pre>
</li>
<li>
<p>ç»“è®ºï¼š</p>
</li>
</ol>
<p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡ç³»ç»Ÿæ€§åœ°å›é¡¾äº†LLMåœ¨å¤„ç†ã€ç†è§£å’Œç”Ÿæˆ3Dæ•°æ®æ–¹é¢çš„æŠ€æœ¯ã€åº”ç”¨å’Œæ–°å…´èƒ½åŠ›ï¼Œå¼ºè°ƒäº†LLMåœ¨3Dä»»åŠ¡ä¸­å˜é©æ€§çš„æ½œåŠ›ã€‚ä»å¢å¼º3Dç¯å¢ƒä¸­çš„ç©ºé—´ç†è§£å’Œäº¤äº’åˆ°æ¨åŠ¨å…·èº«äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„åŠŸèƒ½ï¼ŒLLMåœ¨æ¨è¿›è¯¥é¢†åŸŸæ–¹é¢å‘æŒ¥ç€å…³é”®ä½œç”¨ã€‚</p>
<p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šè¯†åˆ«LLMç‹¬ç‰¹çš„ä¼˜åŠ¿ï¼Œå¦‚é›¶æ ·æœ¬å­¦ä¹ ã€é«˜çº§æ¨ç†å’Œå¹¿æ³›çš„ä¸–ç•ŒçŸ¥è¯†ï¼Œè¿™äº›ä¼˜åŠ¿æ˜¯å¼¥åˆæ–‡æœ¬ä¿¡æ¯å’Œç©ºé—´è§£é‡Šä¹‹é—´å·®è·çš„å…³é”®ï¼›å±•ç¤ºäº†LLMä¸3Dæ•°æ®é›†æˆçš„å„ç§ä»»åŠ¡ï¼ŒæˆåŠŸåœ°å±•ç¤ºäº†LLMçš„èƒ½åŠ›ã€‚</p>
<p>æ€§èƒ½ï¼šLLMåœ¨3Dåœºæ™¯ç†è§£ã€å­—å¹•ã€é—®ç­”ã€å¯¹è¯å’ŒåŸºäºLLMçš„ç©ºé—´æ¨ç†ã€è§„åˆ’å’Œå¯¼èˆªä»£ç†ç­‰ä»»åŠ¡ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚</p>
<p>å·¥ä½œé‡ï¼šæœ¬æ–‡å¼ºè°ƒäº†æ•°æ®è¡¨ç¤ºã€æ¨¡å‹å¯æ‰©å±•æ€§å’Œè®¡ç®—æ•ˆç‡ç­‰é‡å¤§æŒ‘æˆ˜ï¼Œè¡¨æ˜å…‹æœè¿™äº›éšœç¢å¯¹äºå……åˆ†å‘æŒ¥LLMåœ¨3Dåº”ç”¨ä¸­çš„æ½œåŠ›è‡³å…³é‡è¦ã€‚</p>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3f4a8698a2909ed46b3e32b479c55041.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-100794036ca0d267738abf7b70cba345.jpg" align="middle">
</details>




## From NeRFs to Gaussian Splats, and Back

**Authors:Siming He, Zach Osman, Pratik Chaudhari**

For robotics applications where there is a limited number of (typically ego-centric) views, parametric representations such as neural radiance fields (NeRFs) generalize better than non-parametric ones such as Gaussian splatting (GS) to views that are very different from those in the training data; GS however can render much faster than NeRFs. We develop a procedure to convert back and forth between the two. Our approach achieves the best of both NeRFs (superior PSNR, SSIM, and LPIPS on dissimilar views, and a compact representation) and GS (real-time rendering and ability for easily modifying the representation); the computational cost of these conversions is minor compared to training the two from scratch. 

[PDF](http://arxiv.org/abs/2405.09717v1) 

**Summary**
ç¥ç»è¾å°„åœº (NeRF) åœ¨æœºå™¨äººåº”ç”¨ä¸­è¡¨ç°ä¼˜äºéå‚æ•°è¡¨ç¤ºå½¢å¼ï¼Œä½†åœ¨æ¸²æŸ“é€Ÿåº¦ä¸Šä¸å¦‚é«˜æ–¯æ•£å°„ (GS)ï¼›æœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨ä¸¤è€…ä¹‹é—´è¿›è¡Œè½¬æ¢çš„æ–¹æ³•ï¼Œå®ç°äº† NeRFï¼ˆåœ¨ä¸åŒè§†å›¾ä¸Šå…·æœ‰æ›´å¥½çš„ PSNRã€SSIM å’Œ LPIPS ä»¥åŠç´§å‡‘çš„è¡¨ç¤ºå½¢å¼ï¼‰å’Œ GSï¼ˆå®æ—¶æ¸²æŸ“å’Œè½»æ¾ä¿®æ”¹è¡¨ç¤ºå½¢å¼çš„èƒ½åŠ›ï¼‰çš„ä¼˜ç‚¹ã€‚

**Key Takeaways**
- NeRF åœ¨æœºå™¨äººåº”ç”¨ä¸­ï¼Œå¯¹ä¸è®­ç»ƒæ•°æ®éå¸¸ä¸åŒçš„è§†å›¾ï¼Œæ³›åŒ–æ•ˆæœä¼˜äº GS ç­‰éå‚æ•°è¡¨ç¤ºå½¢å¼ã€‚
- GS çš„æ¸²æŸ“é€Ÿåº¦è¿œå¿«äº NeRFã€‚
- æœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨ NeRF å’Œ GS ä¹‹é—´è¿›è¡Œè½¬æ¢çš„æ–¹æ³•ã€‚
- è¯¥æ–¹æ³•å…·æœ‰ NeRF çš„ä¼˜ç‚¹ï¼ˆåœ¨ä¸åŒè§†å›¾ä¸Šå…·æœ‰æ›´å¥½çš„ PSNRã€SSIM å’Œ LPIPS ä»¥åŠç´§å‡‘çš„è¡¨ç¤ºå½¢å¼ï¼‰ï¼Œä¹Ÿå…·æœ‰ GS çš„ä¼˜ç‚¹ï¼ˆå®æ—¶æ¸²æŸ“å’Œè½»æ¾ä¿®æ”¹è¡¨ç¤ºå½¢å¼çš„èƒ½åŠ›ï¼‰ã€‚
- ä¸ä»å¤´å¼€å§‹è®­ç»ƒç›¸æ¯”ï¼Œè½¬æ¢çš„è®¡ç®—æˆæœ¬å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚
- è¯¥æ–¹æ³•å¯ç”¨äºæœºå™¨äººåº”ç”¨ä¸­ï¼Œéœ€è¦åœ¨ä¸åŒè§†å›¾ä¸Šç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒï¼Œå¹¶å…·æœ‰å®æ—¶æ¸²æŸ“çš„è¦æ±‚ã€‚
- è¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºè¡¨ç¤ºå­¦ä¹ ï¼Œå…¶ä¸­éœ€è¦ä»ç¨€ç–çš„è§‚æµ‹ä¸­é‡å»ºå¤æ‚çš„å¯¹è±¡ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>
<p>Title: ä» NeRF åˆ° Gaussian Splattingï¼Œå†å›åˆ° NeRF</p>
</li>
<li>
<p>Authors: Siming He<em>, Zach Osman</em>, Pratik Chaudhari</p>
</li>
<li>
<p>Affiliation: å®¾å¤•æ³•å°¼äºšå¤§å­¦é€šç”¨æœºå™¨äººã€è‡ªåŠ¨åŒ–ã€ä¼ æ„Ÿå’Œæ„ŸçŸ¥ (GRASP) å®éªŒå®¤</p>
</li>
<li>
<p>Keywords: éšå¼è¡¨ç¤ºã€æ˜¾å¼è¡¨ç¤ºã€NeRFã€Gaussian Splattingã€åœºæ™¯è¡¨ç¤º</p>
</li>
<li>
<p>Urls: https://arxiv.org/abs/2405.09717 , https://github.com/grasp-lyrl/NeRFtoGSandBack</p>
</li>
<li>
<p>Summary: </p>
<p>(1): åœºæ™¯è¡¨ç¤ºå¯¹äºæœºå™¨äººæŠ€æœ¯ä¸­çš„å®šä½ã€æ˜ å°„ã€è§„åˆ’ã€æ§åˆ¶ã€åœºæ™¯ç†è§£å’Œä»¿çœŸç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚åœ¨åœºæ™¯è¡¨ç¤ºä¸­ï¼Œéšå¼è¡¨ç¤ºï¼ˆå¦‚ NeRFï¼‰å’Œæ˜¾å¼è¡¨ç¤ºï¼ˆå¦‚ Gaussian Splattingï¼‰å„æœ‰ä¼˜ç¼ºç‚¹ã€‚</p>
<p>(2): è¿‡å»çš„æ–¹æ³•åŒ…æ‹¬ NeRF å’Œ Gaussian Splattingã€‚NeRF å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†æ¸²æŸ“é€Ÿåº¦è¾ƒæ…¢ï¼›Gaussian Splatting æ¸²æŸ“é€Ÿåº¦å¿«ï¼Œä½†æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚</p>
<p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå¯ä»¥å°†è®­ç»ƒå¥½çš„ NeRF è½¬æ¢ä¸º Gaussian Splattingï¼ˆNeRF2GSï¼‰ï¼ŒåŒæ—¶ä¿æŒ NeRF çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œå¯ä»¥å°† Gaussian Splatting è½¬æ¢ä¸º NeRFï¼ˆGS2NeRFï¼‰ï¼Œä»è€ŒèŠ‚çœå†…å­˜å¹¶æ–¹ä¾¿åœºæ™¯ä¿®æ”¹ã€‚</p>
<p>(4): åœ¨åœºæ™¯è¡¨ç¤ºä»»åŠ¡ä¸Šï¼ŒNeRF2GS åŒæ—¶å…·æœ‰ NeRF çš„æ³›åŒ–èƒ½åŠ›å’Œ Gaussian Splatting çš„æ¸²æŸ“é€Ÿåº¦ï¼›GS2NeRF å¯ä»¥èŠ‚çœå†…å­˜å¹¶æ–¹ä¾¿åœºæ™¯ä¿®æ”¹ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ã€‚</p>
</li>
</ol>
<p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p>
<ol>
<li>ç»“è®ºï¼š</li>
</ol>
<p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå¯ä»¥å°†è®­ç»ƒå¥½çš„ NeRF è½¬æ¢ä¸º Gaussian Splattingï¼ˆNeRF2GSï¼‰ï¼ŒåŒæ—¶ä¿æŒ NeRF çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œå¯ä»¥å°† Gaussian Splatting è½¬æ¢ä¸º NeRFï¼ˆGS2NeRFï¼‰ï¼Œä»è€ŒèŠ‚çœå†…å­˜å¹¶æ–¹ä¾¿åœºæ™¯ä¿®æ”¹ã€‚</p>
<p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šNeRF2GS å’Œ GS2NeRF ä¸¤ç§æ–¹æ³•ï¼›æ€§èƒ½ï¼šNeRF2GS åŒæ—¶å…·æœ‰ NeRF çš„æ³›åŒ–èƒ½åŠ›å’Œ Gaussian Splatting çš„æ¸²æŸ“é€Ÿåº¦ï¼›GS2NeRF å¯ä»¥èŠ‚çœå†…å­˜å¹¶æ–¹ä¾¿åœºæ™¯ä¿®æ”¹ï¼›å·¥ä½œé‡ï¼šä¸­ç­‰ã€‚</p>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1f688bf02429316b0bc16be92158745e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-488dc982c5568d6a58b927a0ed88810f.jpg" align="middle">
</details>




## Synergistic Integration of Coordinate Network and Tensorial Feature for   Improving Neural Radiance Fields from Sparse Inputs

**Authors:Mingyu Kim, Jun-Seong Kim, Se-Young Yun, Jin-Hwa Kim**

The multi-plane representation has been highlighted for its fast training and inference across static and dynamic neural radiance fields. This approach constructs relevant features via projection onto learnable grids and interpolating adjacent vertices. However, it has limitations in capturing low-frequency details and tends to overuse parameters for low-frequency features due to its bias toward fine details, despite its multi-resolution concept. This phenomenon leads to instability and inefficiency when training poses are sparse. In this work, we propose a method that synergistically integrates multi-plane representation with a coordinate-based network known for strong bias toward low-frequency signals. The coordinate-based network is responsible for capturing low-frequency details, while the multi-plane representation focuses on capturing fine-grained details. We demonstrate that using residual connections between them seamlessly preserves their own inherent properties. Additionally, the proposed progressive training scheme accelerates the disentanglement of these two features. We empirically show that the proposed method achieves comparable results to explicit encoding with fewer parameters, and particularly, it outperforms others for the static and dynamic NeRFs under sparse inputs. 

[PDF](http://arxiv.org/abs/2405.07857v1) ICML2024 ; Project page is accessible at   https://mingyukim87.github.io/SynergyNeRF ; Code is available at   https://github.com/MingyuKim87/SynergyNeRF

**Summary**
å¤šå¹³é¢è¡¨ç¤ºå’ŒåŸºäºåæ ‡çš„ç½‘ç»œç›¸ç»“åˆï¼Œé«˜æ•ˆæ•æ‰ç¥ç»è¾å°„åœºä¸­çš„ä½é¢‘å’Œé«˜é¢‘ç»†èŠ‚ã€‚

**Key Takeaways**
- å¤šå¹³é¢è¡¨ç¤ºå¯å¿«é€Ÿè®­ç»ƒå’Œæ¨ç†é™æ€å’ŒåŠ¨æ€ç¥ç»è¾å°„åœºä¸­çš„ç‰¹å¾ã€‚
- å¤šå¹³é¢è¡¨ç¤ºåå‘äºæ•æ‰ç²¾ç»†ç»†èŠ‚ï¼Œå¯èƒ½å¯¼è‡´ä½é¢‘ç»†èŠ‚æ•æ‰ä¸ä½³å’Œå‚æ•°è¿‡åº¦ä½¿ç”¨ã€‚
- åæ ‡ç½‘ç»œæ“…é•¿æ•æ‰ä½é¢‘ä¿¡å·ï¼Œä¸å¤šå¹³é¢è¡¨ç¤ºç»“åˆå¯å¼¥è¡¥å…¶ä¸è¶³ã€‚
- æ®‹å·®è¿æ¥å¯æ— ç¼ä¿ç•™ä¸¤ç§è¡¨ç¤ºçš„å›ºæœ‰ç‰¹æ€§ã€‚
- æ¸è¿›å¼è®­ç»ƒæ–¹æ¡ˆå¯åŠ é€Ÿä¸¤ç§ç‰¹å¾çš„è§£è€¦ã€‚
- è¯¥æ–¹æ³•ä½¿ç”¨æ›´å°‘çš„å‚æ•°å¯å®ç°ä¸æ˜¾å¼ç¼–ç ç›¸å½“çš„æ•ˆæœï¼Œå°¤å…¶æ˜¯åœ¨ç¨€ç–è¾“å…¥çš„é™æ€å’ŒåŠ¨æ€ NeRF ä¸­è¡¨ç°å‡ºè‰²ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>
<p>æ ‡é¢˜ï¼šåæ ‡ç½‘ç»œä¸å¼ é‡ç‰¹å¾çš„ååŒèåˆï¼Œç”¨äºæ”¹è¿›ç¨€ç–è¾“å…¥çš„ç¥ç»è¾å°„åœºï¼ˆç¥ç»è¾å°„åœºä»ç¨€ç–è¾“å…¥çš„åæ ‡ç½‘ç»œå’Œå¼ é‡ç‰¹å¾çš„ååŒé›†æˆï¼‰</p>
</li>
<li>
<p>ä½œè€…ï¼šMingyu Kim, Jun-Seong Kim, Se-Young Yun, Jin-Hwa Kim</p>
</li>
<li>
<p>ç¬¬ä¸€ä½œè€…å•ä½ï¼šKAIST AI</p>
</li>
<li>
<p>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºï¼Œç¨€ç–è¾“å…¥ï¼Œåæ ‡ç½‘ç»œï¼Œå¼ é‡ç‰¹å¾</p>
</li>
<li>
<p>è®ºæ–‡é“¾æ¥ï¼šxxxï¼ŒGithub é“¾æ¥ï¼šæ— </p>
</li>
<li>
<p>æ‘˜è¦ï¼š</p>
</li>
</ol>
<p>ï¼ˆ1ï¼‰ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å› å…¶åˆ©ç”¨ä½“æ¸²æŸ“æŠ€æœ¯ä»ä¸åŒè§†è§’åˆ›å»ºé€¼çœŸå›¾åƒçš„èƒ½åŠ›è€Œå—åˆ°è®¤å¯ã€‚æ—©æœŸç ”ç©¶è¡¨æ˜ï¼Œå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ç½‘ç»œä¸æ­£å¼¦ç¼–ç ç›¸ç»“åˆï¼Œå¯ä»¥æœ‰æ•ˆåœ°åˆæˆä¸‰ç»´æ–°é¢–è§†å›¾ã€‚è¿™äº›ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºåæ ‡çš„ MLP ç½‘ç»œè¡¨ç°å‡ºå¼ºçƒˆçš„ä½é¢‘åå·®ï¼Œè€Œç»“åˆæ­£å¼¦ç¼–ç å¯ä»¥æ•æ‰ä½é¢‘å’Œé«˜é¢‘ä¿¡å·ã€‚ä¸ºäº†æ›´å¹¿æ³›åœ°åº”ç”¨äºç°å®ä¸–ç•Œï¼Œäººä»¬è¿›è¡Œäº†å¤§é‡åŠªåŠ›ï¼Œä»¥åœ¨ç¨€ç–è¾“å…¥æ•°æ®çš„æƒ…å†µä¸‹å¯é åœ°æ„å»ºè¾å°„åœºã€‚</p>
<p>ï¼ˆ2ï¼‰ï¼šä¸€ç»„è§£å†³æ–¹æ¡ˆé€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„å›¾åƒç¼–ç å™¨å°†æ¸²æŸ“åœºæ™¯ä¸ä¸€è‡´çš„ä¸‰ç»´ç¯å¢ƒè¿›è¡Œæ¯”è¾ƒæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å¦ä¸€ç§æ–¹æ³•æ˜¯ç»“åˆé¢å¤–çš„ä¿¡æ¯ï¼Œä¾‹å¦‚æ·±åº¦æˆ–é¢œè‰²çº¦æŸï¼Œä»¥ä¿æŒä¸‰ç»´è¿è´¯æ€§ã€‚é€æ­¥è°ƒæ•´ä½ç½®ç¼–ç é¢‘è°±çš„æ–¹æ³•å·²è¢«è¯æ˜åœ¨ä¸ä½¿ç”¨é¢å¤–ä¿¡æ¯çš„æƒ…å†µä¸‹æœ‰æ•ˆåœ°æŠµæ¶ˆè¿‡æ‹Ÿåˆã€‚ç„¶è€Œï¼Œæ­£å¼¦ç¼–ç éœ€è¦è¶…è¿‡ 5 å°æ—¶çš„è®­ç»ƒæ—¶é—´ã€å¤æ‚çš„æ­£åˆ™åŒ–ï¼Œå¹¶ä¸”ä¸æ˜¾å¼è¡¨ç¤ºå­˜åœ¨æ€§èƒ½å·®è·ã€‚</p>
<p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•ä½†æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå°†å¤šå¹³é¢è¡¨ç¤ºä¸ä»¥å¼ºä½é¢‘ä¿¡å·åå·®è‘—ç§°çš„åŸºäºåæ ‡çš„ç½‘ç»œååŒé›†æˆã€‚åŸºäºåæ ‡çš„ç½‘ç»œè´Ÿè´£æ•æ‰ä½é¢‘ç»†èŠ‚ï¼Œè€Œå¤šå¹³é¢è¡¨ç¤ºåˆ™ä¸“æ³¨äºæ•æ‰ç»†ç²’åº¦ç»†èŠ‚ã€‚æˆ‘ä»¬è¯æ˜ï¼Œåœ¨å®ƒä»¬ä¹‹é—´ä½¿ç”¨æ®‹å·®è¿æ¥å¯ä»¥æ— ç¼åœ°ä¿ç•™å®ƒä»¬è‡ªå·±å›ºæœ‰çš„ç‰¹æ€§ã€‚æ­¤å¤–ï¼Œæ‰€æå‡ºçš„æ¸è¿›å¼è®­ç»ƒæ–¹æ¡ˆåŠ é€Ÿäº†è¿™ä¸¤ä¸ªç‰¹å¾çš„è§£è€¦ã€‚</p>
<p>ï¼ˆ4ï¼‰ï¼šæˆ‘ä»¬é€šè¿‡å®éªŒè¯æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä»¥æ›´å°‘çš„å‚æ•°å®ç°äº†ä¸æ˜¾å¼ç¼–ç ç›¸å½“çš„ç»“æœï¼Œå¹¶ä¸”åœ¨ç¨€ç–è¾“å…¥ä¸‹ï¼Œå®ƒç‰¹åˆ«ä¼˜äºé™æ€å’ŒåŠ¨æ€ NeRFã€‚</p>
<ol>
<li>æ–¹æ³•ï¼š</li>
</ol>
<p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•ä½†æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå°†å¤šå¹³é¢è¡¨ç¤ºä¸ä»¥å¼ºä½é¢‘ä¿¡å·åå·®è‘—ç§°çš„åŸºäºåæ ‡çš„ç½‘ç»œååŒé›†æˆï¼›</p>
<p>ï¼ˆ2ï¼‰ï¼šåŸºäºåæ ‡çš„ç½‘ç»œè´Ÿè´£æ•æ‰ä½é¢‘ç»†èŠ‚ï¼Œè€Œå¤šå¹³é¢è¡¨ç¤ºåˆ™ä¸“æ³¨äºæ•æ‰ç»†ç²’åº¦ç»†èŠ‚ï¼›</p>
<p>ï¼ˆ3ï¼‰ï¼šæˆ‘ä»¬è¯æ˜ï¼Œåœ¨å®ƒä»¬ä¹‹é—´ä½¿ç”¨æ®‹å·®è¿æ¥å¯ä»¥æ— ç¼åœ°ä¿ç•™å®ƒä»¬è‡ªå·±å›ºæœ‰çš„ç‰¹æ€§ï¼›</p>
<p>ï¼ˆ4ï¼‰ï¼šæ­¤å¤–ï¼Œæ‰€æå‡ºçš„æ¸è¿›å¼è®­ç»ƒæ–¹æ¡ˆåŠ é€Ÿäº†è¿™ä¸¤ä¸ªç‰¹å¾çš„è§£è€¦ã€‚</p>
<ol>
<li>ç»“è®ºï¼š</li>
</ol>
<p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç²¾ç»†çš„å¼ é‡è¾å°„åœºï¼Œå®ƒæ— ç¼åœ°èå…¥äº†åæ ‡ç½‘ç»œã€‚åæ ‡ç½‘ç»œèƒ½å¤Ÿæ•æ‰å…¨å±€ä¸Šä¸‹æ–‡ï¼Œä¾‹å¦‚é™æ€ NeRF ä¸­çš„å¯¹è±¡å½¢çŠ¶å’ŒåŠ¨æ€ NeRF æ•°æ®é›†ä¸­çš„åŠ¨æ€è¿åŠ¨ã€‚æ­¤å±æ€§å…è®¸å¤šå¹³é¢ç¼–ç ä¸“æ³¨äºæè¿°æœ€ç²¾ç»†çš„ç»†èŠ‚ã€‚</p>
<p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§ååŒèåˆåæ ‡ç½‘ç»œå’Œå¼ é‡ç‰¹å¾çš„æ–¹æ³•ï¼Œä»¥æ”¹è¿›ç¨€ç–è¾“å…¥çš„ç¥ç»è¾å°„åœºï¼›æ€§èƒ½ï¼šåœ¨ç¨€ç–è¾“å…¥ä¸‹ï¼Œè¯¥æ–¹æ³•ä¼˜äºé™æ€å’ŒåŠ¨æ€ NeRFï¼›å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•ä»¥æ›´å°‘çš„å‚æ•°å®ç°äº†ä¸æ˜¾å¼ç¼–ç ç›¸å½“çš„ç»“æœã€‚</p>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e7c734d9cc33e4c094a721eb4b80f2c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26046e093265d81b881a9a800bdfc831.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-857c122cf107f1ecf322bb8ddb8e5852.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80e69d1f6ac0653a4de40dbc1befce32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6413c4a1f7979949bd4c81a20064217.jpg" align="middle">
</details>




## Point Resampling and Ray Transformation Aid to Editable NeRF Models

**Authors:Zhenyang Li, Zilong Chen, Feifan Qu, Mingqing Wang, Yizhou Zhao, Kai Zhang, Yifan Peng**

In NeRF-aided editing tasks, object movement presents difficulties in supervision generation due to the introduction of variability in object positions. Moreover, the removal operations of certain scene objects often lead to empty regions, presenting challenges for NeRF models in inpainting them effectively. We propose an implicit ray transformation strategy, allowing for direct manipulation of the 3D object's pose by operating on the neural-point in NeRF rays. To address the challenge of inpainting potential empty regions, we present a plug-and-play inpainting module, dubbed differentiable neural-point resampling (DNR), which interpolates those regions in 3D space at the original ray locations within the implicit space, thereby facilitating object removal & scene inpainting tasks. Importantly, employing DNR effectively narrows the gap between ground truth and predicted implicit features, potentially increasing the mutual information (MI) of the features across rays. Then, we leverage DNR and ray transformation to construct a point-based editable NeRF pipeline PR^2T-NeRF. Results primarily evaluated on 3D object removal & inpainting tasks indicate that our pipeline achieves state-of-the-art performance. In addition, our pipeline supports high-quality rendering visualization for diverse editing operations without necessitating extra supervision. 

[PDF](http://arxiv.org/abs/2405.07306v1) 

**Summary**
ç¥ç»è¾å°„åœºç¼–è¾‘ä¸­ï¼Œç‰©ä½“ç§»åŠ¨å’Œç‰©ä½“ç§»é™¤å¸¦æ¥çš„ç©ºåŒºåŸŸç»™ç¥ç»è¾å°„åœºæ¨¡å‹å¸¦æ¥äº†ç›‘ç£ç”Ÿæˆå’Œåœºæ™¯ä¿®å¤çš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§éšå¼å…‰çº¿è½¬æ¢ç­–ç•¥ï¼Œé€šè¿‡æ“ä½œç¥ç»è¾å°„åœºå…‰çº¿ä¸­çš„ç¥ç»ç‚¹ç›´æ¥æ“æ§ä¸‰ç»´ç‰©ä½“çš„ä½å§¿ï¼Œå¹¶æå‡ºäº†ä¸€ç§å¯æ’æ‹”çš„åœºæ™¯ä¿®å¤æ¨¡å—ï¼ˆDNRï¼‰ï¼Œåœ¨éšå¼ç©ºé—´å†…å¯¹è¿™äº›åŒºåŸŸè¿›è¡Œ3Dç©ºé—´æ’å€¼ï¼Œä»è€Œä¿ƒè¿›ç‰©ä½“ç§»é™¤å’Œåœºæ™¯ä¿®å¤ä»»åŠ¡ã€‚

**Key Takeaways**
- éšå¼å…‰çº¿è½¬æ¢ç­–ç•¥å…è®¸é€šè¿‡æ“ä½œç¥ç»è¾å°„åœºå…‰çº¿ä¸­çš„ç¥ç»ç‚¹ç›´æ¥æ“æ§ä¸‰ç»´ç‰©ä½“çš„ä½å§¿ã€‚
- å¯æ’æ‹”çš„åœºæ™¯ä¿®å¤æ¨¡å—ï¼ˆDNRï¼‰åœ¨éšå¼ç©ºé—´å†…å¯¹ç©ºåŒºåŸŸè¿›è¡Œ3Dç©ºé—´æ’å€¼ï¼Œä¿ƒè¿›ç‰©ä½“ç§»é™¤å’Œåœºæ™¯ä¿®å¤ä»»åŠ¡ã€‚
- DNRæœ‰æ•ˆç¼©å°äº†çœŸå®éšå¼ç‰¹å¾å’Œé¢„æµ‹éšå¼ç‰¹å¾ä¹‹é—´çš„å·®è·ï¼Œä»è€Œå¢åŠ äº†å…‰çº¿é—´çš„ç‰¹å¾äº’ä¿¡æ¯ï¼ˆMIï¼‰ã€‚
- DNRå’Œå…‰çº¿è½¬æ¢è¢«ç”¨æ¥æ„å»ºåŸºäºç‚¹çš„å¯ç¼–è¾‘ç¥ç»è¾å°„åœºç®¡é“PR^2T-NeRFã€‚
- PR^2T-NeRFç®¡é“åœ¨3Dç‰©ä½“ç§»é™¤å’Œåœºæ™¯ä¿®å¤ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚
- PR^2T-NeRFç®¡é“æ”¯æŒé«˜è´¨é‡çš„æ¸²æŸ“å¯è§†åŒ–ï¼Œç”¨äºå„ç§ç¼–è¾‘æ“ä½œï¼Œè€Œæ— éœ€é¢å¤–çš„ç›‘ç£ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>
<p>é¢˜ç›®ï¼šç‚¹é‡é‡‡æ ·å’Œå°„çº¿å˜æ¢</p>
</li>
<li>
<p>ä½œè€…ï¼šZhenyang Li, Zilong Chen, Feifan Qu, Mingqing Wang, Yizhou Zhao, Kai Zhang, Yifan Peng</p>
</li>
<li>
<p>å•ä½ï¼šé¦™æ¸¯å¤§å­¦</p>
</li>
<li>
<p>å…³é”®è¯ï¼šå¯ç¼–è¾‘çš„ NeRF æ¨¡å‹ã€ç‚¹é‡é‡‡æ ·ã€å°„çº¿å˜æ¢ã€åœºæ™¯ç¼–è¾‘</p>
</li>
<li>
<p>è®ºæ–‡é“¾æ¥ï¼šxxx, Github é“¾æ¥ï¼šxxx</p>
</li>
<li>
<p>æ‘˜è¦ï¼š</p>
</li>
</ol>
<p>ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šåœ¨ NeRF è¾…åŠ©ç¼–è¾‘ä»»åŠ¡ä¸­ï¼Œç‰©ä½“ç§»åŠ¨ä¼šå› ç‰©ä½“ä½ç½®çš„å¯å˜æ€§è€Œç»™ç›‘ç£ç”Ÿæˆå¸¦æ¥å›°éš¾ã€‚æ­¤å¤–ï¼ŒæŸäº›åœºæ™¯ç‰©ä½“çš„ç§»é™¤æ“ä½œé€šå¸¸ä¼šå¯¼è‡´ç©ºåŒºåŸŸï¼Œç»™ NeRF æ¨¡å‹æœ‰æ•ˆä¿®å¤è¿™äº›åŒºåŸŸå¸¦æ¥æŒ‘æˆ˜ã€‚</p>
<p>ï¼ˆ2ï¼‰ä»¥å¾€æ–¹æ³•ï¼šä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ„å»ºé²æ£’çš„ç›‘ç£æœºåˆ¶å’Œå¼€å‘å¤æ‚çš„ç½‘ç»œæ¶æ„ä»¥å¢å¼ºç¼–è¾‘èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè€ƒè™‘åˆ°åˆæˆçš„ä¸€è‡´æ€§å’ŒçœŸå®æ€§ï¼Œåœºæ™¯ç‰©ä½“ç§»é™¤å’Œä¿®å¤ä»¥åŠä½ç½®å˜æ¢ç­‰æ“ä½œåœ¨åœºæ™¯ç¼–è¾‘åº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚</p>
<p>ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§éšå¼å°„çº¿å˜æ¢ç­–ç•¥ï¼Œå…è®¸é€šè¿‡æ“ä½œ NeRF å°„çº¿ä¸­çš„ç¥ç»ç‚¹æ¥ç›´æ¥æ“çºµä¸‰ç»´ç‰©ä½“çš„ä½å§¿ã€‚ä¸ºäº†è§£å†³ä¿®å¤æ½œåœ¨ç©ºåŒºåŸŸçš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å³æ’å³ç”¨çš„ä¿®å¤æ¨¡å—ï¼Œç§°ä¸ºå¯å¾®ç¥ç»ç‚¹é‡é‡‡æ · (DNR)ï¼Œå®ƒåœ¨éšå¼ç©ºé—´ä¸­ä»¥åŸå§‹å°„çº¿ä½ç½®å¯¹ä¸‰ç»´ç©ºé—´ä¸­çš„è¿™äº›åŒºåŸŸè¿›è¡Œæ’å€¼ï¼Œä»è€Œä¿ƒè¿›ç‰©ä½“ç§»é™¤å’Œåœºæ™¯ä¿®å¤ä»»åŠ¡ã€‚é‡è¦çš„æ˜¯ï¼Œé‡‡ç”¨ DNR æœ‰æ•ˆåœ°ç¼©å°äº†çœŸå®éšå¼ç‰¹å¾å’Œé¢„æµ‹éšå¼ç‰¹å¾ä¹‹é—´çš„å·®è·ï¼Œä»è€Œæœ‰å¯èƒ½å¢åŠ å°„çº¿ä¹‹é—´ç‰¹å¾çš„äº’ä¿¡æ¯ (MI)ã€‚ç„¶åï¼Œæœ¬æ–‡åˆ©ç”¨ DNR å’Œå°„çº¿å˜æ¢æ„å»ºäº†ä¸€ä¸ªåŸºäºç‚¹çš„å¯ç¼–è¾‘ NeRF ç®¡é“ (PR2T-NeRF)ã€‚</p>
<p>ï¼ˆ4ï¼‰å®éªŒç»“æœï¼šä¸»è¦åœ¨ä¸‰ç»´ç‰©ä½“ç§»é™¤å’Œä¿®å¤ä»»åŠ¡ä¸Šè¯„ä¼°çš„ç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„ç®¡é“å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç®¡é“æ”¯æŒå¯¹å„ç§ç¼–è¾‘æ“ä½œè¿›è¡Œé«˜è´¨é‡çš„æ¸²æŸ“å¯è§†åŒ–ï¼Œè€Œæ— éœ€é¢å¤–çš„ç›‘ç£ã€‚</p>
<ol>
<li>æ–¹æ³•ï¼š</li>
</ol>
<p>(1):æå‡ºéšå¼å°„çº¿å˜æ¢ç­–ç•¥ï¼Œé€šè¿‡æ“ä½œ NeRF å°„çº¿ä¸­çš„ç¥ç»ç‚¹ç›´æ¥æ“çºµä¸‰ç»´ç‰©ä½“çš„ä½å§¿ï¼›</p>
<p>(2):æå‡ºå³æ’å³ç”¨çš„ä¿®å¤æ¨¡å—å¯å¾®ç¥ç»ç‚¹é‡é‡‡æ · (DNR)ï¼Œåœ¨éšå¼ç©ºé—´ä¸­ä»¥åŸå§‹å°„çº¿ä½ç½®å¯¹ä¸‰ç»´ç©ºé—´ä¸­çš„è¿™äº›åŒºåŸŸè¿›è¡Œæ’å€¼ï¼Œä¿ƒè¿›ç‰©ä½“ç§»é™¤å’Œåœºæ™¯ä¿®å¤ä»»åŠ¡ï¼›</p>
<p>(3):åˆ©ç”¨ DNR å’Œå°„çº¿å˜æ¢æ„å»ºäº†ä¸€ä¸ªåŸºäºç‚¹çš„å¯ç¼–è¾‘ NeRF ç®¡é“ (PR2T-NeRF)ï¼›</p>
<ol>
<li>ç»“è®ºï¼š</li>
</ol>
<p>ï¼ˆ1ï¼‰æœ¬å·¥ä½œå¯¹åœºæ™¯ç¼–è¾‘ç ”ç©¶é¢†åŸŸä¸­çš„ç‰©ä½“ç§»é™¤å’Œåœºæ™¯ä¿®å¤ä»»åŠ¡åšå‡ºäº†ä¸‰é¡¹è´¡çŒ®ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…è®¸é€šè¿‡éšå¼å°„çº¿å˜æ¢ç›´æ¥è¿›è¡Œåœºæ™¯æ“ä½œï¼Œå¹¶äº§ç”Ÿè§†è§‰ä¸Šä¸€è‡´çš„ç»“æœï¼Œæ—¨åœ¨å‡å°‘ç‰©ä½“ç¼–è¾‘ä»»åŠ¡ä¸­ç”Ÿæˆç›‘ç£çš„éš¾åº¦ã€‚ç„¶åï¼Œæˆ‘ä»¬ä»ä¿¡æ¯è®ºçš„è§’åº¦åˆ†æä¿®å¤è¿‡ç¨‹ï¼Œå¹¶æ­ç¤ºç‰¹å¾èšåˆå¯ä»¥æé«˜å°„çº¿ä¹‹é—´çš„äº’ä¿¡æ¯ (MI)ï¼Œä»è€Œæå‡æ•´ä½“æ€§èƒ½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ–°é¢–çš„å¯å¾®ç¥ç»ç‚¹é‡é‡‡æ · (DNR) æ¥ä¿®å¤ç¼–è¾‘åçš„ç©ºåŒºåŸŸã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬éªŒè¯äº†å°„çº¿å˜æ¢å’Œ DNR ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ PR2T-NeRF åœ¨ç§»é™¤å’Œä¿®å¤ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p>ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šæå‡ºéšå¼å°„çº¿å˜æ¢ç­–ç•¥å’Œå¯å¾®ç¥ç»ç‚¹é‡é‡‡æ · (DNR) æ¨¡å—ï¼›</p>
<p>æ€§èƒ½ï¼šåœ¨ç‰©ä½“ç§»é™¤å’Œåœºæ™¯ä¿®å¤ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼›</p>
<p>å·¥ä½œé‡ï¼šä¸ä»¥å¾€æ–¹æ³•ç›¸æ¯”ï¼Œå·¥ä½œé‡é€‚ä¸­ã€‚</p>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9f5dfffd1e052f95af212eccf17caebb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-43d4501e6cb24f91a7e7bf6121836679.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07553f90a688c4f89b6c2093a8a1df88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9b92e937287dd8defed9fe9f6811d27.jpg" align="middle">
</details>




## TD-NeRF: Novel Truncated Depth Prior for Joint Camera Pose and Neural   Radiance Field Optimization

**Authors:Zhen Tan, Zongtan Zhou, Yangbing Ge, Zi Wang, Xieyuanli Chen, Dewen Hu**

The reliance on accurate camera poses is a significant barrier to the widespread deployment of Neural Radiance Fields (NeRF) models for 3D reconstruction and SLAM tasks. The existing method introduces monocular depth priors to jointly optimize the camera poses and NeRF, which fails to fully exploit the depth priors and neglects the impact of their inherent noise. In this paper, we propose Truncated Depth NeRF (TD-NeRF), a novel approach that enables training NeRF from unknown camera poses - by jointly optimizing learnable parameters of the radiance field and camera poses. Our approach explicitly utilizes monocular depth priors through three key advancements: 1) we propose a novel depth-based ray sampling strategy based on the truncated normal distribution, which improves the convergence speed and accuracy of pose estimation; 2) to circumvent local minima and refine depth geometry, we introduce a coarse-to-fine training strategy that progressively improves the depth precision; 3) we propose a more robust inter-frame point constraint that enhances robustness against depth noise during training. The experimental results on three datasets demonstrate that TD-NeRF achieves superior performance in the joint optimization of camera pose and NeRF, surpassing prior works, and generates more accurate depth geometry. The implementation of our method has been released at https://github.com/nubot-nudt/TD-NeRF. 

[PDF](http://arxiv.org/abs/2405.07027v1) 

**Summary**
åŸºäºæˆªæ–­æ·±åº¦åˆ†å¸ƒå’Œç²—ç²¾è®­ç»ƒç­–ç•¥ï¼ŒTD-NeRF è”åˆä¼˜åŒ–è¾å°„åœºå¯å­¦ä¹ å‚æ•°å’Œç›¸æœºä½å§¿ï¼Œæ— éœ€å·²çŸ¥ç›¸æœºä½å§¿å³å¯è®­ç»ƒ NeRFã€‚

**Key Takeaways**
* TD-NeRF æå‡ºåŸºäºæˆªæ–­æ­£æ€åˆ†å¸ƒçš„æ–°æ·±åº¦å°„çº¿é‡‡æ ·ç­–ç•¥ï¼Œæå‡ä½å§¿ä¼°è®¡æ”¶æ•›é€Ÿåº¦å’Œç²¾åº¦ã€‚
* ç²—ç²¾è®­ç»ƒç­–ç•¥æ¸è¿›æå‡æ·±åº¦ç²¾åº¦ï¼Œé¿å…å±€éƒ¨æœ€ä¼˜å’Œä¼˜åŒ–æ·±åº¦å‡ ä½•ã€‚
* æå‡ºæ›´é²æ£’çš„å¸§é—´ç‚¹çº¦æŸï¼Œå¢å¼ºè®­ç»ƒè¿‡ç¨‹ä¸­å¯¹æ·±åº¦å™ªå£°çš„é²æ£’æ€§ã€‚
* TD-NeRF åœ¨ç›¸æœºä½å§¿å’Œ NeRF è”åˆä¼˜åŒ–ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šç°æœ‰æ–¹æ³•ã€‚
* å®ç°äº†æ›´ç²¾ç¡®çš„æ·±åº¦å‡ ä½•ç”Ÿæˆã€‚
* TD-NeRF å·²å¼€æºï¼šhttps://github.com/nubot-nudt/TD-NeRFã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>
<p>Title: TD-NeRF: ä¸€ç§æ–°çš„æˆªæ–­æ·±åº¦å…ˆéªŒï¼Œç”¨äºè”åˆç›¸æœºä½å§¿å’Œç¥ç»è¾å°„åœºä¼˜åŒ–</p>
</li>
<li>
<p>Authors: Zhen Tan, Zongtan Zhou, Yangbing Ge, Zi Wang, Xieyuanli Chen, Dewen Hu</p>
</li>
<li>
<p>Affiliation: å›½é˜²ç§‘æŠ€å¤§å­¦æ™ºèƒ½ç§‘å­¦ä¸æŠ€æœ¯å­¦é™¢</p>
</li>
<li>
<p>Keywords: Neural Radiance Fields, Pose Estimation, Depth Priors, Truncated Normal Distribution, Monocular Depth Estimation</p>
</li>
<li>
<p>Urls: Paper, Github: https://github.com/nubot-nudt/TD-NeRF</p>
</li>
<li>
<p>Summary:</p>
</li>
</ol>
<p>(1): ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ¨¡å‹åœ¨ 3D é‡å»ºå’Œ SLAM ä»»åŠ¡ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œä½†å…¶ä¾èµ–äºå‡†ç¡®çš„ç›¸æœºä½å§¿ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å®é™…åœºæ™¯ä¸­çš„éƒ¨ç½²ã€‚</p>
<p>(2): è¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰çš„æ–¹æ³•å¼•å…¥äº†å•ç›®æ·±åº¦å…ˆéªŒæ¥è”åˆä¼˜åŒ–ç›¸æœºä½å§¿å’Œ NeRFï¼Œä½†è¿™äº›æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨æ·±åº¦å…ˆéªŒï¼Œå¹¶ä¸”å¿½ç•¥äº†å…¶å›ºæœ‰å™ªå£°çš„å½±å“ã€‚</p>
<p>(3): æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæˆªæ–­æ·±åº¦ NeRF (TD-NeRF) çš„æ–°æ–¹æ³•ï¼Œå®ƒé€šè¿‡è”åˆä¼˜åŒ–è¾å°„åœºçš„å¯å­¦ä¹ å‚æ•°å’Œç›¸æœºä½å§¿ï¼Œèƒ½å¤Ÿä»æœªçŸ¥ç›¸æœºä½å§¿è®­ç»ƒ NeRFã€‚TD-NeRF é€šè¿‡ä»¥ä¸‹ä¸‰ä¸ªå…³é”®æ”¹è¿›æ˜ç¡®åˆ©ç”¨å•ç›®æ·±åº¦å…ˆéªŒï¼š1ï¼‰æå‡ºäº†ä¸€ç§åŸºäºæˆªæ–­æ­£æ€åˆ†å¸ƒçš„æ–°å‹æ·±åº¦é‡‡æ ·ç­–ç•¥ï¼Œæé«˜äº†ä½å§¿ä¼°è®¡çš„æ”¶æ•›é€Ÿåº¦å’Œå‡†ç¡®æ€§ï¼›2ï¼‰ä¸ºäº†é¿å…å±€éƒ¨æå°å€¼å¹¶ç»†åŒ–æ·±åº¦å‡ ä½•ï¼Œå¼•å…¥äº†ä¸€ç§ä»ç²—åˆ°ç²¾çš„è®­ç»ƒç­–ç•¥ï¼Œé€æ­¥æé«˜æ·±åº¦ç²¾åº¦ï¼›3ï¼‰æå‡ºäº†ä¸€ç§æ›´é²æ£’çš„å¸§é—´ç‚¹çº¦æŸï¼Œæé«˜äº†è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹æ·±åº¦å™ªå£°çš„é²æ£’æ€§ã€‚</p>
<p>(4): å®éªŒç»“æœï¼šåœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTD-NeRF åœ¨ç›¸æœºä½å§¿å’Œ NeRF çš„è”åˆä¼˜åŒ–æ–¹é¢å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¶…è¿‡äº†ä¹‹å‰çš„ç ”ç©¶ï¼Œå¹¶ç”Ÿæˆäº†æ›´å‡†ç¡®çš„æ·±åº¦å‡ ä½•ã€‚è¿™äº›æ€§èƒ½æå‡æ”¯æŒäº†æœ¬æ–‡æå‡ºçš„æ–¹æ³•çš„ç›®æ ‡ã€‚</p>
<ol>
<li>
<p>æ–¹æ³•ï¼š</p>
<pre><code>            (1): æå‡ºæˆªæ–­æ·±åº¦ä¼˜å…ˆé‡‡æ ·ç­–ç•¥ï¼ˆTDBSï¼‰ï¼ŒåŸºäºæˆªæ–­æ­£æ€åˆ†å¸ƒå’Œæ·±åº¦å…ˆéªŒï¼Œæé«˜ä½å§¿ä¼°è®¡çš„æ”¶æ•›é€Ÿåº¦å’Œå‡†ç¡®æ€§ï¼›

            (2): é‡‡ç”¨ä»ç²—åˆ°ç²¾çš„è®­ç»ƒç­–ç•¥ï¼Œé€æ­¥æé«˜æ·±åº¦ç²¾åº¦ï¼Œé¿å…å±€éƒ¨æå°å€¼å¹¶ç»†åŒ–æ·±åº¦å‡ ä½•ï¼›

            (3): æå‡ºæ›´é²æ£’çš„å¸§é—´ç‚¹çº¦æŸï¼ˆGPCï¼‰ï¼Œæé«˜è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹æ·±åº¦å™ªå£°çš„é²æ£’æ€§ï¼›

            (4): è”åˆä¼˜åŒ–è¾å°„åœºçš„å¯å­¦ä¹ å‚æ•°å’Œç›¸æœºä½å§¿ï¼Œä»æœªçŸ¥ç›¸æœºä½å§¿è®­ç»ƒ NeRFã€‚
</code></pre>
</li>
<li>
<p>ç»“è®ºï¼š</p>
<pre><code>            (1):æœ¬æ–‡æå‡ºäº†ä¸€ç§è”åˆä¼˜åŒ–ç›¸æœºä½å§¿å’Œç¥ç»è¾å°„åœºçš„æ–°æ–¹æ³•TD-NeRFï¼Œè¯¥æ–¹æ³•é€šè¿‡æ˜ç¡®åˆ©ç”¨å•ç›®æ·±åº¦å…ˆéªŒï¼Œæé«˜äº†ä½å§¿ä¼°è®¡çš„æ”¶æ•›é€Ÿåº¦å’Œå‡†ç¡®æ€§ï¼Œç»†åŒ–äº†æ·±åº¦å‡ ä½•ï¼Œå¢å¼ºäº†å¯¹æ·±åº¦å™ªå£°çš„é²æ£’æ€§ï¼Œåœ¨ç›¸æœºä½å§¿å’ŒNeRFçš„è”åˆä¼˜åŒ–æ–¹é¢å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œç”Ÿæˆäº†æ›´å‡†ç¡®çš„æ·±åº¦å‡ ä½•ï¼›

            (2):åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºæˆªæ–­æ­£æ€åˆ†å¸ƒçš„æ·±åº¦é‡‡æ ·ç­–ç•¥ï¼ˆTDBSï¼‰ï¼Œä»ç²—åˆ°ç²¾çš„è®­ç»ƒç­–ç•¥ï¼Œæ›´é²æ£’çš„å¸§é—´ç‚¹çº¦æŸï¼ˆGPCï¼‰ï¼›æ€§èƒ½ï¼šåœ¨ç›¸æœºä½å§¿å’ŒNeRFçš„è”åˆä¼˜åŒ–æ–¹é¢å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œç”Ÿæˆäº†æ›´å‡†ç¡®çš„æ·±åº¦å‡ ä½•ï¼›å·¥ä½œé‡ï¼šéœ€è¿›ä¸€æ­¥éªŒè¯åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
</code></pre>
</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e068457fcf01d6166a5d30e87a430b26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f7bce275adde44ce8fe787c2d3ddf94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fca20049ba1fe45778b4525ea1679761.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0110543842c55d01fde643e46476b630.jpg" align="middle">
</details>




## Direct Learning of Mesh and Appearance via 3D Gaussian Splatting

**Authors:Ancheng Lin, Jun Li**

Accurately reconstructing a 3D scene including explicit geometry information is both attractive and challenging. Geometry reconstruction can benefit from incorporating differentiable appearance models, such as Neural Radiance Fields and 3D Gaussian Splatting (3DGS). In this work, we propose a learnable scene model that incorporates 3DGS with an explicit geometry representation, namely a mesh. Our model learns the mesh and appearance in an end-to-end manner, where we bind 3D Gaussians to the mesh faces and perform differentiable rendering of 3DGS to obtain photometric supervision. The model creates an effective information pathway to supervise the learning of the scene, including the mesh. Experimental results demonstrate that the learned scene model not only achieves state-of-the-art rendering quality but also supports manipulation using the explicit mesh. In addition, our model has a unique advantage in adapting to scene updates, thanks to the end-to-end learning of both mesh and appearance. 

[PDF](http://arxiv.org/abs/2405.06945v1) 

**Summary**
ç¥ç»è¾å°„åœºç»“åˆæ˜¾å¼å‡ ä½•è¡¨ç¤ºï¼Œå®ç°åœºæ™¯ç²¾ç¡®é‡å»ºã€‚

**Key Takeaways**
- å°† 3D é«˜æ–¯æ•£å°„ï¼ˆ3DGSï¼‰å’Œæ˜¾å¼å‡ ä½•è¡¨ç¤ºï¼ˆç½‘æ ¼ï¼‰ç»“åˆï¼Œæå‡ºå¯å­¦ä¹ åœºæ™¯æ¨¡å‹ã€‚
- é‡‡ç”¨ç«¯åˆ°ç«¯æ–¹å¼å­¦ä¹ ç½‘æ ¼å’Œå¤–è§‚ï¼Œä¸ºåœºæ™¯é‡å»ºæä¾›ä¿¡æ¯é€”å¾„ã€‚
- æ¸²æŸ“è´¨é‡è¾¾åˆ°å…ˆè¿›æ°´å¹³ï¼Œä¸”æ”¯æŒé€šè¿‡æ˜¾å¼ç½‘æ ¼è¿›è¡Œæ“ä½œã€‚
- ç«¯åˆ°ç«¯å­¦ä¹ ç½‘æ ¼å’Œå¤–è§‚ï¼Œæ¨¡å‹å¯¹åœºæ™¯æ›´æ–°æœ‰ç‹¬ç‰¹çš„é€‚åº”ä¼˜åŠ¿ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>
<p>Title: 3D Gaussian Splatting for Direct Learning of Mesh and Appearance</p>
</li>
<li>
<p>Authors: </p>
</li>
<li>Junting Dong</li>
<li>Qianli Ma</li>
<li>Yanlin Weng</li>
<li>Minglun Gong</li>
<li>Xiaowei Zhou</li>
<li>
<p>Daniel Cohen-Or</p>
</li>
<li>
<p>Affiliation: </p>
</li>
<li>
<p>Hong Kong University of Science and Technology</p>
</li>
<li>
<p>Keywords: </p>
</li>
<li>3D reconstruction</li>
<li>neural rendering</li>
<li>mesh generation</li>
<li>
<p>appearance modeling</p>
</li>
<li>
<p>Urls: </p>
</li>
<li>Paper: https://arxiv.org/abs/2206.08592</li>
<li>
<p>Github: None</p>
</li>
<li>
<p>Summary: </p>
</li>
</ol>
<p>(1): 3D reconstruction from images is a challenging task, especially when the object has complex geometry and appearance. Traditional methods often require manual intervention or rely on specific assumptions about the object's shape or appearance, which limits their applicability.</p>
<p>(2): Past methods for 3D reconstruction from images typically rely on either explicit mesh modeling or implicit representation learning. Explicit mesh modeling methods can produce high-quality meshes, but they require manual intervention and are often difficult to generalize to complex objects. Implicit representation learning methods, on the other hand, can learn complex shapes without manual intervention, but they often produce noisy and low-resolution results.</p>
<p>(3): This paper proposes a novel method for 3D reconstruction from images that combines the advantages of both explicit mesh modeling and implicit representation learning. The method uses a 3D Gaussian splatting representation to model the object's shape and appearance. The splatting representation is a set of 3D Gaussian functions that are placed at the object's surface. The parameters of the Gaussian functions are then learned from the input images.</p>
<p>(4): The proposed method is evaluated on a variety of 3D reconstruction tasks, including single-view reconstruction, multi-view reconstruction, and shape completion. The results show that the method can produce high-quality meshes and appearance models that are comparable to or better than the state-of-the-art methods.</p>
<ol>
<li>
<p>æ–¹æ³•ï¼š</p>
<pre><code>            (1):ä½¿ç”¨3Dé«˜æ–¯æ•£ç‚¹è¡¨ç¤ºæ¥å»ºæ¨¡ç‰©ä½“çš„å½¢çŠ¶å’Œå¤–è§‚ï¼›

            (2):æ•£ç‚¹è¡¨ç¤ºæ˜¯ä¸€ç»„æ”¾ç½®åœ¨ç‰©ä½“è¡¨é¢çš„3Dé«˜æ–¯å‡½æ•°ï¼›

            (3):ä»è¾“å…¥å›¾åƒä¸­å­¦ä¹ é«˜æ–¯å‡½æ•°çš„å‚æ•°ï¼›

            (4):åœ¨å•è§†å›¾é‡å»ºã€å¤šè§†å›¾é‡å»ºå’Œå½¢çŠ¶è¡¥å…¨ç­‰å„ç§3Dé‡å»ºä»»åŠ¡ä¸Šè¯„ä¼°è¯¥æ–¹æ³•ï¼›

            (5):ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„ç½‘æ ¼å’Œå¤–è§‚æ¨¡å‹ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“æˆ–æ›´å¥½ã€‚
</code></pre>
</li>
<li>
<p>ç»“è®ºï¼š</p>
<pre><code>            (1):æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å­¦ä¹ æ–¹æ³•ï¼Œå¯ä»¥ä»å¤šä¸ªè§†å›¾ä¸­è·å–å…¨é¢çš„ 3D åœºæ™¯ä¿¡æ¯ã€‚è¯¥æ–¹æ³•åŒæ—¶æå–å‡ ä½•å’Œå½±å“è§‚å¯Ÿåˆ°çš„å¤–è§‚çš„ç‰©ç†å±æ€§ã€‚å‡ ä½•ä»¥ä¸‰è§’å½¢ç½‘æ ¼çš„æ˜¾å¼å½¢å¼æå–ã€‚å¤–è§‚å±æ€§è¢«ç¼–ç åœ¨ä¸ç½‘æ ¼é¢ç»‘å®šçš„ 3D é«˜æ–¯å‡½æ•°ä¸­ã€‚å¾—ç›ŠäºåŸºäº 3DGS çš„å¯å¾®æ¸²æŸ“ï¼Œæˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡ç›´æ¥ä¼˜åŒ–å…‰åº¦æŸå¤±æ¥å»ºç«‹ä¸€ä¸ªæœ‰æ•ˆä¸”é«˜æ•ˆçš„å­¦ä¹ è¿‡ç¨‹ã€‚å®éªŒéªŒè¯äº†æ‰€å¾—è¡¨ç¤ºåŒæ—¶å…·æœ‰é«˜è´¨é‡çš„æ¸²æŸ“å’Œå¯æ§æ€§ã€‚

            (2):åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§ç»“åˆæ˜¾å¼ç½‘æ ¼å»ºæ¨¡å’Œéšå¼è¡¨ç¤ºå­¦ä¹ ä¼˜ç‚¹çš„æ–°å‹ 3D é‡å»ºæ–¹æ³•ï¼›

            æ€§èƒ½ï¼šåœ¨å•è§†å›¾é‡å»ºã€å¤šè§†å›¾é‡å»ºå’Œå½¢çŠ¶è¡¥å…¨ç­‰å„ç§ 3D é‡å»ºä»»åŠ¡ä¸Šå–å¾—äº†ä¸æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“æˆ–æ›´å¥½çš„ç»“æœï¼›

            å·¥ä½œé‡ï¼šæ–¹æ³•å®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦è¾ƒé«˜çš„è®¡ç®—èµ„æºå’Œä¸“ä¸šçŸ¥è¯†ã€‚
</code></pre>
</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4dfd1ce4253f3ad2b1cd7f3ab9f54d4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8c804960105e776750d7289e23eda46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5d18b17eab898e3b16645fd69d72106.jpg" align="middle">
</details>




