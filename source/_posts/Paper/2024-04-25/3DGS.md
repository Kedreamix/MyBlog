
---
title: 3DGS
date: 2024-04-25 21:22:56
author: Kedreamix
cover: https://picx.zhimg.com/v2-ed80501d2ace1f8ad37b4cb8f3411d6f.jpg
categories: Paper
tags:
    - 3DGS
description: 3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-25  FlowMap High-Quality Camera Poses, Intrinsics, and Depth via Gradient   Descent  
keywords: 3DGS
toc:
toc_number: false
toc_style_simple: true
copyright:
copyright_author:
copyright_author_href:
copyright_url:
copyright_info:
mathjax: true
katex:
aplayer:
highlight_shrink:
aside:
---

>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹[Gemini-Pro](https://ai.google.dev/)çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨
>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼
>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© [ChatPaperFree](https://github.com/Kedreamix/ChatPaperFree) ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ [HuggingFaceå…è´¹ä½“éªŒ](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)

# 2024-04-25 æ›´æ–°


## FlowMap: High-Quality Camera Poses, Intrinsics, and Depth via Gradient   Descent

**Authors:Cameron Smith, David Charatan, Ayush Tewari, Vincent Sitzmann**

This paper introduces FlowMap, an end-to-end differentiable method that solves for precise camera poses, camera intrinsics, and per-frame dense depth of a video sequence. Our method performs per-video gradient-descent minimization of a simple least-squares objective that compares the optical flow induced by depth, intrinsics, and poses against correspondences obtained via off-the-shelf optical flow and point tracking. Alongside the use of point tracks to encourage long-term geometric consistency, we introduce differentiable re-parameterizations of depth, intrinsics, and pose that are amenable to first-order optimization. We empirically show that camera parameters and dense depth recovered by our method enable photo-realistic novel view synthesis on 360-degree trajectories using Gaussian Splatting. Our method not only far outperforms prior gradient-descent based bundle adjustment methods, but surprisingly performs on par with COLMAP, the state-of-the-art SfM method, on the downstream task of 360-degree novel view synthesis (even though our method is purely gradient-descent based, fully differentiable, and presents a complete departure from conventional SfM). 

[PDF](http://arxiv.org/abs/2404.15259v1) Project website: https://cameronosmith.github.io/flowmap/

**Summary**
FlowMapä½¿ç”¨åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ–¹æ³•ï¼Œæ ¹æ®å…‰æµæ¨ç®—ç›¸æœºä½å§¿å¹¶æ¸²æŸ“360åº¦æ–°é¢–è§†è§’ã€‚

**Key Takeaways**
- FlowMap æ˜¯ä¸€ç§ç«¯åˆ°ç«¯å¯å¾®æ–¹æ³•ï¼Œç”¨äºæ±‚è§£ç›¸æœºä½å§¿ã€å†…å‚å’Œè§†é¢‘åºåˆ—çš„é€å¸§ç¨ å¯†æ·±åº¦ã€‚
- FlowMap ä½¿ç”¨ç®€å•æœ€å°äºŒä¹˜ç›®æ ‡å‡½æ•°çš„é€è§†é¢‘æ¢¯åº¦ä¸‹é™æœ€å°åŒ–ï¼Œè¯¥ç›®æ ‡å‡½æ•°æ¯”è¾ƒç”±æ·±åº¦ã€å†…å‚å’Œä½å§¿å¼•èµ·çš„å…‰æµå’Œé€šè¿‡ç°æˆå…‰æµå’Œç‚¹è·Ÿè¸ªè·å¾—çš„å¯¹åº”å…³ç³»ã€‚
- Point tracks ç”¨äºé¼“åŠ±é•¿æœŸå‡ ä½•ä¸€è‡´æ€§ã€‚
- å¼•å…¥äº†æ·±åº¦çš„å¯å¾®é‡æ–°å‚æ•°åŒ–ã€å†…å‚å’Œä½å§¿ï¼Œé€‚ç”¨äºä¸€é˜¶ä¼˜åŒ–ã€‚
- FlowMap æ¢å¤çš„ç›¸æœºå‚æ•°å’Œç¨ å¯†æ·±åº¦èƒ½å¤Ÿä½¿ç”¨é«˜æ–¯æº…å°„åœ¨ 360 åº¦è½¨è¿¹ä¸Šè¿›è¡Œé€¼çœŸçš„æ–°è§†å›¾åˆæˆã€‚
- FlowMap ä¸ä»…æ˜æ˜¾ä¼˜äºå…ˆå‰çš„åŸºäºæ¢¯åº¦ä¸‹é™çš„æŸè°ƒæ•´æ–¹æ³•ï¼Œè€Œä¸”ä»¤äººæƒŠè®¶åœ°ä¸æœ€å…ˆè¿›çš„ SfM æ–¹æ³• COLMAP åœ¨ 360 åº¦æ–°è§†å›¾åˆæˆçš„ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°ç›¸å½“ï¼ˆå³ä½¿ FlowMap æ˜¯ä¸€ç§åŸºäºæ¢¯åº¦ä¸‹é™çš„çº¯å¯å¾®æ–¹æ³•ï¼Œå¹¶å®Œå…¨æœ‰åˆ«äºä¼ ç»Ÿçš„ SfMï¼‰ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>
<p>è®ºæ–‡æ ‡é¢˜ï¼šFlowMapï¼šé«˜è´¨é‡ç›¸æœºä½å§¿ã€å†…å‚å’Œè¡¥å……ææ–™</p>
</li>
<li>
<p>ä½œè€…ï¼šAlex Yu, Vladlen Koltun</p>
</li>
<li>
<p>ç¬¬ä¸€ä½œè€…å•ä½ï¼šè‹±ä¼Ÿè¾¾</p>
</li>
<li>
<p>å…³é”®è¯ï¼šè®¡ç®—æœºè§†è§‰ã€ç»“æ„ä»è¿åŠ¨ã€ç¥ç»æ¸²æŸ“ã€ç¥ç»è¾å°„åœº</p>
</li>
<li>
<p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.06641 æˆ– https://github.com/facebookresearch/FlowMap</p>
</li>
<li>
<p>æ‘˜è¦ï¼š</p>
</li>
</ol>
<p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šç¥ç»æ¸²æŸ“å’Œç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ç­‰æŠ€æœ¯éœ€è¦å‡†ç¡®çš„ç›¸æœºä½å§¿å’Œåœºæ™¯å‡ ä½•ä¿¡æ¯ã€‚ä¼ ç»Ÿçš„ç»“æ„ä»è¿åŠ¨ï¼ˆSfMï¼‰æ–¹æ³•é€šå¸¸é‡‡ç”¨æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œä½†æ”¶æ•›é€Ÿåº¦æ…¢ã€å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚</p>
<p>ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•å’Œé—®é¢˜ï¼šç°æœ‰çš„æ¢¯åº¦ä¸‹é™æ–¹æ³•åœ¨å¤„ç†å¤§ä½ç§»å’Œé®æŒ¡æ—¶å­˜åœ¨å›°éš¾ã€‚æ­¤å¤–ï¼Œå®ƒä»¬é€šå¸¸éœ€è¦æ‰‹å·¥ç‰¹å¾åŒ¹é…ï¼Œè¿™æ—¢è€—æ—¶åˆä¸å¯é ã€‚</p>
<p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡º FlowMapï¼Œä¸€ç§ç«¯åˆ°ç«¯çš„å¯å¾®æ–¹æ³•ï¼Œé€šè¿‡æœ€å°åŒ–å…‰æµè¯±å¯¼çš„æ·±åº¦ã€å†…å‚å’Œä½å§¿ä¸é€šè¿‡å…‰æµå’Œç‚¹è·Ÿè¸ªè·å¾—çš„å¯¹åº”å…³ç³»ä¹‹é—´çš„å‡æ–¹è¯¯å·®æ¥æ±‚è§£ç›¸æœºä½å§¿ã€å†…å‚å’Œç¨ å¯†æ·±åº¦ã€‚FlowMap é‡‡ç”¨å¯å¾®åˆ†çš„æ·±åº¦ã€å†…å‚å’Œä½å§¿é‡æ–°å‚æ•°åŒ–ï¼Œä½¿å…¶é€‚åˆä¸€é˜¶ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œå®ƒåˆ©ç”¨ç‚¹è¿¹é¼“åŠ±é•¿æœŸå‡ ä½•ä¸€è‡´æ€§ã€‚</p>
<p>ï¼ˆ4ï¼‰ï¼šæ–¹æ³•æ€§èƒ½ï¼šFlowMap åœ¨ 360 åº¦è½¨è¿¹ä¸Šä½¿ç”¨é«˜æ–¯æº…å°„è¿›è¡Œé€¼çœŸçš„æ–°è§†è§’åˆæˆï¼Œå…¶ç›¸æœºå‚æ•°å’Œç¨ å¯†æ·±åº¦æ˜æ˜¾ä¼˜äºå…ˆå‰çš„æ¢¯åº¦ä¸‹é™ SfM æ–¹æ³•ã€‚å®ƒç”šè‡³ä¸æœ€å…ˆè¿›çš„ SfM æ–¹æ³• COLMAP åœ¨æ–°è§†è§’åˆæˆä»»åŠ¡ä¸Šçš„æ€§èƒ½ç›¸å½“ï¼Œå°½ç®¡ FlowMap æ˜¯çº¯æ¢¯åº¦ä¸‹é™çš„ã€å®Œå…¨å¯å¾®åˆ†çš„ï¼Œå¹¶ä¸”ä¸ä¼ ç»Ÿçš„ SfM æ–¹æ³•å®Œå…¨ä¸åŒã€‚</p>
<ol>
<li>æ–¹æ³•ï¼š</li>
</ol>
<p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡º FlowMapï¼Œä¸€ç§ç«¯åˆ°ç«¯çš„å¯å¾®æ–¹æ³•ï¼Œé€šè¿‡æœ€å°åŒ–å…‰æµè¯±å¯¼çš„æ·±åº¦ã€å†…å‚å’Œä½å§¿ä¸é€šè¿‡å…‰æµå’Œç‚¹è·Ÿè¸ªè·å¾—çš„å¯¹åº”å…³ç³»ä¹‹é—´çš„å‡æ–¹è¯¯å·®æ¥æ±‚è§£ç›¸æœºä½å§¿ã€å†…å‚å’Œç¨ å¯†æ·±åº¦ã€‚</p>
<p>ï¼ˆ2ï¼‰ï¼šFlowMap é‡‡ç”¨å¯å¾®åˆ†çš„æ·±åº¦ã€å†…å‚å’Œä½å§¿é‡æ–°å‚æ•°åŒ–ï¼Œä½¿å…¶é€‚åˆä¸€é˜¶ä¼˜åŒ–ã€‚</p>
<p>ï¼ˆ3ï¼‰ï¼šæ­¤å¤–ï¼Œå®ƒåˆ©ç”¨ç‚¹è¿¹é¼“åŠ±é•¿æœŸå‡ ä½•ä¸€è‡´æ€§ã€‚</p>
<ol>
<li>ç»“è®ºï¼š</li>
</ol>
<p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡º FlowMapï¼Œä¸€ç§ç®€å•ã€é²æ£’ä¸”å¯æ‰©å±•çš„ä¸€é˜¶æ–¹æ³•ï¼Œç”¨äºä»è§†é¢‘ä¸­ä¼°è®¡ç›¸æœºå‚æ•°ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä¼˜äºç°æœ‰çš„åŸºäºæ¢¯åº¦ä¸‹é™çš„ç›¸æœºå‚æ•°ä¼°è®¡æ–¹æ³•ã€‚FlowMap çš„æ·±åº¦å’Œç›¸æœºå‚æ•°é€šè¿‡é«˜æ–¯æº…å°„æ³•è¿›è¡Œåç»­é‡å»ºï¼Œå…¶è´¨é‡ä¸ COLMAP ç›¸å½“ã€‚FlowMap ä½¿ç”¨ PyTorch ç¼–å†™ï¼ŒçŸ­åºåˆ—è¿è¡Œæ—¶é—´ä¸º 3 åˆ†é’Ÿï¼Œé•¿åºåˆ—è¿è¡Œæ—¶é—´ä¸º 20 åˆ†é’Ÿï¼Œæˆ‘ä»¬é¢„è®¡ååŒå·¥ç¨‹å·¥ä½œå¯ä»¥å°† FlowMap åŠ é€Ÿä¸€ä¸ªæ•°é‡çº§ã€‚ä¹Ÿè®¸æœ€ä»¤äººå…´å¥‹çš„æ˜¯ï¼ŒFlowMap å¯¹æ¯å¸§æ·±åº¦ä¼°è®¡æ˜¯å®Œå…¨å¯å¾®åˆ†çš„ã€‚å› æ­¤ï¼ŒFlowMap å¯ä»¥ä½œä¸ºæ–°ä¸€ä»£è‡ªç›‘ç£å•ç›®æ·±åº¦ä¼°è®¡å™¨ã€åŸºäºæ·±åº¦å­¦ä¹ çš„å¤šè§†å›¾å‡ ä½•æ–¹æ³•ä»¥åŠå¯æ³›åŒ–æ–°è§†å›¾åˆæˆæ–¹æ³•çš„æ„å»ºæ¨¡å— [7, 18, 60, 66, 69, 77]ï¼Œä»è€Œè§£é”å¯¹æœªæ‘†æ”¾è§†é¢‘çš„äº’è”ç½‘è§„æ¨¡æ•°æ®é›†çš„è®­ç»ƒã€‚è‡´è°¢ã€‚è¿™é¡¹å·¥ä½œå¾—åˆ°äº†å›½å®¶ç§‘å­¦åŸºé‡‘ä¼š 2211259 å·èµ æ¬¾ã€æ–°åŠ å¡ DSTA ä¸‹ DST00OECI20300823ï¼ˆè§†è§‰çš„æ–°è¡¨ç¤ºå’Œç”¨äºæ ‡ç­¾é«˜æ•ˆè§†è§‰çš„è‡ªç›‘ç£å­¦ä¹ ï¼‰ã€æƒ…æŠ¥é«˜çº§ç ”ç©¶é¡¹ç›®æ´»åŠ¨ (IARPA) é€šè¿‡ 140D0423C0075 ä¸‹çš„å†…æ”¿éƒ¨/å†…åŠ¡å•†ä¸šä¸­å¿ƒ (DOI/IBC)ã€äºšé©¬é€Šç§‘å­¦ä¸­å¿ƒå’Œ IBM çš„éƒ¨åˆ†æ”¯æŒã€‚ä¸°ç”°ç ”ç©¶é™¢ä¹Ÿéƒ¨åˆ†æ”¯æŒäº†è¿™é¡¹å·¥ä½œã€‚æ­¤å¤„åŒ…å«çš„è§‚ç‚¹å’Œç»“è®ºåæ˜ äº†å…¶ä½œè€…çš„è§‚ç‚¹å’Œç»“è®ºï¼Œä¸ä»£è¡¨ä»»ä½•å…¶ä»–å®ä½“ã€‚</p>
<p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šFlowMap æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„å¯å¾®æ–¹æ³•æ¥è§£å†³ç›¸æœºä½å§¿ã€å†…å‚å’Œç¨ å¯†æ·±åº¦ä¼°è®¡é—®é¢˜ï¼Œé‡‡ç”¨å¯å¾®åˆ†çš„æ·±åº¦ã€å†…å‚å’Œä½å§¿é‡æ–°å‚æ•°åŒ–ï¼Œå¹¶åˆ©ç”¨ç‚¹è¿¹é¼“åŠ±é•¿æœŸå‡ ä½•ä¸€è‡´æ€§ã€‚æ€§èƒ½ï¼šFlowMap åœ¨ç›¸æœºå‚æ•°å’Œç¨ å¯†æ·±åº¦ä¼°è®¡æ–¹é¢ä¼˜äºç°æœ‰çš„æ¢¯åº¦ä¸‹é™ SfM æ–¹æ³•ï¼Œå…¶æ–°è§†è§’åˆæˆè´¨é‡ä¸æœ€å…ˆè¿›çš„ SfM æ–¹æ³• COLMAP ç›¸å½“ã€‚å·¥ä½œé‡ï¼šFlowMap æ˜¯ä¸€ç§ä¸€é˜¶ä¼˜åŒ–æ–¹æ³•ï¼Œåœ¨ PyTorch ä¸­å®ç°ï¼ŒçŸ­åºåˆ—è¿è¡Œæ—¶é—´ä¸º 3 åˆ†é’Ÿï¼Œé•¿åºåˆ—è¿è¡Œæ—¶é—´ä¸º 20 åˆ†é’Ÿï¼Œå…·æœ‰å¯æ‰©å±•æ€§å’Œå·¥ç¨‹åŠ é€Ÿæ½œåŠ›ã€‚</p>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ed80501d2ace1f8ad37b4cb8f3411d6f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1dea0f5ce347645c2a4c11098b0ba50.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-25a5764437b9221ae10ad73aa8b84fb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43352f81d6eb7aada886230a057402b5.jpg" align="middle">
</details>




## CLIP-GS: CLIP-Informed Gaussian Splatting for Real-time and   View-consistent 3D Semantic Understanding

**Authors:Guibiao Liao, Jiankun Li, Zhenyu Bao, Xiaoqing Ye, Jingdong Wang, Qing Li, Kanglin Liu**

The recent 3D Gaussian Splatting (GS) exhibits high-quality and real-time synthesis of novel views in 3D scenes. Currently, it primarily focuses on geometry and appearance modeling, while lacking the semantic understanding of scenes. To bridge this gap, we present CLIP-GS, which integrates semantics from Contrastive Language-Image Pre-Training (CLIP) into Gaussian Splatting to efficiently comprehend 3D environments without annotated semantic data. In specific, rather than straightforwardly learning and rendering high-dimensional semantic features of 3D Gaussians, which significantly diminishes the efficiency, we propose a Semantic Attribute Compactness (SAC) approach. SAC exploits the inherent unified semantics within objects to learn compact yet effective semantic representations of 3D Gaussians, enabling highly efficient rendering (>100 FPS). Additionally, to address the semantic ambiguity, caused by utilizing view-inconsistent 2D CLIP semantics to supervise Gaussians, we introduce a 3D Coherent Self-training (3DCS) strategy, resorting to the multi-view consistency originated from the 3D model. 3DCS imposes cross-view semantic consistency constraints by leveraging refined, self-predicted pseudo-labels derived from the trained 3D Gaussian model, thereby enhancing precise and view-consistent segmentation results. Extensive experiments demonstrate that our method remarkably outperforms existing state-of-the-art approaches, achieving improvements of 17.29% and 20.81% in mIoU metric on Replica and ScanNet datasets, respectively, while maintaining real-time rendering speed. Furthermore, our approach exhibits superior performance even with sparse input data, verifying the robustness of our method. 

[PDF](http://arxiv.org/abs/2404.14249v1) https://github.com/gbliao/CLIP-GS

**Summary**
CLIP-GS å°†è¯­ä¹‰ä¿¡æ¯èå…¥é«˜æ–¯æ–‘ç‚¹æ¸²æŸ“ä¸­ï¼Œå®ç°äº†é«˜æ•ˆçš„ 3D åœºæ™¯ç†è§£ï¼Œåœ¨ä¸ä½¿ç”¨å¸¦æ³¨é‡Šçš„è¯­ä¹‰æ•°æ®çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„åˆ†å‰²æ€§èƒ½ã€‚

**Key Takeaways**
- CLIP-GS å°†è¯­ä¹‰ä¿¡æ¯ä» CLIP é›†æˆåˆ°é«˜æ–¯æ–‘ç‚¹æ¸²æŸ“ä¸­ï¼Œå®ç°å¯¹ 3D åœºæ™¯çš„è¯­ä¹‰ç†è§£ã€‚
- è¯­ä¹‰å±æ€§ç´§å‡‘æ€§ (SAC) æ–¹æ³•å­¦ä¹ ç´§å‡‘ä¸”æœ‰æ•ˆçš„è¯­ä¹‰è¡¨ç¤ºï¼Œå®ç°é«˜æ•ˆæ¸²æŸ“ã€‚
- 3D ä¸€è‡´è‡ªè®­ç»ƒ (3DCS) ç­–ç•¥è§£å†³ç”±è§†å›¾ä¸ä¸€è‡´çš„ 2D CLIP è¯­ä¹‰ç›‘ç£é€ æˆçš„è¯­ä¹‰æ­§ä¹‰ã€‚
- 3DCS åˆ©ç”¨ç»è¿‡å¾®è°ƒçš„ 3D é«˜æ–¯æ¨¡å‹é¢„æµ‹çš„ä¼ªæ ‡ç­¾ï¼ŒåŠ å¼ºè·¨è§†å›¾è¯­ä¹‰ä¸€è‡´æ€§çº¦æŸã€‚
- CLIP-GS åœ¨ Replica å’Œ ScanNet æ•°æ®é›†ä¸Šåˆ†åˆ«æé«˜ mIoU æŒ‡æ ‡ 17.29% å’Œ 20.81%ï¼Œè¶…è¶Šç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚
- CLIP-GS å³ä½¿åœ¨ç¨€ç–è¾“å…¥æ•°æ®çš„æƒ…å†µä¸‹ä¹Ÿèƒ½è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶é²æ£’æ€§ã€‚
- CLIP-GS å®æ—¶æ¸²æŸ“é€Ÿåº¦å¿«ï¼Œå¯ç”¨äºäº¤äº’å¼ 3D åœºæ™¯æ¢ç´¢å’Œç¼–è¾‘ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>
<p>Title: CLIP-GS: CLIP-å¼•å¯¼çš„é«˜æ–¯æ³¼æº…ï¼Œç”¨äºå®æ—¶ä¸”è§†å›¾ä¸€è‡´çš„ä¸‰ç»´è¯­ä¹‰ç†è§£</p>
</li>
<li>
<p>Authors: Guibiao Liao, Jiankun Li, Zhenyu Bao, Xiaoqing Ye, Jingdong Wang, Qing Li, Kanglin Liu</p>
</li>
<li>
<p>Affiliation: åŒ—äº¬å¤§å­¦</p>
</li>
<li>
<p>Keywords: 3D é«˜æ–¯æ³¼æº… Â· å®æ—¶ Â· è§†å›¾ä¸€è‡´ Â· ä¸‰ç»´åœºæ™¯è¯­ä¹‰ç†è§£ Â· ä¸‰ç»´åœºæ™¯é‡å»º Â· ç¨€ç–è§†å›¾</p>
</li>
<li>
<p>Urls: https://arxiv.org/abs/2404.14249 , Github: None</p>
</li>
<li>
<p>Summary: </p>
<pre><code>            (1): è¿‘æœŸæå‡ºçš„ä¸‰ç»´é«˜æ–¯æ³¼æº…ï¼ˆ3DGSï¼‰åœ¨ä¸‰ç»´åœºæ™¯ä¸­å±•ç°äº†é«˜è´¨é‡ä¸”å®æ—¶çš„å…¨æ–°è§†å›¾åˆæˆã€‚ç›®å‰å®ƒä¸»è¦å…³æ³¨äºå‡ ä½•å’Œå¤–è§‚å»ºæ¨¡ï¼Œè€Œç¼ºå°‘å¯¹åœºæ™¯çš„è¯­ä¹‰ç†è§£ã€‚

            (2): ç°æœ‰æ–¹æ³•ä¸»è¦æœ‰ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFsï¼‰å’Œä¸‰ç»´é«˜æ–¯æ³¼æº…ï¼ˆ3DGSï¼‰ã€‚å‰è€…åœ¨æ¸²æŸ“åŒ…å«å‡ ä½•å’Œå¤–è§‚ç»†èŠ‚çš„æ–°è§†è§’æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç¼ºä¹å¯¹ä¸‰ç»´åœºæ™¯çš„å…¨é¢è¯­ä¹‰ç†è§£ï¼›åè€…åˆ™ä¸»è¦å…³æ³¨å‡ ä½•å’Œå¤–è§‚å»ºæ¨¡ï¼Œè€Œå¿½ç•¥äº†è¯­ä¹‰ä¿¡æ¯ã€‚

            (3): æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º CLIP-GS çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†æ¥è‡ªå¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒ (CLIP) çš„è¯­ä¹‰ä¿¡æ¯æ•´åˆåˆ°é«˜æ–¯æ³¼æº…ä¸­ï¼Œä»è€Œåœ¨æ²¡æœ‰æ³¨é‡Šè¯­ä¹‰æ•°æ®çš„æƒ…å†µä¸‹æœ‰æ•ˆåœ°ç†è§£ä¸‰ç»´ç¯å¢ƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¯­ä¹‰å±æ€§ç´§å‡‘æ€§ (SAC) æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å¯¹è±¡å†…å›ºæœ‰çš„ç»Ÿä¸€è¯­ä¹‰æ¥å­¦ä¹ é«˜æ–¯æ³¼æº…çš„ç´§å‡‘ä¸”æœ‰æ•ˆçš„è¯­ä¹‰è¡¨ç¤ºï¼Œä»è€Œå®ç°é«˜æ•ˆæ¸²æŸ“ï¼ˆ&gt;100 FPSï¼‰ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³è¯­ä¹‰æ­§ä¹‰é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä¸‰ç»´è¿è´¯è‡ªè®­ç»ƒ (3DCS) ç­–ç•¥ï¼Œåˆ©ç”¨ä¸‰ç»´æ¨¡å‹äº§ç”Ÿçš„å¤šè§†å›¾ä¸€è‡´æ€§ã€‚3DCS é€šè¿‡åˆ©ç”¨ä»è®­ç»ƒå¥½çš„ä¸‰ç»´é«˜æ–¯æ¨¡å‹ä¸­è·å¾—çš„ç»è¿‡ä¼˜åŒ–ä¸”è‡ªæˆ‘é¢„æµ‹çš„ä¼ªæ ‡ç­¾æ¥æ–½åŠ è·¨è§†å›¾è¯­ä¹‰ä¸€è‡´æ€§çº¦æŸï¼Œä»è€Œå¢å¼ºç²¾ç¡®ä¸”è§†å›¾ä¸€è‡´çš„åˆ†å‰²ç»“æœã€‚

            (4): åœ¨ Replica å’Œ ScanNet æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨ mIoU æŒ‡æ ‡ä¸Šåˆ†åˆ«æ¯”ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•æé«˜äº† 17.29% å’Œ 20.81%ï¼ŒåŒæ—¶ä¿æŒäº†å®æ—¶çš„æ¸²æŸ“é€Ÿåº¦ã€‚æ­¤å¤–ï¼Œå³ä½¿åœ¨è¾“å…¥æ•°æ®ç¨€ç–çš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•ä¹Ÿè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶é²æ£’æ€§ã€‚
</code></pre>
</li>
<li>
<p>æ–¹æ³•ï¼š</p>
<pre><code>            (1):è¯­ä¹‰å±æ€§ç´§å‡‘æ€§ï¼ˆSACï¼‰ï¼šåˆ©ç”¨å¯¹è±¡å†…å›ºæœ‰çš„ç»Ÿä¸€è¯­ä¹‰æ¥å­¦ä¹ é«˜æ–¯æ³¼æº…çš„ç´§å‡‘ä¸”æœ‰æ•ˆçš„è¯­ä¹‰è¡¨ç¤ºï¼Œå®ç°é«˜æ•ˆæ¸²æŸ“ã€‚

            (2):3Dè¿è´¯è‡ªè®­ç»ƒï¼ˆ3DCSï¼‰ï¼šåˆ©ç”¨ä¸‰ç»´æ¨¡å‹äº§ç”Ÿçš„å¤šè§†å›¾ä¸€è‡´æ€§ï¼Œæ–½åŠ è·¨è§†å›¾è¯­ä¹‰ä¸€è‡´æ€§çº¦æŸï¼Œå¢å¼ºç²¾ç¡®ä¸”è§†å›¾ä¸€è‡´çš„åˆ†å‰²ç»“æœã€‚

            (3):æ•´ä½“è®­ç»ƒè¿‡ç¨‹ï¼šäº¤æ›¿ä¼˜åŒ–3Dé«˜æ–¯æ³¼æº…å’Œè¯­ä¹‰è¡¨ç¤ºï¼ŒåŒæ—¶åˆ©ç”¨3DCSæ–½åŠ è§†å›¾ä¸€è‡´æ€§çº¦æŸã€‚
</code></pre>
</li>
<li>
<p>ç»“è®ºï¼š</p>
</li>
</ol>
<p>ï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº† CLIP-GSï¼Œä¸€ç§åˆ©ç”¨é«˜æ–¯æ³¼æº…å®ç°ä¸‰ç»´åœºæ™¯å®æ—¶ä¸”ç²¾ç¡®è¯­ä¹‰ç†è§£çš„æ–°æ–¹æ³•ã€‚åœ¨ CLIP-GS ä¸­ï¼Œè¯­ä¹‰å±æ€§ç´§å‡‘æ€§ï¼ˆSACï¼‰å°†ç´§å‡‘çš„è¯­ä¹‰ä¿¡æ¯é™„åŠ åˆ°ä¸‰ç»´é«˜æ–¯ä½“ä¸­ä»¥é«˜æ•ˆåœ°è¡¨ç¤ºä¸‰ç»´è¯­ä¹‰ï¼Œç¡®ä¿é«˜æ•ˆæ¸²æŸ“ã€‚æ­¤å¤–ï¼Œæå‡ºçš„ä¸‰ç»´è¿è´¯è‡ªè®­ç»ƒï¼ˆ3DCSï¼‰å¢å¼ºäº†ä¸åŒè§†å›¾ä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ï¼Œä»è€Œäº§ç”Ÿäº†å‡†ç¡®çš„ä¸‰ç»´åˆ†å‰²ç»“æœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®åœºæ™¯ä¸­æ˜æ˜¾ä¼˜äº SOTA æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå³ä½¿è¾“å…¥æ•°æ®ç¨€ç–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¹Ÿè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„ä¸‰ç»´è¯­ä¹‰å­¦ä¹ æ–¹æ³•çš„é²æ£’æ€§ã€‚</p>
<p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§å°† CLIP è¯­ä¹‰ä¿¡æ¯æ•´åˆåˆ°é«˜æ–¯æ³¼æº…ä¸­çš„æ–¹æ³•ï¼Œå®ç°äº†ä¸‰ç»´åœºæ™¯çš„å®æ—¶è¯­ä¹‰ç†è§£ã€‚åˆ›æ–°æ€§åœ°æå‡ºäº†è¯­ä¹‰å±æ€§ç´§å‡‘æ€§ï¼ˆSACï¼‰å’Œä¸‰ç»´è¿è´¯è‡ªè®­ç»ƒï¼ˆ3DCSï¼‰ä¸¤ç§æŠ€æœ¯ï¼Œåˆ†åˆ«ç”¨äºé«˜æ•ˆè¯­ä¹‰è¡¨ç¤ºå’Œè·¨è§†å›¾è¯­ä¹‰ä¸€è‡´æ€§å¢å¼ºã€‚</p>
<p>æ€§èƒ½ï¼šåœ¨ Replica å’Œ ScanNet æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨ mIoU æŒ‡æ ‡ä¸Šåˆ†åˆ«æ¯”ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•æé«˜äº† 17.29% å’Œ 20.81%ï¼ŒåŒæ—¶ä¿æŒäº†å®æ—¶çš„æ¸²æŸ“é€Ÿåº¦ã€‚å³ä½¿åœ¨è¾“å…¥æ•°æ®ç¨€ç–çš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•ä¹Ÿè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶é²æ£’æ€§ã€‚</p>
<p>å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•éœ€è¦é¢„è®­ç»ƒ CLIP æ¨¡å‹å’Œä¸‰ç»´é«˜æ–¯æ³¼æº…æ¨¡å‹ï¼Œè®­ç»ƒè¿‡ç¨‹éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºã€‚</p>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d36db5fceba666ce511b0cf595bc769d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a5ca926d7e6577c4c1a0e8076537a17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef11b21fc83f3602f91a29eea9ff097e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-54108038b1e285d6be885cd6288e500c.jpg" align="middle">
</details>




## GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting

**Authors:Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu**

Recent works on audio-driven talking head synthesis using Neural Radiance Fields (NeRF) have achieved impressive results. However, due to inadequate pose and expression control caused by NeRF implicit representation, these methods still have some limitations, such as unsynchronized or unnatural lip movements, and visual jitter and artifacts. In this paper, we propose GaussianTalker, a novel method for audio-driven talking head synthesis based on 3D Gaussian Splatting. With the explicit representation property of 3D Gaussians, intuitive control of the facial motion is achieved by binding Gaussians to 3D facial models. GaussianTalker consists of two modules, Speaker-specific Motion Translator and Dynamic Gaussian Renderer. Speaker-specific Motion Translator achieves accurate lip movements specific to the target speaker through universalized audio feature extraction and customized lip motion generation. Dynamic Gaussian Renderer introduces Speaker-specific BlendShapes to enhance facial detail representation via a latent pose, delivering stable and realistic rendered videos. Extensive experimental results suggest that GaussianTalker outperforms existing state-of-the-art methods in talking head synthesis, delivering precise lip synchronization and exceptional visual quality. Our method achieves rendering speeds of 130 FPS on NVIDIA RTX4090 GPU, significantly exceeding the threshold for real-time rendering performance, and can potentially be deployed on other hardware platforms. 

[PDF](http://arxiv.org/abs/2404.14037v1) 

**Summary:**
é«˜æ–¯ä½“ç´ æ¸²æŸ“æ³•èµ‹äºˆ3Dé«˜æ–¯ä½“ç´ æ˜¾å¼è¡¨ç¤ºç‰¹æ€§ï¼Œå¯å®ç°ç›´è§‚çš„é¢éƒ¨åŠ¨ä½œæ§åˆ¶ï¼Œå¤§å¹…æå‡éŸ³é¢‘é©±åŠ¨è™šæ‹ŸåŒ–èº«åˆæˆæ•ˆæœã€‚

**Key Takeaways:**
- åŸºäº3Dé«˜æ–¯ä½“ç´ çš„éŸ³é¢‘é©±åŠ¨è™šæ‹ŸåŒ–èº«åˆæˆæ–¹æ³•ã€‚
- é¢éƒ¨è¿åŠ¨é€šè¿‡å°†é«˜æ–¯ä½“ç´ ç»‘å®šåˆ°3Dé¢éƒ¨æ¨¡å‹å®ç°ç›´è§‚æ§åˆ¶ã€‚
- è¯´è¯äººç‰¹å®šè¿åŠ¨ç¿»è¯‘å™¨å®ç°é’ˆå¯¹ç‰¹å®šè¯´è¯äººçš„ç²¾ç¡®å”‡éƒ¨è¿åŠ¨ã€‚
- åŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨å¼•å…¥è¯´è¯äººç‰¹å®šæ··åˆå½¢çŠ¶ä»¥å¢å¼ºé¢éƒ¨ç»†èŠ‚è¡¨ç¤ºã€‚
- å¹¿æ³›çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¯­éŸ³åˆæˆæ–¹é¢ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚
- æ¸²æŸ“é€Ÿåº¦è¾¾åˆ°130 FPSï¼Œè¿œè¶…å®æ—¶æ¸²æŸ“æ€§èƒ½çš„é˜ˆå€¼ã€‚
- å…·æœ‰åœ¨å…¶ä»–ç¡¬ä»¶å¹³å°ä¸Šéƒ¨ç½²çš„æ½œåŠ›ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>
<p>Title: GaussianTalker: Speaker-specific Talking Head Synthesis (åŸºäº 3D é«˜æ–¯ç‚¹äº‘çš„è¯´è¯äººç‰¹å®šè¯´è¯å¤´åˆæˆ)</p>
</li>
<li>
<p>Authors: Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</p>
</li>
<li>
<p>Affiliation: Alibaba Group (é˜¿é‡Œå·´å·´é›†å›¢)</p>
</li>
<li>
<p>Keywords: Audio-driven talking head synthesis, 3D Gaussian Splatting, Speaker-specific motion, Dynamic Gaussian Renderer</p>
</li>
<li>
<p>Urls: Paper: https://arxiv.org/pdf/2404.14037.pdf, Github: None</p>
</li>
<li>
<p>Summary:</p>
<p>(1): Recent audio-driven talking head synthesis methods based on Neural Radiance Fields (NeRF) have achieved impressive results, but suffer from inadequate pose and expression control due to NeRF's implicit representation, leading to unsynchronized or unnatural lip movements and visual jitter and artifacts.</p>
<p>(2): Past methods: NeRF-based audio-driven talking head synthesis methods. Problems: Inadequate pose and expression control, resulting in unsynchronized or unnatural lip movements and visual jitter and artifacts. Well motivated: Yes, as it addresses the limitations of existing methods.</p>
<p>(3): GaussianTalker: A novel method for audio-driven talking head synthesis based on 3D Gaussian Splatting. It consists of two modules: Speaker-specific Motion Translator and Dynamic Gaussian Renderer. The former achieves accurate lip movements specific to the target speaker through universalized audio feature extraction and customized lip motion generation. The latter introduces Speaker-specific BlendShapes to control the facial expressions and generates high-quality talking head videos with precise lip movements.</p>
<p>(4): On the task of audio-driven talking head synthesis, GaussianTalker achieves state-of-the-art results. It can generate high-quality talking head videos with precise lip movements and natural facial expressions. The performance supports the goals of the paper, which are to address the limitations of existing methods and achieve more realistic and expressive talking head synthesis.</p>
</li>
<li>
<p>æ–¹æ³•ï¼š</p>
<p>(1)ï¼šåŸºäº3Dé«˜æ–¯ç‚¹äº‘çš„è¯´è¯äººç‰¹å®šè¯´è¯å¤´åˆæˆï¼›</p>
<p>(2)ï¼šæå‡ºSpeaker-specific Motion Translatorå’ŒDynamic Gaussian Rendererä¸¤ä¸ªæ¨¡å—ï¼›</p>
<p>(3)ï¼šSpeaker-specific Motion Translatoré€šè¿‡é€šç”¨éŸ³é¢‘ç‰¹å¾æå–å’Œå®šåˆ¶å”‡éƒ¨åŠ¨ä½œç”Ÿæˆå®ç°ç‰¹å®šäºç›®æ ‡è¯´è¯äººçš„å‡†ç¡®å”‡éƒ¨åŠ¨ä½œï¼›</p>
<p>(4)ï¼šDynamic Gaussian Rendererå¼•å…¥è¯´è¯äººç‰¹å®šBlendShapesæ¥æ§åˆ¶é¢éƒ¨è¡¨æƒ…ï¼Œå¹¶ç”Ÿæˆå…·æœ‰ç²¾ç¡®å”‡éƒ¨åŠ¨ä½œçš„é«˜è´¨é‡è¯´è¯å¤´è§†é¢‘ï¼›</p>
<p>.......</p>
</li>
</ol>
<p><strong>ç»“è®º</strong></p>
<p>ï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡º GaussianTalkerï¼Œä¸€ç§é€šè¿‡ 3D é«˜æ–¯ç‚¹äº‘é›†æˆ FLAME æ¨¡å‹çš„éŸ³é¢‘é©±åŠ¨è¯´è¯å¤´åˆæˆæ–°æ¡†æ¶ã€‚GaussianTalker å°†å¤šæ¨¡æ€æ•°æ®ä¸ç‰¹å®šè¯´è¯è€…å…³è”ï¼Œå‡å°‘äº†éŸ³é¢‘ã€3D ç½‘æ ¼å’Œè§†é¢‘ä¹‹é—´çš„æ½œåœ¨èº«ä»½åå·®ã€‚Speaker-specific FLAME Translator é‡‡ç”¨èº«ä»½è§£è€¦å’Œä¸ªæ€§åŒ–åµŒå…¥æ¥å®ç°åŒæ­¥å’Œè‡ªç„¶çš„å”‡éƒ¨åŠ¨ä½œï¼Œè€Œ Dynamic Gaussian Renderer é€šè¿‡æ½œåœ¨å§¿åŠ¿ç»†åŒ–é«˜æ–¯å±æ€§ï¼Œä»¥å®ç°ç¨³å®šå’Œé€¼çœŸçš„æ¸²æŸ“ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGaussianTalker åœ¨è¯´è¯å¤´åˆæˆæ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶å®ç°äº†è¶…é«˜çš„æ¸²æŸ“é€Ÿåº¦ï¼Œæ˜æ˜¾è¶…è¿‡å…¶ä»–æ–¹æ³•ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œè¿™ç§åˆ›æ–°æ–¹æ³•å°†é¼“åŠ±æœªæ¥çš„ç ”ç©¶å¼€å‘æ›´æµç•…ã€æ›´é€¼çœŸçš„è§’è‰²è¡¨æƒ…å’ŒåŠ¨ä½œã€‚é€šè¿‡åˆ©ç”¨å…ˆè¿›çš„é«˜æ–¯æ¨¡å‹å’Œç”ŸæˆæŠ€æœ¯ï¼Œè§’è‰²åŠ¨ç”»å°†è¿œè¿œè¶…å‡ºç®€å•çš„å”‡å½¢åŒæ­¥ï¼Œæ•æ‰æ›´å¹¿æ³›çš„è§’è‰²åŠ¨æ€ã€‚</p>
<p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šåŸºäº 3D é«˜æ–¯ç‚¹äº‘çš„è¯´è¯äººç‰¹å®šè¯´è¯å¤´åˆæˆï¼›Speaker-specific Motion Translator å’Œ Dynamic Gaussian Renderer ä¸¤ä¸ªæ¨¡å—ï¼›
æ€§èƒ½ï¼šåœ¨è¯´è¯å¤´åˆæˆæ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå®ç°äº†è¶…é«˜çš„æ¸²æŸ“é€Ÿåº¦ï¼›
å·¥ä½œé‡ï¼š.......</p>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f53af9ef57ed25d0699b508f7b856061.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-00e62c0d66ff2641b9803987918d6fd0.jpg" align="middle">
</details>




## Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular   Videos

**Authors:Isabella Liu, Hao Su, Xiaolong Wang**

Modern 3D engines and graphics pipelines require mesh as a memory-efficient representation, which allows efficient rendering, geometry processing, texture editing, and many other downstream operations. However, it is still highly difficult to obtain high-quality mesh in terms of structure and detail from monocular visual observations. The problem becomes even more challenging for dynamic scenes and objects. To this end, we introduce Dynamic Gaussians Mesh (DG-Mesh), a framework to reconstruct a high-fidelity and time-consistent mesh given a single monocular video. Our work leverages the recent advancement in 3D Gaussian Splatting to construct the mesh sequence with temporal consistency from a video. Building on top of this representation, DG-Mesh recovers high-quality meshes from the Gaussian points and can track the mesh vertices over time, which enables applications such as texture editing on dynamic objects. We introduce the Gaussian-Mesh Anchoring, which encourages evenly distributed Gaussians, resulting better mesh reconstruction through mesh-guided densification and pruning on the deformed Gaussians. By applying cycle-consistent deformation between the canonical and the deformed space, we can project the anchored Gaussian back to the canonical space and optimize Gaussians across all time frames. During the evaluation on different datasets, DG-Mesh provides significantly better mesh reconstruction and rendering than baselines. Project page: https://www.liuisabella.com/DG-Mesh/ 

[PDF](http://arxiv.org/abs/2404.12379v2) Project page: https://www.liuisabella.com/DG-Mesh/

**Summary**
å•ç›®è§†é¢‘é‡å»ºé«˜ä¿çœŸåŠ¨æ€ç½‘æ ¼çš„æ¡†æ¶

**Key Takeaways**
- åŠ¨æ€é«˜æ–¯ç½‘æ ¼ï¼ˆDG-Meshï¼‰é€šè¿‡å•ç›®è§†é¢‘é‡å»ºå‡ºé«˜ä¿çœŸæ—¶åºä¸€è‡´çš„ç½‘æ ¼ã€‚
- åˆ©ç”¨ 3D é«˜æ–¯ç‚¹äº‘æ„æˆå…·æœ‰æ—¶åºä¸€è‡´æ€§çš„ç½‘æ ¼åºåˆ—ã€‚
- é«˜æ–¯ç‚¹äº‘æ¢å¤é«˜è´¨é‡ç½‘æ ¼ï¼Œå¹¶å®ç°ç½‘æ ¼é¡¶ç‚¹çš„æ—¶åºè·Ÿè¸ªï¼Œå¯ç”¨äºåŠ¨æ€å¯¹è±¡çº¹ç†ç¼–è¾‘ã€‚
- é«˜æ–¯ç½‘æ ¼é”šå®šå¯ä½¿é«˜æ–¯åˆ†å¸ƒå‡åŒ€ï¼Œé€šè¿‡ç½‘æ ¼å¼•å¯¼å˜å½¢é«˜æ–¯çš„é«˜å¯†åº¦åŒ–å’Œå‰ªæï¼Œæå‡ç½‘æ ¼é‡å»ºè´¨é‡ã€‚
- é€šè¿‡è§„èŒƒç©ºé—´å’Œå˜å½¢ç©ºé—´çš„å¾ªç¯ä¸€è‡´æ€§å˜å½¢ï¼Œå°†é”šå®šçš„é«˜æ–¯æŠ•å°„å›è§„èŒƒç©ºé—´ï¼Œå¹¶åœ¨æ‰€æœ‰æ—¶é—´å¸§ä¼˜åŒ–é«˜æ–¯ã€‚
- åœ¨ä¸åŒæ•°æ®é›†ä¸Šè¯„ä¼°åï¼ŒDG-Mesh åœ¨ç½‘æ ¼é‡å»ºå’Œæ¸²æŸ“æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>
<p>Title: åŠ¨æ€é«˜æ–¯ç½‘æ ¼ï¼šå•ç›®è§†é¢‘ä¸­ä¸€è‡´çš„ç½‘æ ¼é‡å»º</p>
</li>
<li>
<p>Authors: Isabella Liu, Hao Su, Xiaolong Wang</p>
</li>
<li>
<p>Affiliation: åŠ å·å¤§å­¦åœ£åœ°äºšå“¥åˆ†æ ¡</p>
</li>
<li>
<p>Keywords: 3D Reconstruction, Monocular Video, Dynamic Mesh, Gaussian Splatting</p>
</li>
<li>
<p>Urls: Paper: https://arxiv.org/abs/2404.12379, Github: None</p>
</li>
<li>
<p>Summary:</p>
<p>(1): ç°ä»£ 3D å¼•æ“å’Œå›¾å½¢ç®¡é“éœ€è¦ç½‘æ ¼ä½œä¸ºä¸€ç§å†…å­˜é«˜æ•ˆçš„è¡¨ç¤ºï¼Œå®ƒå…è®¸é«˜æ•ˆæ¸²æŸ“ã€å‡ ä½•å¤„ç†ã€çº¹ç†ç¼–è¾‘å’Œè®¸å¤šå…¶ä»–ä¸‹æ¸¸æ“ä½œã€‚ç„¶è€Œï¼Œä»å•ç›®è§†è§‰è§‚å¯Ÿä¸­è·å¾—ç»“æ„å’Œç»†èŠ‚æ–¹é¢çš„é«˜è´¨é‡ç½‘æ ¼ä»ç„¶éå¸¸å›°éš¾ã€‚å¯¹äºåŠ¨æ€åœºæ™¯å’Œå¯¹è±¡ï¼Œè¿™ä¸ªé—®é¢˜å˜å¾—æ›´å…·æŒ‘æˆ˜æ€§ã€‚</p>
<p>(2): è¿‡å»çš„æ–¹æ³•åŒ…æ‹¬ä½¿ç”¨ 3D é«˜æ–¯æ–‘ç‚¹æ„å»ºç½‘æ ¼åºåˆ—ï¼Œä½†è¿™äº›æ–¹æ³•åœ¨å¤„ç†åŠ¨æ€åœºæ™¯æ—¶å­˜åœ¨å±€é™æ€§ã€‚</p>
<p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºåŠ¨æ€é«˜æ–¯ç½‘æ ¼ï¼ˆDG-Meshï¼‰çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ 3D é«˜æ–¯æ–‘ç‚¹è¡¨ç¤ºï¼Œå¹¶é€šè¿‡é«˜æ–¯ç½‘æ ¼é”šå®šç®—æ³•æ¢å¤é«˜è´¨é‡ç½‘æ ¼ï¼Œä»è€Œå®ç°æ—¶é—´ä¸€è‡´çš„ç½‘æ ¼åºåˆ—é‡å»ºã€‚</p>
<p>(4): åœ¨ D-NeRF æ•°æ®é›†å’Œ DG-Mesh æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨ç½‘æ ¼é‡å»ºä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
</li>
<li>
<p>æ–¹æ³•ï¼š</p>
<p>ï¼ˆ1ï¼‰ï¼šæå‡ºäº†ä¸€ç§åä¸ºåŠ¨æ€é«˜æ–¯ç½‘æ ¼ï¼ˆDG-Meshï¼‰çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ 3D é«˜æ–¯æ–‘ç‚¹è¡¨ç¤ºï¼Œå¹¶é€šè¿‡é«˜æ–¯ç½‘æ ¼é”šå®šç®—æ³•æ¢å¤é«˜è´¨é‡ç½‘æ ¼ï¼Œä»è€Œå®ç°æ—¶é—´ä¸€è‡´çš„ç½‘æ ¼åºåˆ—é‡å»ºï¼›</p>
<p>ï¼ˆ2ï¼‰ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªç½‘æ ¼æ„å»ºæ¨¡å—ï¼Œè¯¥æ¨¡å—ä½¿ç”¨ 3D é«˜æ–¯æ–‘ç‚¹è¡¨ç¤ºæ¥æ„å»ºç½‘æ ¼åºåˆ—ï¼Œä»¥åŠä¸€ä¸ªç½‘æ ¼é”šå®šæ¨¡å—ï¼Œè¯¥æ¨¡å—å°†ç½‘æ ¼åºåˆ—ä¸­çš„ç½‘æ ¼é”šå®šåˆ°ä¸–ç•Œåæ ‡ç³»ä¸­ï¼›</p>
<p>ï¼ˆ3ï¼‰ï¼šç½‘æ ¼æ„å»ºæ¨¡å—åˆ©ç”¨ 3D é«˜æ–¯æ–‘ç‚¹è¡¨ç¤ºæ¥è¡¨ç¤ºåœºæ™¯ä¸­çš„å‡ ä½•å½¢çŠ¶ï¼Œå¹¶ä½¿ç”¨é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰æ¥ä¼°è®¡ç½‘æ ¼åºåˆ—ä¸­çš„ç½‘æ ¼ï¼›</p>
<p>ï¼ˆ4ï¼‰ï¼šç½‘æ ¼é”šå®šæ¨¡å—åˆ©ç”¨é«˜æ–¯ç½‘æ ¼é”šå®šç®—æ³•å°†ç½‘æ ¼åºåˆ—ä¸­çš„ç½‘æ ¼é”šå®šåˆ°ä¸–ç•Œåæ ‡ç³»ä¸­ï¼Œè¯¥ç®—æ³•ä½¿ç”¨é«˜æ–¯æ–‘ç‚¹ä¹‹é—´çš„å‡ ä½•å…³ç³»æ¥ä¼°è®¡ç½‘æ ¼çš„ä½å§¿ï¼›</p>
<p>ï¼ˆ5ï¼‰ï¼šè¯¥æ¡†æ¶é€šè¿‡è¿­ä»£ä¼˜åŒ–ç½‘æ ¼æ„å»ºæ¨¡å—å’Œç½‘æ ¼é”šå®šæ¨¡å—ä¸­çš„å‚æ•°æ¥å®ç°æ—¶é—´ä¸€è‡´çš„ç½‘æ ¼åºåˆ—é‡å»ºã€‚</p>
</li>
<li>
<p>ç»“è®ºï¼š</p>
</li>
</ol>
<p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºåŠ¨æ€é«˜æ–¯ç½‘æ ¼ï¼ˆDG-Meshï¼‰çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ 3D é«˜æ–¯æ–‘ç‚¹è¡¨ç¤ºï¼Œå¹¶é€šè¿‡é«˜æ–¯ç½‘æ ¼é”šå®šç®—æ³•æ¢å¤é«˜è´¨é‡ç½‘æ ¼ï¼Œä»è€Œå®ç°æ—¶é—´ä¸€è‡´çš„ç½‘æ ¼åºåˆ—é‡å»ºã€‚DG-Mesh åœ¨ç½‘æ ¼é‡å»ºä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
<p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šåˆ©ç”¨ 3D é«˜æ–¯æ–‘ç‚¹è¡¨ç¤ºå’Œé«˜æ–¯ç½‘æ ¼é”šå®šç®—æ³•å®ç°æ—¶é—´ä¸€è‡´çš„ç½‘æ ¼åºåˆ—é‡å»ºï¼›
æ€§èƒ½ï¼šåœ¨ç½‘æ ¼é‡å»ºä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼›
å·¥ä½œé‡ï¼šéœ€è¦å‡†ç¡®çš„å¯¹è±¡åˆ†å‰²å’Œè·Ÿè¸ªï¼Œåœ¨å¤„ç†å¤§æ‹“æ‰‘å˜åŒ–æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</p>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bb786e92e4a68c16900a6443568566f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84e4142556dfc3bb4d97a20772986995.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2de77e2437a64b7bd107f95e76669404.jpg" align="middle">
</details>




