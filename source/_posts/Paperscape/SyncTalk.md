# SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis



Paper   : [https://arxiv.org/abs/2311.17590](https://arxiv.org/abs/2311.17590)

Project : [https://ziqiaopeng.github.io/synctalk/](https://ziqiaopeng.github.io/synctalk/)

Video	: [https://ziqiaopeng.github.io/synctalk/#teaser](https://ziqiaopeng.github.io/synctalk/#teaser)

Code    : [https://github.com/ziqiaopeng/SyncTalk](https://github.com/ziqiaopeng/SyncTalk)



**摘要**

神经辐射场 - 生成对抗网络框架用于实现说话人头部视频的同步合成。

（1）研究背景： 生成逼真的、由语音驱动的谈话头部视频是一项具有挑战性的任务。传统生成对抗网络（GAN）难以保持一致的面部身份，而神经辐射场（NeRF）方法虽然可以解决这个问题，但通常会产生不匹配的唇部动作、不充分的面部表情和不稳定的头部姿势。一个逼真的谈话头部需要同步协调主体身份、唇部动作、面部表情和头部姿势。缺乏这些同步是导致不真实和人工结果的根本缺陷。 

（2）过去的方法及其问题： GAN 方法难以保持一致的面部身份。NeRF 方法虽然可以解决这个问题，但通常会产生不匹配的唇部动作、不充分的面部表情和不稳定的头部姿势。 

（3）提出的研究方法： SyncTalk 是一种基于 NeRF 的方法，它有效地保持了主体身份，增强了谈话头部合成的同步性和真实性。SyncTalk 使用面部同步控制器将唇部动作与语音对齐，并创新地使用 3D 面部混合形状模型来捕捉准确的面部表情。头部同步稳定器优化头部姿势，实现更自然的头部运动。肖像同步生成器恢复头发细节，并将生成的头部与躯干融合，以获得无缝的视觉体验。

（4）方法在什么任务上取得了什么性能，这些性能是否支持了它们的目标： SyncTalk 在谈话头部合成同步性和真实性方面优于最先进的方法。广泛的实验和用户研究表明，SyncTalk 在同步性和真实性方面优于最先进的方法。

**关键要点**

- 传统生成对抗网络难以维持一致的面部身份。
- 神经辐射场方法可以解决面部身份一致性问题，但经常出现嘴唇运动不匹配、面部表情不足和头部姿势不稳定的问题。
- 逼真的说话人头部视频需要同步协调主体身份、嘴唇运动、面部表情和头部姿势。
- 缺少同步性是导致不真实和人为结果的根本缺陷。
- SyncTalk 是一种基于神经辐射场的方法，有效地保持了主体身份，提高了说话人头部合成中的同步性和真实感。
- SyncTalk 使用面部同步控制器将嘴唇运动与语音对齐，并创新地使用 3D 面部混合形状模型来捕捉准确的面部表情。
- SyncTalk 的头部同步稳定器优化了头部姿势，实现了更自然的头部运动。
- 人像同步生成器恢复头发细节，将生成的头部与躯干融合，以获得无缝的视觉体验。

![SyncTalk](https://github.com/ZiqiaoPeng/SyncTalk/raw/main/assets/image/synctalk.png)





## 介绍

这篇论文中，解决最好的就是同步的问题，所以也称为同步的Devil 魔鬼😈。现有方法在四个关键领域需要更多的同步：**主体身份**、**唇部运动**、**面部表情**和**头部姿势**。

- 首先，在基于GAN的方法中，由于连续帧中特征的不稳定性以及仅使用少量帧作为面部重建参考，保持视频中主体的身份是具有挑战性的。

- 其次，唇部运动与语音不同步。在基于NeRF的方法中，仅基于5分钟语音数据集训练的音频特征难以泛化到不同的语音输入。

- 第三，缺乏面部表情控制，大多数方法只能产生唇部运动或控制眨眼，导致面部动作不自然。

- 第四，头部姿势不同步。

先前的方法依赖于稀疏的landmarks来计算投影误差，但这些landmarks的抖动和不准确性导致头部姿势不稳定。这些同步问题会引入伪影，并显著降低真实感。

为了解决这些同步挑战，引入了SyncTalk，这是一种基于NeRF的方法，专注于高度同步、逼真的、语音驱动的说话头部合成，采用三平面哈希表示来维护主体身份。通过面部同步控制器和头部同步稳定器，SyncTalk显著提高了合成视频的同步性和视觉质量。PortraitSync Generator进一步改善了视觉质量，精心细化了视觉细节。整个渲染过程可以实现50 FPS，并输出高分辨率视频。

| 模块                    | 描述                                                         |
| ----------------------- | ------------------------------------------------------------ |
| Face-Sync Controller    | 在Face-Sync控制器中，预先在2D音频视听数据集上对音频视觉编码器进行预训练，得到了一种通用表示，确保了不同语音样本之间的唇部同步运动。对于控制面部表情，采用了一个语义丰富的3D面部混合形状模型，该模型通过52个参数控制特定的面部表情区域。 |
| Head-Sync Stabilizer    | 在Head-Sync稳定器中，使用AD-NeRF中的头部运动跟踪器来推断头部的粗略旋转和平移参数。由于粗略参数的不稳定性，借鉴了同步定位与地图(SLAM)的思想，结合头部关键点跟踪器跟踪稠密关键点，并采用bundle adjustment method 束调整方法来优化头部姿势，从而实现稳定连续的头部运动。 |
| Portrait-Sync Generator | 为了进一步提高SyncTalk的视觉保真度，设计了一个Portrait-Sync生成器。这个模块修复了NeRF建模中的伪影，特别是头发和背景等细节，输出高分辨率视频。 |

**主要贡献**

- 提出了一个Face-Sync控制器，结合音频视觉编码器和面部动画捕捉器，确保准确的唇部同步和动态面部表情渲染。 
- 引入了一个Head-Sync稳定器，跟踪头部旋转和面部运动关键点。利用束调整方法，该稳定器保证了平滑同步的头部运动。
- 设计了一个Portrait-Sync生成器，通过修复NeRF建模中的伪影和细化头发和背景等细节，提高了视觉保真度。



## 相关工作

**GAN-based Method**

近来，基于GAN的说话头合成成为了计算机视觉中的一个重要研究领域。然而，它们在保持视频中主体的身份一致性方面存在挑战。

例如，Wav2Lip引入了一个唇部同步专家来监督唇部运动。然而，由于使用了来自参考帧的五帧来重建唇部，它难以保持主体的身份。另一些方法尝试进行全脸合成，但往往难以确保面部表情和头部姿势之间的同步。除了视频流技术外，还有一些方法试图通过语音使单张图像“说话”，如SadTalker可以从单张图像生成一个人说话的视频。然而，这些方法无法生成自然的头部姿势和面部表情，难以保持主体的身份，影响了同步效果，导致视觉感知不真实。

与这些方法相比，SyncTalk使用NeRF对人脸进行三维建模。其能够在规范空间中表示连续的3D场景的能力，使其在保持主体身份一致性和保留细节方面表现出色。

**NeRF-based Method**

近来，随着NeRF的崛起，许多领域已开始利用它来解决相关挑战。先前的工作已将NeRF整合到合成说话头像的任务中，并将音频作为驱动信号，但这些方法都是基于普通的NeRF模型。

例如，AD-NeRF需要大约10秒来渲染单个图像。RADNeRF旨在实现实时视频生成，并使用了基于Instant-NGP的NeRF。ER-NeRF通过引入三平面哈希编码器来修剪空白空间区域，提倡紧凑且加速的渲染方法。GeneFace试图通过将语音特征转换为面部标志来减少NeRF的伪影，但这往往导致唇部运动不准确。尝试使用基于NeRF的方法创建角色头像，例如，不能直接由语音驱动。这些方法仅将音频作为条件，没有清晰的同步概念，并且通常导致唇部运动平均。

此外，先前的方法缺乏对面部表情的控制，仅限于控制眨眼，并且无法对抬眉毛或皱眉等动作进行建模。此外，这些方法在头部姿势不稳定方面存在显着问题，导致头部和躯干分离。相比之下，使用Face-Sync控制器来建模音频和唇部运动之间的关系，从而增强唇部运动和表情的同步性，使用Head-Sync稳定器来稳定头部姿势，通过解决这些同步问题，提高了视觉质量。

