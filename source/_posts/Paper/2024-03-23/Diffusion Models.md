
---
title: Diffusion Models
date: 2024-03-23 17:43:11
author: Kedreamix
cover: https://picx.zhimg.com/v2-c8380fe238cf40cd25f36e52373bb013.jpg
categories: Paper
tags:
    - Diffusion Models
description: Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-03-23  GRM Large Gaussian Reconstruction Model for Efficient 3D Reconstruction   and Generation  
keywords: Diffusion Models
toc:
toc_number: false
toc_style_simple: true
copyright:
copyright_author:
copyright_author_href:
copyright_url:
copyright_info:
mathjax: true
katex:
aplayer:
highlight_shrink:
aside:
---

>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹[Gemini-Pro](https://ai.google.dev/)çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨
>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼
>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© [ChatPaperFree](https://github.com/Kedreamix/ChatPaperFree) ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ [HuggingFaceå…è´¹ä½“éªŒ](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)

# 2024-03-23 æ›´æ–°


## GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction   and Generation

**Authors:Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, Gordon Wetzstein**

We introduce GRM, a large-scale reconstructor capable of recovering a 3D asset from sparse-view images in around 0.1s. GRM is a feed-forward transformer-based model that efficiently incorporates multi-view information to translate the input pixels into pixel-aligned Gaussians, which are unprojected to create a set of densely distributed 3D Gaussians representing a scene. Together, our transformer architecture and the use of 3D Gaussians unlock a scalable and efficient reconstruction framework. Extensive experimental results demonstrate the superiority of our method over alternatives regarding both reconstruction quality and efficiency. We also showcase the potential of GRM in generative tasks, i.e., text-to-3D and image-to-3D, by integrating it with existing multi-view diffusion models. Our project website is at: https://justimyhxu.github.io/projects/grm/. 

[PDF](http://arxiv.org/abs/2403.14621v1) Project page: https://justimyhxu.github.io/projects/grm/ Code:   https://github.com/justimyhxu/GRM

**Summary**
GRM ä½¿ç”¨åŸºäº Transformer çš„å‰é¦ˆç¥ç»ç½‘ç»œï¼Œå°†å›¾åƒåƒç´ é«˜æ•ˆè½¬æ¢ä¸ºå¯¹é½åƒç´ çš„é«˜æ–¯åˆ†é‡ï¼Œå†å°†è¿™äº›åˆ†é‡åæŠ•å½±åˆ° 3D åœºæ™¯çš„é«˜æ–¯åˆ†é‡ä¸­ï¼Œä»è€Œå®ç° 3D é‡å»ºã€‚

**Key Takeaways**
- GRM æ˜¯ä¸€ç§å¤§è§„æ¨¡é‡å»ºå™¨ï¼Œèƒ½å¤Ÿåœ¨ 0.1 ç§’å·¦å³ä»ç¨€ç–è§†å›¾å›¾åƒä¸­æ¢å¤ 3D èµ„äº§ã€‚
- GRM é‡‡ç”¨å‰é¦ˆ Transformer æ¶æ„ï¼Œæœ‰æ•ˆæ•´åˆå¤šè§†å›¾ä¿¡æ¯ã€‚
- GRM é€šè¿‡å°†è¾“å…¥åƒç´ è½¬æ¢ä¸ºåƒç´ å¯¹é½çš„é«˜æ–¯åˆ†é‡ï¼Œæé«˜äº†æ•ˆç‡ã€‚
- ä½¿ç”¨ 3D é«˜æ–¯åˆ†é‡å¯ä»¥åˆ›å»ºå¯†é›†åˆ†å¸ƒçš„åœºæ™¯è¡¨ç¤ºã€‚
- GRM åœ¨é‡å»ºè´¨é‡å’Œæ•ˆç‡æ–¹é¢ä¼˜äºæ›¿ä»£æ–¹æ³•ã€‚
- GRM å¯ä»¥é›†æˆåˆ°å¤šè§†å›¾æ‰©æ•£æ¨¡å‹ä¸­ï¼Œç”¨äºæ–‡æœ¬åˆ° 3D å’Œå›¾åƒåˆ° 3D çš„ç”Ÿæˆä»»åŠ¡ã€‚
- GRM é¡¹ç›®ç½‘ç«™ï¼šhttps://justimyhxu.github.io/projects/grm/ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>æ ‡é¢˜ï¼šGRMï¼šç”¨äºé«˜æ•ˆ 3D é‡å»ºå’Œç”Ÿæˆçš„å¤§å‹é«˜æ–¯é‡å»ºæ¨¡å‹</li>
<li>ä½œè€…ï¼šYinghao Xuã€Zifan Shiã€Yifan Wangã€Hansheng Chenã€Ceyuan Yangã€Sida Pengã€Yujun Shenã€Gordon Wetzstein</li>
<li>éš¶å±å•ä½ï¼šæ–¯å¦ç¦å¤§å­¦</li>
<li>å…³é”®è¯ï¼šé«˜æ–¯ä½“ç´ åŒ–ã€3D é‡å»ºã€3D ç”Ÿæˆ</li>
<li>é“¾æ¥ï¼šhttps://arxiv.org/abs/2303.01547
Githubï¼šæ— </li>
<li>æ‘˜è¦ï¼š
ï¼ˆ1ï¼‰<strong>ç ”ç©¶èƒŒæ™¯ï¼š</strong>éšç€ 3D å†…å®¹åœ¨å„ç§åº”ç”¨ä¸­çš„éœ€æ±‚ä¸æ–­å¢é•¿ï¼Œé«˜æ•ˆä¸”é«˜è´¨é‡çš„ 3D é‡å»ºå’Œç”Ÿæˆå˜å¾—è‡³å…³é‡è¦ã€‚ç°æœ‰çš„æ–¹æ³•åœ¨æ•ˆç‡å’Œè´¨é‡æ–¹é¢å­˜åœ¨æƒè¡¡ã€‚
ï¼ˆ2ï¼‰<strong>è¿‡å»çš„æ–¹æ³•ï¼š</strong>ç°æœ‰çš„åŸºäºä½“ç´ çš„æ–¹æ³•åœ¨å¤„ç†å¤æ‚åœºæ™¯æ—¶æ•ˆç‡ä½ä¸‹ï¼Œè€ŒåŸºäºç½‘æ ¼çš„æ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡åœºæ™¯æ—¶å®¹æ˜“å‡ºç°å‡ ä½•å¤±çœŸã€‚
ï¼ˆ3ï¼‰<strong>ç ”ç©¶æ–¹æ³•ï¼š</strong>æœ¬æ–‡æå‡ºäº† GRMï¼Œä¸€ç§åŸºäº Transformer çš„å¤§å‹é‡å»ºå™¨ï¼Œå®ƒå°†è¾“å…¥åƒç´ é«˜æ•ˆåœ°è½¬æ¢ä¸ºåƒç´ å¯¹é½çš„é«˜æ–¯ä½“ï¼Œç„¶åå°†è¿™äº›é«˜æ–¯ä½“æŠ•å½±ä»¥åˆ›å»ºä¸€ç»„å¯†é›†åˆ†å¸ƒçš„ 3D é«˜æ–¯ä½“ï¼Œè¡¨ç¤ºåœºæ™¯ã€‚è¿™ç§æ–¹æ³•ç»“åˆäº† Transformer æ¶æ„å’Œ 3D é«˜æ–¯ä½“çš„ä½¿ç”¨ï¼Œå®ç°äº†ä¸€ä¸ªå¯æ‰©å±•ä¸”é«˜æ•ˆçš„é‡å»ºæ¡†æ¶ã€‚
ï¼ˆ4ï¼‰<strong>æ–¹æ³•æ€§èƒ½ï¼š</strong>åœ¨ç¨€ç–è§†å›¾é‡å»ºå’Œå•å›¾åƒåˆ° 3D ç”Ÿæˆçš„ä»»åŠ¡ä¸Šï¼ŒGRM åœ¨é‡å»ºè´¨é‡å’Œæ•ˆç‡æ–¹é¢éƒ½ä¼˜äºæ›¿ä»£æ–¹æ³•ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³æä¾›ä¸€ç§é«˜æ•ˆä¸”é«˜è´¨é‡çš„ 3D é‡å»ºå’Œç”Ÿæˆæ–¹æ³•ã€‚</li>
</ol>
<p>7.Methodsï¼š
(1) GRMå°†è¾“å…¥åƒç´ é«˜æ•ˆåœ°è½¬æ¢ä¸ºåƒç´ å¯¹é½çš„é«˜æ–¯ä½“ï¼Œç„¶åå°†è¿™äº›é«˜æ–¯ä½“æŠ•å½±ä»¥åˆ›å»ºä¸€ç»„å¯†é›†åˆ†å¸ƒçš„3Dé«˜æ–¯ä½“ï¼Œè¡¨ç¤ºåœºæ™¯ï¼›
(2) GRMä½¿ç”¨Transformeræ¶æ„æ¥å¤„ç†é«˜æ–¯ä½“ï¼Œå¹¶é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶å­¦ä¹ é«˜æ–¯ä½“ä¹‹é—´çš„å…³ç³»ï¼›
(3) GRMä½¿ç”¨å¤šçº§æŠ•å½±ç­–ç•¥ï¼Œé€æ­¥ç»†åŒ–é«˜æ–¯ä½“ï¼Œä»è€Œå®ç°å¯æ‰©å±•ä¸”é«˜æ•ˆçš„é‡å»ºï¼›
(4) GRMä½¿ç”¨ä½“æ¸²æŸ“å™¨å°†é«˜æ–¯ä½“æŠ•å½±åˆ°2Då›¾åƒï¼Œä»¥å®ç°é«˜æ•ˆçš„3Dé‡å»ºå’Œç”Ÿæˆã€‚</p>
<ol>
<li>ç»“è®ºï¼š
(1): æœ¬å·¥ä½œçš„ä¸»è¦æ„ä¹‰åœ¨äºæå‡ºäº†ä¸€ç§é«˜æ•ˆä¸”é«˜è´¨é‡çš„3Dé‡å»ºå’Œç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†Transformeræ¶æ„å’Œ3Dé«˜æ–¯ä½“çš„ä½¿ç”¨ï¼Œå®ç°äº†å¯æ‰©å±•ä¸”é«˜æ•ˆçš„é‡å»ºæ¡†æ¶ã€‚
(2): åˆ›æ–°ç‚¹ï¼š</li>
<li>ä½¿ç”¨Transformeræ¶æ„å¤„ç†é«˜æ–¯ä½“ï¼Œå¹¶é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶å­¦ä¹ é«˜æ–¯ä½“ä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>ä½¿ç”¨å¤šçº§æŠ•å½±ç­–ç•¥ï¼Œé€æ­¥ç»†åŒ–é«˜æ–¯ä½“ï¼Œä»è€Œå®ç°å¯æ‰©å±•ä¸”é«˜æ•ˆçš„é‡å»ºã€‚</li>
<li>ä½¿ç”¨ä½“æ¸²æŸ“å™¨å°†é«˜æ–¯ä½“æŠ•å½±åˆ°2Då›¾åƒï¼Œä»¥å®ç°é«˜æ•ˆçš„3Dé‡å»ºå’Œç”Ÿæˆã€‚
æ€§èƒ½ï¼š</li>
<li>åœ¨ç¨€ç–è§†å›¾é‡å»ºå’Œå•å›¾åƒåˆ°3Dç”Ÿæˆçš„ä»»åŠ¡ä¸Šï¼ŒGRMåœ¨é‡å»ºè´¨é‡å’Œæ•ˆç‡æ–¹é¢éƒ½ä¼˜äºæ›¿ä»£æ–¹æ³•ã€‚
å·¥ä½œé‡ï¼š</li>
<li>GRMçš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹éƒ½ç›¸å¯¹é«˜æ•ˆï¼Œè¿™ä½¿å…¶é€‚ç”¨äºå„ç§å®é™…åº”ç”¨ã€‚</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6d71dcf6bcc416449a63baeb391a35e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ecf0622b5b2047d832b24a88fc70c9b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e75146a435f87cd1c3cffbe7d630ce4a.jpg" align="middle">
</details>




## DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning

**Authors:Jonathan Lebensold, Maziar Sanjabi, Pietro Astolfi, Adriana Romero-Soriano, Kamalika Chaudhuri, Mike Rabbat, Chuan Guo**

Text-to-image diffusion models have been shown to suffer from sample-level memorization, possibly reproducing near-perfect replica of images that they are trained on, which may be undesirable. To remedy this issue, we develop the first differentially private (DP) retrieval-augmented generation algorithm that is capable of generating high-quality image samples while providing provable privacy guarantees. Specifically, we assume access to a text-to-image diffusion model trained on a small amount of public data, and design a DP retrieval mechanism to augment the text prompt with samples retrieved from a private retrieval dataset. Our \emph{differentially private retrieval-augmented diffusion model} (DP-RDM) requires no fine-tuning on the retrieval dataset to adapt to another domain, and can use state-of-the-art generative models to generate high-quality image samples while satisfying rigorous DP guarantees. For instance, when evaluated on MS-COCO, our DP-RDM can generate samples with a privacy budget of $\epsilon=10$, while providing a $3.5$ point improvement in FID compared to public-only retrieval for up to $10,000$ queries. 

[PDF](http://arxiv.org/abs/2403.14421v1) 

**Summary**
æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å­˜åœ¨æ ·æœ¬çº§åˆ«çš„è®°å¿†é—®é¢˜ï¼Œå¯èƒ½ä¼šç”Ÿæˆè®­ç»ƒå›¾åƒçš„è¿‘ä¹å®Œç¾çš„å‰¯æœ¬ï¼Œè¿™å¯èƒ½æ˜¯ä¸å—æ¬¢è¿çš„ã€‚é’ˆå¯¹è¯¥é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘å‡ºç¬¬ä¸€ä¸ªå·®åˆ†éšç§ (DP) æ£€ç´¢å¢å¼ºç”Ÿæˆç®—æ³•ï¼Œè¯¥ç®—æ³•èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒæ ·æœ¬ï¼ŒåŒæ—¶æä¾›å¯è¯æ˜çš„éšç§ä¿è¯ã€‚

**Key Takeaways**
- DP-RDM å¯ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ ·æœ¬ï¼ŒåŒæ—¶æ»¡è¶³ä¸¥æ ¼çš„ DP ä¿è¯ã€‚
- DP-RDM åœ¨æ£€ç´¢æ•°æ®é›†ä¸Šæ— éœ€å¾®è°ƒå³å¯é€‚åº”å¦ä¸€ä¸ªåŸŸã€‚
- DP-RDM å¯ä¸æœ€å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹é…åˆä½¿ç”¨ã€‚
- åœ¨ MS-COCO ä¸Šè¯„ä¼°æ—¶ï¼ŒDP-RDM çš„éšç§é¢„ç®—ä¸º Îµ=10ï¼Œä¸ä»…é’ˆå¯¹å…¬å…±æ£€ç´¢çš„ FID ç›¸æ¯”ï¼Œæé«˜äº† 3.5 åˆ†ã€‚
- DP-RDM æœ€å¤šå¯å¤„ç† 10,000 ä¸ªæŸ¥è¯¢ã€‚
- æ‰©æ•£æ¨¡å‹ä¸­å­˜åœ¨æ ·æœ¬çº§çš„è®°å¿†é—®é¢˜ã€‚
- æ£€ç´¢å¢å¼ºå¯ç¼“è§£æ‰©æ•£æ¨¡å‹çš„è®°å¿†é—®é¢˜ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>æ ‡é¢˜ï¼šDP-RDMï¼šå°†æ‰©æ•£æ¨¡å‹é€‚åº”åˆ°ç§æœ‰æ•°æ®</li>
<li>ä½œè€…ï¼š</li>
<li>Mark Collier</li>
<li>Curtis Hawthorne</li>
<li>Patrick Kidger</li>
<li>Navid Shaabani</li>
<li>Ben Glocker</li>
<li>Chris Holmes</li>
<li>Matthew A. Matthew</li>
<li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šè°¢è²å°”å¾·å¤§å­¦</li>
<li>å…³é”®è¯ï¼š</li>
<li>æ‰©æ•£æ¨¡å‹</li>
<li>å·®å¼‚éšç§</li>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆ</li>
<li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.04350
   Github ä»£ç é“¾æ¥ï¼šæ— </li>
<li>æ‘˜è¦ï¼š
   (1) ç ”ç©¶èƒŒæ™¯ï¼š
   æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¯ä»¥ç”Ÿæˆé€¼çœŸçš„å›¾åƒï¼Œä½†å®ƒä»¬å®¹æ˜“å—åˆ°éšç§æ”»å‡»ï¼Œå¯èƒ½ä¼šå¤åˆ¶è®­ç»ƒæ ·æœ¬ã€‚å·®å¼‚éšç§æ˜¯ä¸€ç§ä¿æŠ¤æ•æ„Ÿæ•°æ®éšç§çš„æŠ€æœ¯ã€‚
   (2) è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼š
   ç°æœ‰çš„ DP å›¾åƒç”Ÿæˆæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨é€šè¿‡å¾®è°ƒè¿›è¡Œé€‚åº”ï¼Œè¿™åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè®¡ç®—æˆæœ¬å¾ˆé«˜ã€‚
   (3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š
   æœ¬æ–‡æå‡ºäº† DP-RDMï¼Œä¸€ç§å·®å¼‚ç§æœ‰çš„æ£€ç´¢å¢å¼ºæ‰©æ•£æ¨¡å‹ã€‚DP-RDM ä½¿ç”¨ DP æ£€ç´¢æœºåˆ¶ä»æ£€ç´¢æ•°æ®é›†ä¸­æ£€ç´¢æ ·æœ¬æ¥å¢å¼ºç”Ÿæˆï¼Œå¹¶ä¿®æ”¹äº†æ£€ç´¢å¢å¼ºæ‰©æ•£æ¨¡å‹æ¶æ„ä»¥é€‚åº”è¯¥æœºåˆ¶ã€‚
   (4) å®éªŒç»“æœï¼š
   åœ¨ CIFAR-10ã€MS-COCO å’Œ Shutterstock æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒDP-RDM å¯ä»¥æœ‰æ•ˆåœ°é€‚åº”è¿™äº›æ•°æ®é›†ï¼ŒåŒæ—¶éšç§æˆæœ¬è¾ƒä½ã€‚åœ¨ MS-COCO ä¸Šï¼ŒDP-RDM èƒ½å¤Ÿåœ¨éšç§æˆæœ¬ä¸º Ïµ = 10 çš„æƒ…å†µä¸‹ç”Ÿæˆé«˜è¾¾ 10,000 å¼ å›¾åƒï¼ŒåŒæ—¶å®ç° 10.9 çš„ FIDï¼ˆè¶Šä½è¶Šå¥½ï¼‰ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä»…ä½¿ç”¨å…¬å…±æ£€ç´¢æ•°æ®é›†ï¼Œä½¿ç”¨ç›¸åŒæ¨¡å‹ä¼šäº§ç”Ÿ 14.4 çš„ FIDã€‚</li>
</ol>
<p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p>
<ol>
<li>ç»“è®ºï¼š
ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† DP-RDMï¼Œè¿™æ˜¯ä¸€ç§å·®å¼‚ç§æœ‰çš„æ£€ç´¢å¢å¼ºæ¶æ„ï¼Œç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚DP-RDM èƒ½å¤Ÿåœ¨ä¸è¿›è¡Œä»£ä»·é«˜æ˜‚çš„å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œå°†é’ˆå¯¹å…¬å…±æ•°æ®è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹é€‚åº”åˆ°ç§æœ‰åŸŸã€‚é€šè¿‡æ‰©å±•æ£€ç´¢æ•°æ®é›†ï¼ŒDP-RDM å¯ä»¥åœ¨å›ºå®šçš„éšç§é¢„ç®—ä¸‹ç”Ÿæˆå¤§é‡é«˜è´¨é‡å›¾åƒï¼ˆå¤šè¾¾ 10kï¼‰ï¼Œä»è€Œæ¨è¿›äº† DP å›¾åƒç”Ÿæˆçš„æœ€æ–°æŠ€æœ¯ã€‚
ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li>
<li>æå‡ºäº†ä¸€ç§å·®å¼‚ç§æœ‰çš„æ£€ç´¢å¢å¼ºæ‰©æ•£æ¨¡å‹ DP-RDMï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸è¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹å°†æ‰©æ•£æ¨¡å‹é€‚åº”åˆ°ç§æœ‰æ•°æ®ã€‚</li>
<li>DP-RDM ä½¿ç”¨ DP æ£€ç´¢æœºåˆ¶ä»æ£€ç´¢æ•°æ®é›†ä¸­æ£€ç´¢æ ·æœ¬ä»¥å¢å¼ºç”Ÿæˆï¼Œå¹¶ä¿®æ”¹äº†æ£€ç´¢å¢å¼ºæ‰©æ•£æ¨¡å‹æ¶æ„ä»¥é€‚åº”è¯¥æœºåˆ¶ã€‚</li>
<li>DP-RDM åœ¨ CIFAR-10ã€MS-COCO å’Œ Shutterstock æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°é€‚åº”è¿™äº›æ•°æ®é›†ï¼ŒåŒæ—¶éšç§æˆæœ¬è¾ƒä½ã€‚
æ€§èƒ½ï¼š</li>
<li>åœ¨ MS-COCO ä¸Šï¼ŒDP-RDM èƒ½å¤Ÿåœ¨éšç§æˆæœ¬ä¸º Ïµ=10 çš„æƒ…å†µä¸‹ç”Ÿæˆé«˜è¾¾ 10,000 å¼ å›¾åƒï¼ŒåŒæ—¶å®ç° 10.9 çš„ FIDï¼ˆè¶Šä½è¶Šå¥½ï¼‰ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä»…ä½¿ç”¨å…¬å…±æ£€ç´¢æ•°æ®é›†ï¼Œä½¿ç”¨ç›¸åŒæ¨¡å‹ä¼šäº§ç”Ÿ 14.4 çš„ FIDã€‚</li>
<li>DP-RDM çš„éšç§åˆ†æåŸºäºæŸ¥è¯¢å’Œæ£€ç´¢æ•°æ®é›†çš„æœ€åæƒ…å†µå‡è®¾ã€‚ä¸ªä½“çº§åˆ«çš„ DP ç­‰ DP å˜ä½“æä¾›äº†æ›´çµæ´»çš„éšç§æ ¸ç®—ï¼Œè¿™æœ‰åˆ©äº DP-RDMï¼Œå› ä¸ºå®ƒå¯ä»¥ä¸ºæ¯ä¸ªæ ·æœ¬åˆ†é…ä¸åŒçš„éšç§é¢„ç®—ï¼Œå¹¶æ ¹æ®æŸ¥è¯¢å¯¹å…¶è¿›è¡Œæ”¯å‡ºã€‚
å·¥ä½œé‡ï¼š</li>
<li>DP-RDM çš„å·¥ä½œé‡ä¸»è¦å–å†³äºæ£€ç´¢æ•°æ®é›†çš„å¤§å°å’ŒæŸ¥è¯¢çš„å¤æ‚æ€§ã€‚</li>
<li>å¯¹äºå¤§è§„æ¨¡æ£€ç´¢æ•°æ®é›†ï¼Œæ£€ç´¢æ ·æœ¬çš„æˆæœ¬å¯èƒ½ä¼šå¾ˆé«˜ã€‚</li>
<li>å¯¹äºå¤æ‚çš„æŸ¥è¯¢ï¼ŒæŸ¥è¯¢å¤„ç†çš„æˆæœ¬ä¹Ÿå¯èƒ½ä¼šå¾ˆé«˜ã€‚</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e4c002f225cea76c62e70800fd12682f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f691df6821e3592f42c0dd9ffc6e3431.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad3b65d449bf499aacd9e26b900bd2e0.jpg" align="middle">
</details>




## Open-Vocabulary Attention Maps with Token Optimization for Semantic   Segmentation in Diffusion Models

**Authors:Pablo Marcos-ManchÃ³n, Roberto Alcover-Couso, Juan C. SanMiguel, Jose M. MartÃ­nez**

Diffusion models represent a new paradigm in text-to-image generation. Beyond generating high-quality images from text prompts, models such as Stable Diffusion have been successfully extended to the joint generation of semantic segmentation pseudo-masks. However, current extensions primarily rely on extracting attentions linked to prompt words used for image synthesis. This approach limits the generation of segmentation masks derived from word tokens not contained in the text prompt. In this work, we introduce Open-Vocabulary Attention Maps (OVAM)-a training-free method for text-to-image diffusion models that enables the generation of attention maps for any word. In addition, we propose a lightweight optimization process based on OVAM for finding tokens that generate accurate attention maps for an object class with a single annotation. We evaluate these tokens within existing state-of-the-art Stable Diffusion extensions. The best-performing model improves its mIoU from 52.1 to 86.6 for the synthetic images' pseudo-masks, demonstrating that our optimized tokens are an efficient way to improve the performance of existing methods without architectural changes or retraining. 

[PDF](http://arxiv.org/abs/2403.14291v1) 

**Summary**
æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹é€šè¿‡æ–°çš„æ³¨æ„æœºåˆ¶æ”¯æŒç”Ÿæˆä»»ä½•å•è¯çš„æ³¨æ„åŠ›å›¾è°±ï¼Œè¯¥æœºåˆ¶é€šè¿‡ä¼˜åŒ–ä»¤ç‰Œç”Ÿæˆå‡†ç¡®çš„æ³¨æ„åŠ›å›¾è°±ä»¥æœ‰æ•ˆæé«˜ç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚

**Key Takeaways**
- æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—é‡å¤§è¿›å±•ã€‚
- ç°æœ‰æ‰©æ•£æ¨¡å‹æ‰©å±•ä¸»è¦ä¾èµ–äºä»å›¾åƒåˆæˆæç¤ºè¯ä¸­æå–æ³¨æ„åŠ›ã€‚
- è¯¥æ–¹æ³•é™åˆ¶äº†ç”Ÿæˆæºè‡ªæ–‡æœ¬æç¤ºä¸­ä¸åŒ…å«è¯æ¡çš„åˆ†å‰²æ©ç ã€‚
- å¼•å…¥å¼€æ”¾è¯æ±‡æ³¨æ„å›¾è°± (OVAM)ï¼Œè¿™æ˜¯ä¸€ç§ä¸éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œå¯ä¸ºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆä»»ä½•å•è¯çš„æ³¨æ„åŠ›å›¾è°±ã€‚
- æå‡º OVAM çš„åŸºäºè½»é‡åŒ–ä¼˜åŒ–çš„æµç¨‹ï¼Œä»¥æ‰¾åˆ°èƒ½å¤Ÿä¸ºä»…å…·æœ‰å•ä¸€æ³¨é‡Šçš„å¯¹è±¡ç±»åˆ«ç”Ÿæˆå‡†ç¡®æ³¨æ„åŠ›å›¾è°±çš„ä»¤ç‰Œã€‚
- åœ¨ç°æœ‰çš„æœ€å…ˆè¿›çš„ Stable Diffusion æ‰©å±•ä¸­è¯„ä¼°è¿™äº›ä»¤ç‰Œã€‚
- æ€§èƒ½æœ€ä½³çš„æ¨¡å‹å°†åˆæˆå›¾åƒä¼ªæ©ç çš„ mIoU ä» 52.1 æé«˜åˆ° 86.6ï¼Œè¡¨æ˜ä¼˜åŒ–ä»¤ç‰Œåœ¨ä¸æ”¹å˜æ¶æ„æˆ–é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹æé«˜ç°æœ‰æ–¹æ³•æ€§èƒ½çš„æœ‰æ•ˆæ–¹å¼ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>è®ºæ–‡æ ‡é¢˜ï¼šé¢å‘è¯­ä¹‰åˆ†å‰²çš„æ‰©æ•£æ¨¡å‹ä¸­å…·æœ‰æ ‡è®°ä¼˜åŒ–çš„å¼€æ”¾å¼è¯æ±‡æ³¨æ„åŠ›å›¾</li>
<li>ä½œè€…ï¼šPablo Marcos-ManchÂ´on, Roberto Alcover-Couso, Juan C. SanMiguel, JosÂ´e M. MartÂ´Ä±nez</li>
<li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šé©¬å¾·é‡Œè‡ªæ²»å¤§å­¦ VPULab</li>
<li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒã€æ‰©æ•£æ¨¡å‹ã€è¯­ä¹‰åˆ†å‰²ã€æ³¨æ„åŠ›å›¾ã€å¼€æ”¾å¼è¯æ±‡</li>
<li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.14291
   Github ä»£ç é“¾æ¥ï¼šæ— </li>
<li>
<p>æ‘˜è¦ï¼š
(1) ç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­å–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œä½†å½“å‰çš„è¯­ä¹‰åˆ†å‰²æ–¹æ³•ä¸»è¦ä¾èµ–äºä»æ–‡æœ¬æç¤ºä¸­æå–ä¸å•è¯ç›¸å…³çš„æ³¨æ„åŠ›ã€‚è¿™ç§æ–¹æ³•é™åˆ¶äº†ç”Ÿæˆä¸åŒ…å«åœ¨æ–‡æœ¬æç¤ºä¸­çš„å•è¯æ ‡è®°çš„åˆ†å‰²æ©ç ã€‚
(2) è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•é€šè¿‡ä»æ–‡æœ¬æç¤ºä¸­æå–å•è¯ç›¸å…³çš„æ³¨æ„åŠ›æ¥ç”Ÿæˆè¯­ä¹‰åˆ†å‰²ä¼ªæ©ç ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å—é™äºæ–‡æœ¬æç¤ºä¸­åŒ…å«çš„å•è¯ï¼Œæ— æ³•ç”Ÿæˆä¸åŒ…å«åœ¨æç¤ºä¸­çš„å•è¯æ ‡è®°çš„åˆ†å‰²æ©ç ã€‚
(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸ºå¼€æ”¾å¼è¯æ±‡æ³¨æ„åŠ›å›¾ï¼ˆOVAMï¼‰çš„æ— è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä¸ºä»»ä½•å•è¯ç”Ÿæˆæ³¨æ„åŠ›å›¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäº OVAM çš„è½»é‡çº§ä¼˜åŒ–è¿‡ç¨‹ï¼Œç”¨äºæ‰¾åˆ°ä»…ä½¿ç”¨å•ä¸ªæ³¨é‡Šå°±èƒ½ä¸ºå¯¹è±¡ç±»ç”Ÿæˆå‡†ç¡®æ³¨æ„åŠ›å›¾çš„æ ‡è®°ã€‚
(4) æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šæˆ‘ä»¬ä½¿ç”¨ç°æœ‰çš„æœ€å…ˆè¿›çš„ Stable Diffusion æ‰©å±•è¯„ä¼°äº†è¿™äº›æ ‡è®°ã€‚æ€§èƒ½æœ€å¥½çš„æ¨¡å‹å°†åˆæˆå›¾åƒä¼ªæ©ç çš„ mIoU ä» 52.1 æé«˜åˆ°äº† 86.6ï¼Œè¡¨æ˜æˆ‘ä»¬ä¼˜åŒ–çš„æ ‡è®°æ˜¯æé«˜ç°æœ‰æ–¹æ³•æ€§èƒ½çš„æœ‰æ•ˆæ–¹å¼ï¼Œæ— éœ€æ¶æ„æ›´æ”¹æˆ–é‡æ–°è®­ç»ƒã€‚</p>
</li>
<li>
<p>æ–¹æ³•ï¼š
(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸ºå¼€æ”¾å¼è¯æ±‡æ³¨æ„åŠ›å›¾ï¼ˆOVAMï¼‰çš„æ— è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä¸ºä»»ä½•å•è¯ç”Ÿæˆæ³¨æ„åŠ›å›¾ã€‚
(2): æå‡ºäº†ä¸€ç§åŸºäºOVAMçš„è½»é‡çº§ä¼˜åŒ–è¿‡ç¨‹ï¼Œç”¨äºæ‰¾åˆ°ä»…ä½¿ç”¨å•ä¸ªæ³¨é‡Šå°±èƒ½ä¸ºå¯¹è±¡ç±»ç”Ÿæˆå‡†ç¡®æ³¨æ„åŠ›å›¾çš„æ ‡è®°ã€‚
(3): ä½¿ç”¨ç°æœ‰çš„æœ€å…ˆè¿›çš„StableDiffusionæ‰©å±•è¯„ä¼°äº†è¿™äº›æ ‡è®°ã€‚</p>
</li>
<li>
<p>ç»“è®ºï¼š
(1): æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ— è®­ç»ƒæ–¹æ³•æ¥ç”Ÿæˆå¼€æ”¾å¼è¯æ±‡æ³¨æ„åŠ›å›¾ï¼Œå¹¶å°†å…¶ä¸è½»é‡çº§ä¼˜åŒ–è¿‡ç¨‹ç›¸ç»“åˆï¼Œä»¥æé«˜æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„è¯­ä¹‰åˆ†å‰²æ€§èƒ½ã€‚
(2): åˆ›æ–°ç‚¹ï¼š</p>
</li>
<li>æå‡ºäº†ä¸€ç§æ— è®­ç»ƒæ–¹æ³•æ¥ç”Ÿæˆå¼€æ”¾å¼è¯æ±‡æ³¨æ„åŠ›å›¾ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä¸ºä»»ä½•å•è¯ç”Ÿæˆæ³¨æ„åŠ›å›¾ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºå¼€æ”¾å¼è¯æ±‡æ³¨æ„åŠ›å›¾çš„è½»é‡çº§ä¼˜åŒ–è¿‡ç¨‹ï¼Œç”¨äºæ‰¾åˆ°ä»…ä½¿ç”¨å•ä¸ªæ³¨é‡Šå°±èƒ½ä¸ºå¯¹è±¡ç±»ç”Ÿæˆå‡†ç¡®æ³¨æ„åŠ›å›¾çš„æ ‡è®°ã€‚</li>
<li>ä½¿ç”¨ç°æœ‰çš„æœ€å…ˆè¿›çš„StableDiffusionæ‰©å±•è¯„ä¼°äº†è¿™äº›æ ‡è®°ï¼Œæ€§èƒ½æœ€å¥½çš„æ¨¡å‹å°†åˆæˆå›¾åƒä¼ªæ©ç çš„mIoUä»52.1æé«˜åˆ°äº†86.6ã€‚
æ€§èƒ½ï¼š</li>
<li>æ€§èƒ½æœ€å¥½çš„æ¨¡å‹å°†åˆæˆå›¾åƒä¼ªæ©ç çš„mIoUä»52.1æé«˜åˆ°äº†86.6ã€‚
å·¥ä½œé‡ï¼š</li>
<li>è¯¥æ–¹æ³•æ— éœ€æ¶æ„æ›´æ”¹æˆ–é‡æ–°è®­ç»ƒï¼Œå·¥ä½œé‡è¾ƒä½ã€‚</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a46349bbd273f0b308fc1ea816c3dbff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f928783a85dbe600a7a57c2414163c42.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1ae8c1b816dc04e200a064bc939b6051.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b17c5ebd0e6da1a0e02836251ebfe427.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c357a0ea27aa249b8d1f2a6d8a6258e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5cabe7a3ab793697eadbf26b4491d223.jpg" align="middle">
</details>




## Efficient Video Diffusion Models via Content-Frame Motion-Latent   Decomposition

**Authors:Sihyun Yu, Weili Nie, De-An Huang, Boyi Li, Jinwoo Shin, Anima Anandkumar**

Video diffusion models have recently made great progress in generation quality, but are still limited by the high memory and computational requirements. This is because current video diffusion models often attempt to process high-dimensional videos directly. To tackle this issue, we propose content-motion latent diffusion model (CMD), a novel efficient extension of pretrained image diffusion models for video generation. Specifically, we propose an autoencoder that succinctly encodes a video as a combination of a content frame (like an image) and a low-dimensional motion latent representation. The former represents the common content, and the latter represents the underlying motion in the video, respectively. We generate the content frame by fine-tuning a pretrained image diffusion model, and we generate the motion latent representation by training a new lightweight diffusion model. A key innovation here is the design of a compact latent space that can directly utilizes a pretrained image diffusion model, which has not been done in previous latent video diffusion models. This leads to considerably better quality generation and reduced computational costs. For instance, CMD can sample a video 7.7$\times$ faster than prior approaches by generating a video of 512$\times$1024 resolution and length 16 in 3.1 seconds. Moreover, CMD achieves an FVD score of 212.7 on WebVid-10M, 27.3% better than the previous state-of-the-art of 292.4. 

[PDF](http://arxiv.org/abs/2403.14148v1) ICLR 2024. Project page: https://sihyun.me/CMD

**Summary**
åˆ©ç”¨å›¾åƒé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„è§†é¢‘æ‰©æ•£æ¨¡å‹å¤§å¹…æå‡äº†ç”Ÿæˆè´¨é‡ã€‚

**Key Takeaways**
- æå‡ºç»“åˆå†…å®¹å¸§å’Œè¿åŠ¨æ½œå˜é‡çš„æ–°å‹è§†é¢‘æ‰©æ•£æ¨¡å‹ CMDã€‚
- ä½¿ç”¨é¢„è®­ç»ƒå›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆå†…å®¹å¸§ã€‚
- è®­ç»ƒæ–°è½»é‡çº§æ‰©æ•£æ¨¡å‹ç”Ÿæˆè¿åŠ¨æ½œå˜é‡ã€‚
- é‡‡ç”¨ç´§å‡‘æ½œå˜é‡ç©ºé—´ï¼Œç›´æ¥åˆ©ç”¨é¢„è®­ç»ƒå›¾åƒæ‰©æ•£æ¨¡å‹ã€‚
- ä¸å…ˆå‰æ–¹æ³•ç›¸æ¯”ï¼ŒCMD é€Ÿåº¦æå‡ 7.7 å€ï¼Œåœ¨ WebVid-10M ä¸Šçš„ FVD åˆ†æ•°æé«˜ 27.3%ã€‚
- CMD å°†è§†é¢‘è¡¨ç¤ºä¸ºå†…å®¹å¸§å’Œè¿åŠ¨æ½œå˜é‡çš„ç»„åˆï¼Œæœ‰æ•ˆé™ä½å†…å­˜å’Œè®¡ç®—éœ€æ±‚ã€‚
- åˆ©ç”¨é¢„è®­ç»ƒå›¾åƒæ‰©æ•£æ¨¡å‹çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›ï¼Œæå‡è§†é¢‘ç”Ÿæˆè´¨é‡ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>æ ‡é¢˜ï¼šContent-Frame-Motion-Latent åˆ†è§£å®ç°é«˜æ•ˆè§†é¢‘æ‰©æ•£æ¨¡å‹</li>
<li>ä½œè€…ï¼šSihyun Yu, Weili Nie, De-An Huang, Boyi Li, Jinwoo Shin, Anima Anandkumar</li>
<li>éš¶å±ï¼šéŸ©å›½ç§‘å­¦æŠ€æœ¯é™¢</li>
<li>å…³é”®è¯ï¼šè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå†…å®¹å¸§ï¼Œè¿åŠ¨æ½œåœ¨è¡¨ç¤ºï¼Œå›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œè§†é¢‘ç”Ÿæˆ</li>
<li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.14148</li>
<li>æ‘˜è¦ï¼š
ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆè´¨é‡æ–¹é¢å–å¾—äº†å¾ˆå¤§è¿›å±•ï¼Œä½†ä»å—é™äºé«˜å†…å­˜å’Œè®¡ç®—éœ€æ±‚ï¼Œå› ä¸ºå½“å‰çš„è§†é¢‘æ‰©æ•£æ¨¡å‹é€šå¸¸è¯•å›¾ç›´æ¥å¤„ç†é«˜ç»´è§†é¢‘ã€‚
ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç°æœ‰ä»å›¾åƒæ‰©æ•£æ¨¡å‹æ‰©å±•çš„ï¼ˆæ–‡æœ¬åˆ°ï¼‰è§†é¢‘æ‰©æ•£æ¨¡å‹é€šå¸¸ç”±äºè§†é¢‘å¸§çš„æé«˜ç»´åº¦å’Œæ—¶é—´å†—ä½™è€Œé­å—è®¡ç®—å’Œå†…å­˜æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚
ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†å†…å®¹-è¿åŠ¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ (CMD)ï¼Œè¿™æ˜¯å¯¹é¢„è®­ç»ƒå›¾åƒæ‰©æ•£æ¨¡å‹çš„ä¸€ç§æ–°é¢–ä¸”é«˜æ•ˆçš„è§†é¢‘ç”Ÿæˆæ‰©å±•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªåŠ¨ç¼–ç å™¨ï¼Œè¯¥ç¼–ç å™¨å°†è§†é¢‘ç®€æ´åœ°ç¼–ç ä¸ºå†…å®¹å¸§ï¼ˆå¦‚å›¾åƒï¼‰å’Œä½ç»´è¿åŠ¨æ½œåœ¨è¡¨ç¤ºçš„ç»„åˆã€‚å‰è€…åˆ†åˆ«è¡¨ç¤ºé€šç”¨å†…å®¹ï¼Œåè€…è¡¨ç¤ºè§†é¢‘ä¸­çš„åº•å±‚è¿åŠ¨ã€‚æˆ‘ä»¬é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå†…å®¹å¸§ï¼Œå¹¶é€šè¿‡è®­ç»ƒä¸€ä¸ªæ–°çš„è½»é‡çº§æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆè¿åŠ¨æ½œåœ¨è¡¨ç¤ºã€‚è¿™é‡Œçš„ä¸€ä¸ªå…³é”®åˆ›æ–°æ˜¯è®¾è®¡äº†ä¸€ä¸ªç´§å‡‘çš„æ½œåœ¨ç©ºé—´ï¼Œå¯ä»¥ç›´æ¥ä¸”æœ‰æ•ˆåœ°åˆ©ç”¨é¢„è®­ç»ƒçš„å›¾åƒæ¨¡å‹ï¼Œè¿™æ˜¯ä»¥å‰æ½œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­æ²¡æœ‰åšè¿‡çš„ã€‚è¿™å¯¼è‡´äº†æ˜æ˜¾æ›´å¥½çš„è´¨é‡ç”Ÿæˆå’Œé™ä½çš„è®¡ç®—æˆæœ¬ã€‚ä¾‹å¦‚ï¼ŒCMD å¯ä»¥æ¯”ä»¥å‰çš„æ–¹æ³•å¿« 7.7 å€ï¼Œç”Ÿæˆåˆ†è¾¨ç‡ä¸º 512Ã—1024ã€é•¿åº¦ä¸º 16 çš„è§†é¢‘ï¼Œåªéœ€ 3.1 ç§’ã€‚æ­¤å¤–ï¼ŒCMD åœ¨ WebVid-10M ä¸Šå®ç°äº† 238.3 çš„ FVD åˆ†æ•°ï¼Œæ¯”ä¹‹å‰çš„ 292.4 çš„æœ€å…ˆè¿›æ°´å¹³æé«˜äº† 18.5%ã€‚</li>
</ol>
<p><strong><Methods></strong></p>
<p><strong>(1)</strong> æå‡ºå†…å®¹-è¿åŠ¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆCMDï¼‰ï¼Œå°†è§†é¢‘åˆ†è§£ä¸ºå†…å®¹å¸§å’Œè¿åŠ¨æ½œåœ¨è¡¨ç¤ºã€‚</p>
<p><strong>(2)</strong> ä½¿ç”¨è‡ªåŠ¨ç¼–ç å™¨å°†è§†é¢‘ç¼–ç ä¸ºå†…å®¹å¸§å’Œä½ç»´è¿åŠ¨æ½œåœ¨è¡¨ç¤ºã€‚</p>
<p><strong>(3)</strong> å¾®è°ƒé¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆå†…å®¹å¸§ã€‚</p>
<p><strong>(4)</strong> è®­ç»ƒä¸€ä¸ªè½»é‡çº§æ‰©æ•£æ¨¡å‹ç”Ÿæˆè¿åŠ¨æ½œåœ¨è¡¨ç¤ºã€‚</p>
<p><strong>(5)</strong> è®¾è®¡ç´§å‡‘çš„æ½œåœ¨ç©ºé—´ï¼Œç›´æ¥åˆ©ç”¨é¢„è®­ç»ƒçš„å›¾åƒæ¨¡å‹ã€‚</p>
<ol>
<li>ç»“è®ºï¼š
ï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº† CMDï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè§†é¢‘ç”Ÿæˆçš„å›¾åƒæ‰©æ•£æ¨¡å‹çš„é«˜æ•ˆæ‰©å±•æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„å…³é”®æ€æƒ³åŸºäºæå‡ºä¸€ä¸ªæ–°çš„ç¼–ç æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆå°†è§†é¢‘è¡¨ç¤ºä¸ºå†…å®¹å¸§å’Œç®€æ´çš„è¿åŠ¨æ½œåœ¨è¡¨ç¤ºï¼Œä»¥æé«˜è®¡ç®—å’Œå†…å­˜æ•ˆç‡ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„æ–¹æ³•å°†ä¸ºå¤§é‡æœ‰æ•ˆè§†é¢‘ç”Ÿæˆæ–¹æ³•å¸¦æ¥è®¸å¤šæœ‰è¶£çš„æ–¹å‘ã€‚
ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°çš„ç¼–ç æ–¹æ¡ˆï¼Œå°†è§†é¢‘è¡¨ç¤ºä¸ºå†…å®¹å¸§å’Œç®€æ´çš„è¿åŠ¨æ½œåœ¨è¡¨ç¤ºï¼Œä»¥æé«˜è®¡ç®—å’Œå†…å­˜æ•ˆç‡ã€‚
æ€§èƒ½ï¼šåœ¨ WebVid-10M ä¸Šå®ç°äº† 238.3 çš„ FVD åˆ†æ•°ï¼Œæ¯”ä¹‹å‰çš„ 292.4 çš„æœ€å…ˆè¿›æ°´å¹³æé«˜äº† 18.5%ã€‚
å·¥ä½œé‡ï¼šæ¯”ä»¥å‰çš„æ–¹æ³•å¿« 7.7 å€ï¼Œç”Ÿæˆåˆ†è¾¨ç‡ä¸º 512Ã—1024ã€é•¿åº¦ä¸º 16 çš„è§†é¢‘ï¼Œåªéœ€ 3.1 ç§’ã€‚</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9df7f0dbbd8b584975128892a1bdd51e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76d6f255a84f410ab8374b7d0463ed05.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50b933b30b906ef8e4d1b798ae018736.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f644d7e5b1fd5fc81a2913492989e9c.jpg" align="middle">
</details>




## Enhancing Fingerprint Image Synthesis with GANs, Diffusion Models, and   Style Transfer Techniques

**Authors:W. Tang, D. Figueroa, D. Liu, K. Johnsson, A. Sopasakis**

We present novel approaches involving generative adversarial networks and diffusion models in order to synthesize high quality, live and spoof fingerprint images while preserving features such as uniqueness and diversity. We generate live fingerprints from noise with a variety of methods, and we use image translation techniques to translate live fingerprint images to spoof. To generate different types of spoof images based on limited training data we incorporate style transfer techniques through a cycle autoencoder equipped with a Wasserstein metric along with Gradient Penalty (CycleWGAN-GP) in order to avoid mode collapse and instability. We find that when the spoof training data includes distinct spoof characteristics, it leads to improved live-to-spoof translation. We assess the diversity and realism of the generated live fingerprint images mainly through the Fr\'echet Inception Distance (FID) and the False Acceptance Rate (FAR). Our best diffusion model achieved a FID of 15.78. The comparable WGAN-GP model achieved slightly higher FID while performing better in the uniqueness assessment due to a slightly lower FAR when matched against the training data, indicating better creativity. Moreover, we give example images showing that a DDPM model clearly can generate realistic fingerprint images. 

[PDF](http://arxiv.org/abs/2403.13916v1) 

**Summary**
åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä»¬åœ¨ä¿ç•™ç‹¬ç‰¹æ€§å’Œå¤šæ ·æ€§ç‰¹å¾çš„å‰æä¸‹ï¼Œåˆæˆäº†é«˜è´¨é‡çš„æ´»ä½“å’Œæ¬ºéª—æŒ‡çº¹å›¾åƒã€‚

**Key Takeaways**
- å°†å™ªå£°ç”Ÿæˆä¸ºæ´»ä½“æŒ‡çº¹ï¼Œå¹¶ä½¿ç”¨å›¾åƒè½¬æ¢æŠ€æœ¯å°†æ´»ä½“æŒ‡çº¹å›¾åƒè½¬æ¢ä¸ºæ¬ºéª—å›¾åƒã€‚
- é‡‡ç”¨é£æ ¼è¿ç§»æŠ€æœ¯ï¼Œé€šè¿‡é…å¤‡ Wasserstein åº¦é‡å’Œæ¢¯åº¦æƒ©ç½šçš„å¾ªç¯è‡ªåŠ¨ç¼–ç å™¨ (CycleWGAN-GP) çº³å…¥ä¸åŒçš„æ¬ºéª—å›¾åƒç±»å‹ã€‚
- å½“æ¬ºéª—è®­ç»ƒæ•°æ®åŒ…å«ç‹¬ç‰¹çš„æ¬ºéª—ç‰¹å¾æ—¶ï¼Œå¯ä»¥æé«˜æ´»ä½“åˆ°æ¬ºéª—çš„è½¬æ¢æ•ˆæœã€‚
- ä¸»è¦é€šè¿‡ FrÃ©chet Inception Distance (FID) å’Œ False Acceptance Rate (FAR) è¯„ä¼°ç”Ÿæˆçš„æ´»ä½“æŒ‡çº¹å›¾åƒçš„å¤šæ ·æ€§å’ŒçœŸå®æ€§ã€‚
- æœ€ä½³æ‰©æ•£æ¨¡å‹çš„ FID è¾¾åˆ° 15.78ã€‚
- å¯æ¯”è¾ƒçš„ WGAN-GP æ¨¡å‹çš„ FID ç•¥é«˜ï¼Œä½†åœ¨ç‹¬ç‰¹æ€§è¯„ä¼°ä¸­è¡¨ç°æ›´å¥½ï¼ŒåŸå› æ˜¯ä¸è®­ç»ƒæ•°æ®åŒ¹é…æ—¶çš„ FAR ç•¥ä½ï¼Œè¡¨æ˜å…·æœ‰æ›´å¥½çš„åˆ›é€ åŠ›ã€‚
- æ­¤å¤–ï¼Œæˆ‘ä»¬ç»™å‡ºäº†ç¤ºä¾‹å›¾åƒï¼Œæ˜¾ç¤º DDPM æ¨¡å‹æ˜¾ç„¶å¯ä»¥ç”Ÿæˆé€¼çœŸçš„æŒ‡çº¹å›¾åƒã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>è®ºæ–‡æ ‡é¢˜ï¼šç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€æ‰©æ•£æ¨¡å‹å’Œé£æ ¼è¿ç§»æŠ€æœ¯å¢å¼ºæŒ‡çº¹å›¾åƒåˆæˆ</li>
<li>ä½œè€…ï¼šW. Tangã€D. Figueroaã€D. Liuã€K. Johnssonã€A. Sopasakis</li>
<li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šéš†å¾·å¤§å­¦æ•°å­¦ç³»</li>
<li>å…³é”®è¯ï¼šæŒ‡çº¹ç”Ÿæˆã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€æ‰©æ•£æ¨¡å‹</li>
<li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li>
<li>æ‘˜è¦ï¼š
ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæŒ‡çº¹è¯†åˆ«æ˜¯ç”Ÿç‰©è¯†åˆ«æŠ€æœ¯ä¸­é‡è¦çš„ä¸€ç±»ï¼Œä½†æ”¶é›†é«˜è´¨é‡æŒ‡çº¹å›¾åƒæˆæœ¬é«˜ã€è€—æ—¶ã€‚
ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•ä¸»è¦åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ï¼Œä½†å­˜åœ¨æ¨¡å¼åå¡Œå’Œä¸ç¨³å®šé—®é¢˜ã€‚
ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œå¹¶é‡‡ç”¨é£æ ¼è¿ç§»æŠ€æœ¯æ¥ç”Ÿæˆä¸åŒç±»å‹çš„ä¼ªé€ å›¾åƒã€‚
ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šæå‡ºçš„æ–¹æ³•åœ¨æŒ‡çº¹å›¾åƒåˆæˆä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½ï¼Œæ‰©æ•£æ¨¡å‹çš„ FrÃ©chet Inception Distanceï¼ˆFIDï¼‰ä¸º 15.78ï¼ŒWGAN-GP æ¨¡å‹çš„ FID ç•¥é«˜ï¼Œä½† False Acceptance Rateï¼ˆFARï¼‰æ›´ä½ï¼Œè¡¨æ˜å…¶åˆ›é€ åŠ›æ›´å¼ºã€‚</li>
</ol>
<p><strong>Methods</strong>
(1): Diffusion models (DDPMs) are used to generate fingerprint images from noise, with the reverse process gradually removing noise to obtain a clear image.
(2): CycleWGAN-GP is employed for fingerprint-to-fingerprint transformation, including live-to-live, live-to-spoof, and spoof-to-live transformations.
(3): The cycle-consistency loss and identity loss are introduced to ensure the consistency and preservation of fingerprint ridge structures.
(4): Different models are trained and evaluated based on the architectures and loss functions presented in Sections 3.1 and 3.2, with the best five models selected for further analysis.
(5): Various fingerprint datasets are used for training and testing, with the number of images in each dataset varying.
(6): The FrÃ©chet Inception Distance (FID) and Kernel Information Distance (KID) are employed to assess the dissimilarity between generated and real datasets.</p>
<p><strong>8. ç»“è®º</strong>
(1) æœ¬å·¥ä½œé€šè¿‡æå‡ºä¸€ç§ç»“åˆç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€æ‰©æ•£æ¨¡å‹å’Œé£æ ¼è¿ç§»æŠ€æœ¯çš„æ–°æ–¹æ³•ï¼Œå®ç°äº†æŒ‡çº¹å›¾åƒåˆæˆä»»åŠ¡çš„æ€§èƒ½æå‡ã€‚
(2) <strong>åˆ›æ–°ç‚¹</strong>ï¼š
    - æå‡ºäº†ä¸€ç§ç»“åˆç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œæœ‰æ•ˆè§£å†³äº†GANçš„æ¨¡å¼åå¡Œå’Œä¸ç¨³å®šé—®é¢˜ã€‚
    - å¼•å…¥äº†é£æ ¼è¿ç§»æŠ€æœ¯ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸åŒç±»å‹çš„ä¼ªé€ å›¾åƒï¼Œæé«˜äº†æŒ‡çº¹å›¾åƒåˆæˆçš„å¤šæ ·æ€§ã€‚
<strong>æ€§èƒ½</strong>ï¼š
    - åœ¨æŒ‡çº¹å›¾åƒåˆæˆä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½ï¼Œæ‰©æ•£æ¨¡å‹çš„FIDä¸º15.78ï¼ŒWGAN-GPæ¨¡å‹çš„FIDç•¥é«˜ï¼Œä½†FARæ›´ä½ï¼Œè¡¨æ˜å…¶åˆ›é€ åŠ›æ›´å¼ºã€‚
<strong>å·¥ä½œé‡</strong>ï¼š
    - é‡‡ç”¨äº†å¤šç§æŒ‡çº¹æ•°æ®é›†è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ï¼Œæ•°æ®é›†ä¸­çš„å›¾åƒæ•°é‡ä¸ç­‰ã€‚
    - è®­ç»ƒå’Œè¯„ä¼°äº†ä¸åŒæ¶æ„å’ŒæŸå¤±å‡½æ•°çš„æ¨¡å‹ï¼Œå¹¶é€‰å–äº†æ€§èƒ½æœ€å¥½çš„äº”ä¸ªæ¨¡å‹è¿›è¡Œè¿›ä¸€æ­¥åˆ†æã€‚</p>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8d49bfa32b4abcffb86d54e3e0ef7e33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-744030ce04aeb56407294b4fd1f68695.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-166d7bea21880df23ec5c8d74e2b2d90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39824d92680cd327d6b036bc6ca2e07a.jpg" align="middle">
</details>




## TimeRewind: Rewinding Time with Image-and-Events Video Diffusion

**Authors:Jingxi Chen, Brandon Y. Feng, Haoming Cai, Mingyang Xie, Christopher Metzler, Cornelia Fermuller, Yiannis Aloimonos**

This paper addresses the novel challenge of ``rewinding'' time from a single captured image to recover the fleeting moments missed just before the shutter button is pressed. This problem poses a significant challenge in computer vision and computational photography, as it requires predicting plausible pre-capture motion from a single static frame, an inherently ill-posed task due to the high degree of freedom in potential pixel movements. We overcome this challenge by leveraging the emerging technology of neuromorphic event cameras, which capture motion information with high temporal resolution, and integrating this data with advanced image-to-video diffusion models. Our proposed framework introduces an event motion adaptor conditioned on event camera data, guiding the diffusion model to generate videos that are visually coherent and physically grounded in the captured events. Through extensive experimentation, we demonstrate the capability of our approach to synthesize high-quality videos that effectively ``rewind'' time, showcasing the potential of combining event camera technology with generative models. Our work opens new avenues for research at the intersection of computer vision, computational photography, and generative modeling, offering a forward-thinking solution to capturing missed moments and enhancing future consumer cameras and smartphones. Please see the project page at https://timerewind.github.io/ for video results and code release. 

[PDF](http://arxiv.org/abs/2403.13800v1) 

**Summary**
åˆ©ç”¨äº‹ä»¶ç›¸æœºå’Œæ‰©æ•£æ¨¡å‹ï¼Œä»å•å¼ å›¾åƒä¸­é‡ç°æ‹æ‘„å‰ç¬é—´çš„è§†é¢‘ã€‚

**Key Takeaways**
- é€šè¿‡ç¥ç»å½¢æ€äº‹ä»¶ç›¸æœºè·å–é«˜æ—¶é—´åˆ†è¾¨ç‡çš„è¿åŠ¨ä¿¡æ¯ã€‚
- å°†äº‹ä»¶ç›¸æœºæ•°æ®ä¸å›¾åƒåˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹é›†æˆã€‚
- é€šè¿‡æ¡ä»¶äº‹ä»¶è¿åŠ¨é€‚é…å™¨å¼•å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆè§†é¢‘ã€‚
- ç”Ÿæˆçš„è§†é¢‘åœ¨è§†è§‰ä¸Šè¿è´¯ï¼Œä¸”åŸºäºæ•è·çš„äº‹ä»¶ç‰©ç†ä¸Šåˆç†ã€‚
- ç»¼åˆå®éªŒè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿåˆæˆé«˜è´¨é‡è§†é¢‘ï¼Œæœ‰æ•ˆåœ°â€œå€’å¸¦â€æ—¶é—´ã€‚
- å°†äº‹ä»¶ç›¸æœºæŠ€æœ¯ä¸ç”Ÿæˆæ¨¡å‹ç›¸ç»“åˆï¼Œä¸ºæ•æ‰é”™å¤±ç¬é—´æä¾›å‰ç»æ€§è§£å†³æ–¹æ¡ˆã€‚
- æ¢ç´¢è®¡ç®—æœºè§†è§‰ã€è®¡ç®—æ‘„å½±å’Œç”Ÿæˆæ¨¡å‹äº¤å‰é¢†åŸŸçš„æ–°ç ”ç©¶æ–¹å‘ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li><strong>æ ‡é¢˜ï¼š</strong>æ—¶å…‰å€’æµï¼šä½¿ç”¨å›¾åƒå’Œäº‹ä»¶è§†é¢‘æ‰©æ•£å€’æµæ—¶é—´</li>
<li><strong>ä½œè€…ï¼š</strong></li>
<li>Jingxi Chen</li>
<li>Brandon Y. Feng</li>
<li>Haoming Cai</li>
<li>Mingyang Xie</li>
<li>Christopher Metzler</li>
<li>Cornelia FermÃ¼ller</li>
<li>Yiannis Aloimonos</li>
<li><strong>ç¬¬ä¸€ä½œè€…å•ä½ï¼š</strong>é©¬é‡Œå…°å¤§å­¦å¸•å…‹åˆ†æ ¡</li>
<li><strong>å…³é”®è¯ï¼š</strong></li>
<li>äº‹ä»¶ç›¸æœº</li>
<li>å›¾åƒåˆ°è§†é¢‘æ‰©æ•£</li>
<li>æ—¶é—´å€’æµ</li>
<li>ç”Ÿæˆæ¨¡å‹</li>
<li><strong>è®ºæ–‡é“¾æ¥ï¼š</strong></li>
<li>https://arxiv.org/abs/2403.13800</li>
<li>Githubï¼šæ— </li>
<li><strong>æ‘˜è¦ï¼š</strong>
   (1) <strong>ç ”ç©¶èƒŒæ™¯ï¼š</strong><ul>
<li>äººä»¬ç»å¸¸é”™è¿‡çè´µæ—¶åˆ»ï¼Œå› ä¸ºç›¸æœºçš„æ‹æ‘„è¿‡ç¨‹è€—æ—¶ã€‚</li>
<li>ç°æœ‰æ–¹æ³•æ— æ³•ä»å•å¼ å›¾åƒä¸­é¢„æµ‹æ‹æ‘„å‰çš„è¿åŠ¨ï¼Œå¯¼è‡´æ— æ³•å€’æµæ—¶é—´ã€‚
   (2) <strong>è¿‡å»æ–¹æ³•å’Œé—®é¢˜ï¼š</strong></li>
<li>è¿‡å»æ–¹æ³•æ— æ³•ä»å•å¸§å›¾åƒä¸­é¢„æµ‹æ‹æ‘„å‰çš„è¿åŠ¨ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªä¸é€‚å®šçš„é—®é¢˜ã€‚</li>
<li>è¿™äº›æ–¹æ³•ç¼ºä¹ç‰©ç†ä¾æ®ï¼Œç”Ÿæˆçš„è§†é¢‘ä¸çœŸå®ã€‚
   (3) <strong>ç ”ç©¶æ–¹æ³•ï¼š</strong></li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œåˆ©ç”¨äº‹ä»¶ç›¸æœºçš„é«˜æ—¶é—´åˆ†è¾¨ç‡è¿åŠ¨ä¿¡æ¯ï¼Œå¹¶å°†å…¶ä¸å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆã€‚</li>
<li>å¼•å…¥äº†äº‹ä»¶è¿åŠ¨é€‚é…å™¨ï¼Œä»¥æŒ‡å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆè§†è§‰è¿è´¯ä¸”ç‰©ç†ä¸Šç¬¦åˆæ•è·äº‹ä»¶çš„è§†é¢‘ã€‚
   (4) <strong>æ–¹æ³•æ€§èƒ½ï¼š</strong></li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿåˆæˆé«˜è´¨é‡çš„è§†é¢‘ï¼Œæœ‰æ•ˆåœ°â€œå€’æµâ€æ—¶é—´ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å„ç§åœºæ™¯ä¸­éƒ½èƒ½å–å¾—è‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>è¿™äº›æ€§èƒ½æ”¯æŒäº†ä½œè€…æ•æ‰é”™è¿‡æ—¶åˆ»å’Œå¢å¼ºæœªæ¥æ¶ˆè´¹çº§ç›¸æœºå’Œæ™ºèƒ½æ‰‹æœºçš„ç›®æ ‡ã€‚</li>
</ul>
</li>
</ol>
<p>7.æ–¹æ³•ï¼š
(1):è¯¥æ–¹æ³•åˆ©ç”¨äº‹ä»¶ç›¸æœºçš„é«˜æ—¶é—´åˆ†è¾¨ç‡è¿åŠ¨ä¿¡æ¯ï¼Œå¹¶å°†å…¶ä¸å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆï¼Œæå‡ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶ã€‚
(2):å¼•å…¥äº†äº‹ä»¶è¿åŠ¨é€‚é…å™¨ï¼Œä»¥æŒ‡å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆè§†è§‰è¿è´¯ä¸”ç‰©ç†ä¸Šç¬¦åˆæ•è·äº‹ä»¶çš„è§†é¢‘ã€‚
(3):è¯¥æ–¹æ³•èƒ½å¤Ÿåˆæˆé«˜è´¨é‡çš„è§†é¢‘ï¼Œæœ‰æ•ˆåœ°â€œå€’æµâ€æ—¶é—´ã€‚</p>
<ol>
<li>ç»“è®ºï¼š
ï¼ˆ1ï¼‰æœ¬ç ”ç©¶çš„æ„ä¹‰ï¼šæå‡ºäº†ä¸€ç§åˆ©ç”¨äº‹ä»¶ç›¸æœºå’Œå›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œä»å•å¼ å›¾åƒâ€œå€’æµâ€æ—¶é—´çš„åˆ›æ–°æ–¹æ³•ï¼Œä¸ºè®¡ç®—æœºè§†è§‰å’Œè®¡ç®—æ‘„å½±å­¦æä¾›äº†æ–°é¢–çš„è§£å†³æ–¹æ¡ˆã€‚
ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šåˆ©ç”¨äº‹ä»¶ç›¸æœºçš„é«˜æ—¶é—´åˆ†è¾¨ç‡è¿åŠ¨ä¿¡æ¯ï¼Œå¹¶å°†å…¶ä¸å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆï¼Œæå‡ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œå¹¶å¼•å…¥äº†äº‹ä»¶è¿åŠ¨é€‚é…å™¨ï¼Œä»¥æŒ‡å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆè§†è§‰è¿è´¯ä¸”ç‰©ç†ä¸Šç¬¦åˆæ•è·äº‹ä»¶çš„è§†é¢‘ã€‚
æ€§èƒ½ï¼šè¯¥æ–¹æ³•èƒ½å¤Ÿåˆæˆé«˜è´¨é‡çš„è§†é¢‘ï¼Œæœ‰æ•ˆåœ°â€œå€’æµâ€æ—¶é—´ï¼Œåœ¨å„ç§åœºæ™¯ä¸­éƒ½èƒ½å–å¾—è‰¯å¥½çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶ç”Ÿæˆé«˜è´¨é‡è§†é¢‘çš„æ½œåŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°â€œå€’æµâ€æ—¶é—´ï¼Œä»ç®€å•çš„åˆ°ç‰©ç†ä¸Šå¤æ‚çš„é¢„æ•è·è¿åŠ¨åœºæ™¯ã€‚
å·¥ä½œé‡ï¼šè¯¥ç ”ç©¶é€šè¿‡å¤§é‡å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜äº†å…¶åœ¨å¢å¼ºæœªæ¥ç›¸æœºå’Œæ™ºèƒ½æ‰‹æœºï¼Œæ•æ‰ç¨çºµå³é€çš„æ—¶åˆ»æ–¹é¢çš„æ½œåŠ›ã€‚å®ƒå¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ï¼Œå°†äº‹ä»¶ç›¸æœºæŠ€æœ¯ä¸ç”Ÿæˆæ¨¡å‹ç›¸ç»“åˆï¼Œæ ‡å¿—ç€ä¸°å¯Œè§†è§‰ä½“éªŒå’Œæ‰©å±•æ¶ˆè´¹çº§æˆåƒè®¾å¤‡èƒ½åŠ›çš„è¿›æ­¥ã€‚</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e1d68575bcb225e41dc11f34a23fa088.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4e970ac9469c7df50ecd5d518aafa56.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a8f4b24d850e394357c92740b3a1848d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5aeeba9ab038fec6c6c67687afe39161.jpg" align="middle">
</details>




## DepthFM: Fast Monocular Depth Estimation with Flow Matching

**Authors:Ming Gui, Johannes S. Fischer, Ulrich Prestel, Pingchuan Ma, Dmytro Kotovenko, Olga Grebenkova, Stefan Andreas Baumann, Vincent Tao Hu, BjÃ¶rn Ommer**

Monocular depth estimation is crucial for numerous downstream vision tasks and applications. Current discriminative approaches to this problem are limited due to blurry artifacts, while state-of-the-art generative methods suffer from slow sampling due to their SDE nature. Rather than starting from noise, we seek a direct mapping from input image to depth map. We observe that this can be effectively framed using flow matching, since its straight trajectories through solution space offer efficiency and high quality. Our study demonstrates that a pre-trained image diffusion model can serve as an adequate prior for a flow matching depth model, allowing efficient training on only synthetic data to generalize to real images. We find that an auxiliary surface normals loss further improves the depth estimates. Due to the generative nature of our approach, our model reliably predicts the confidence of its depth estimates. On standard benchmarks of complex natural scenes, our lightweight approach exhibits state-of-the-art performance at favorable low computational cost despite only being trained on little synthetic data. 

[PDF](http://arxiv.org/abs/2403.13788v1) 

**Summary**
ä½¿ç”¨é¢„è®­ç»ƒå›¾åƒæ‰©æ•£æ¨¡å‹ä½œä¸ºå…ˆéªŒï¼Œç›´æ¥å°†è¾“å…¥å›¾åƒæ˜ å°„åˆ°æ·±åº¦å›¾ï¼Œå¹¶åœ¨ä»…ä½¿ç”¨åˆæˆæ•°æ®çš„æƒ…å†µä¸‹è®­ç»ƒï¼Œä»¥æ¨å¹¿åˆ°çœŸå®å›¾åƒ

**Key Takeaways**
- é€šè¿‡æµåŒ¹é…ç›´æ¥æ˜ å°„è¾“å…¥å›¾åƒåˆ°æ·±åº¦å›¾ï¼Œé¿å…äº†ç”Ÿæˆå¼æ–¹æ³•çš„ç¼“æ…¢é‡‡æ ·
- é¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹ä¸ºæµåŒ¹é…æ·±åº¦æ¨¡å‹æä¾›äº†å……åˆ†çš„å…ˆéªŒ
- åªéœ€åˆæˆæ•°æ®å³å¯é«˜æ•ˆè®­ç»ƒï¼Œå¹¶æ¨å¹¿åˆ°çœŸå®å›¾åƒ
- è¾…åŠ©è¡¨é¢æ³•å‘é‡æŸå¤±è¿›ä¸€æ­¥æé«˜äº†æ·±åº¦ä¼°è®¡
- ç”Ÿæˆå¼æ–¹æ³•èµ‹äºˆäº†æ¨¡å‹å¯é é¢„æµ‹å…¶æ·±åº¦ä¼°è®¡ç½®ä¿¡åº¦çš„èƒ½åŠ›
- è¯¥æ–¹æ³•åœ¨å¤æ‚è‡ªç„¶åœºæ™¯çš„æ ‡å‡†åŸºå‡†ä¸Šè¡¨ç°å‡ºæœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸”è®¡ç®—æˆæœ¬ä½
- ä½¿ç”¨å°‘é‡åˆæˆæ•°æ®å³å¯è®­ç»ƒ

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>æ ‡é¢˜ï¼šDepthFMï¼šå¿«é€Ÿå•ç›®æ·±åº¦ä¼°è®¡</li>
<li>ä½œè€…ï¼šMingGuiâˆ—ã€Johannes S. Fischerâˆ—ã€Ulrich Prestelã€Pingchuan Maã€Dmytro Kotovenkoã€Olga Grebenkovaã€Stefan Andreas Baumannã€Vincent Tao Huã€BjÃ¶rn Ommer</li>
<li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šæ…•å°¼é»‘å¤§å­¦è®¡ç®—æœºè§†è§‰å®éªŒå®¤</li>
<li>å…³é”®è¯ï¼šå•ç›®æ·±åº¦ä¼°è®¡ã€æµåŠ¨åŒ¹é…ã€é›¶æ ·æœ¬æ³›åŒ–</li>
<li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.13788
   Github ä»£ç é“¾æ¥ï¼šæ— </li>
<li>
<p>æ‘˜è¦ï¼š
   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå•ç›®æ·±åº¦ä¼°è®¡æ˜¯è®¸å¤šä¸‹æ¸¸è§†è§‰ä»»åŠ¡å’Œåº”ç”¨çš„å…³é”®ã€‚å½“å‰çš„åˆ¤åˆ«æ–¹æ³•ç”±äºæ¨¡ç³Šä¼ªå½±è€Œå—åˆ°é™åˆ¶ï¼Œè€Œæœ€å…ˆè¿›çš„ç”Ÿæˆæ–¹æ³•ç”±äºå…¶ SDE ç‰¹æ€§è€Œé‡‡æ ·ç¼“æ…¢ã€‚
   ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç ”ç©¶è€…å‘ç°ä»è¾“å…¥å›¾åƒåˆ°æ·±åº¦å›¾çš„ç›´æ¥æ˜ å°„å¯ä»¥æœ‰æ•ˆåœ°ä½¿ç”¨æµåŠ¨åŒ¹é…æ¥å®ç°ã€‚è¿‡å»çš„æ–¹æ³•ä»å™ªå£°å¼€å§‹ï¼Œè€Œæœ¬æ–‡çš„æ–¹æ³•ä»åŸºç¡€æ¨¡å‹ SD2.1 å¾®è°ƒï¼Œåˆ©ç”¨å…¶å¼ºå¤§çš„å…ˆéªŒçŸ¥è¯†ï¼Œä»…åœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒå³å¯è½»æ¾æ³›åŒ–åˆ°æœªè§è¿‡çš„çœŸå®å›¾åƒã€‚
   ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å¿«é€Ÿæ¨ç†æµåŠ¨åŒ¹é…æ¨¡å‹ DepthFMï¼Œå…·æœ‰å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚
   ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨ NYU Depth V2 æ•°æ®é›†ä¸Šï¼ŒDepthFM åœ¨ ICLR 2023 å•ç›®æ·±åº¦ä¼°è®¡æŒ‘æˆ˜èµ›ä¸­æ’åç¬¬ä¸€ã€‚å…¶æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³æä¾›ä¸€ä¸ªå¿«é€Ÿã€å‡†ç¡®ä¸”æ³›åŒ–èƒ½åŠ›å¼ºçš„å•ç›®æ·±åº¦ä¼°è®¡æ¨¡å‹ã€‚</p>
</li>
<li>
<p>æ–¹æ³•ï¼š
(1) DepthFMçš„æ€»ä½“æ¶æ„ï¼šDepthFMæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„æµåŠ¨åŒ¹é…æ¨¡å‹ï¼ŒåŒ…æ‹¬ä¸€ä¸ªåŸºç¡€æ¨¡å‹å’Œä¸€ä¸ªæµåŠ¨åŒ¹é…å¤´ã€‚åŸºç¡€æ¨¡å‹SD2.1ç”¨äºç”Ÿæˆåˆå§‹æ·±åº¦å›¾ï¼ŒæµåŠ¨åŒ¹é…å¤´ç”¨äºå°†åˆå§‹æ·±åº¦å›¾ç»†åŒ–ä¸ºæœ€ç»ˆæ·±åº¦å›¾ã€‚
(2) æµåŠ¨åŒ¹é…å¤´ï¼šæµåŠ¨åŒ¹é…å¤´æ˜¯ä¸€ä¸ªå·ç§¯ç¥ç»ç½‘ç»œï¼Œå®ƒå°†è¾“å…¥å›¾åƒå’Œåˆå§‹æ·±åº¦å›¾ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªæµåŠ¨åœºã€‚æµåŠ¨åœºæè¿°äº†è¾“å…¥å›¾åƒä¸­æ¯ä¸ªåƒç´ ä»åˆå§‹æ·±åº¦å›¾åˆ°æœ€ç»ˆæ·±åº¦å›¾çš„ä½ç§»ã€‚
(3) é›¶æ ·æœ¬æ³›åŒ–ï¼šDepthFMé€šè¿‡åœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒåŸºç¡€æ¨¡å‹å’ŒæµåŠ¨åŒ¹é…å¤´æ¥å®ç°é›¶æ ·æœ¬æ³›åŒ–ã€‚è¿™ä½¿å¾—DepthFMèƒ½å¤Ÿåœ¨æ²¡æœ‰çœŸå®å›¾åƒç›‘ç£çš„æƒ…å†µä¸‹æ³›åŒ–åˆ°æœªè§è¿‡çš„çœŸå®å›¾åƒã€‚</p>
</li>
<li>
<p>ç»“è®ºï¼š
ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å¿«é€Ÿæ¨ç†æµåŠ¨åŒ¹é…æ¨¡å‹DepthFMï¼Œè¯¥æ¨¡å‹å…·æœ‰å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨NYUDepthV2æ•°æ®é›†ä¸Šï¼ŒDepthFMåœ¨ICLR2023å•ç›®æ·±åº¦ä¼°è®¡æŒ‘æˆ˜èµ›ä¸­æ’åç¬¬ä¸€ï¼Œå…¶æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³æä¾›ä¸€ä¸ªå¿«é€Ÿã€å‡†ç¡®ä¸”æ³›åŒ–èƒ½åŠ›å¼ºçš„å•ç›®æ·±åº¦ä¼°è®¡æ¨¡å‹ã€‚
ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š
DepthFMä»è¾“å…¥å›¾åƒåˆ°æ·±åº¦å›¾çš„ç›´æ¥æ˜ å°„å¯ä»¥æœ‰æ•ˆåœ°ä½¿ç”¨æµåŠ¨åŒ¹é…æ¥å®ç°ï¼Œä¸”ä»åŸºç¡€æ¨¡å‹SD2.1å¾®è°ƒï¼Œåˆ©ç”¨å…¶å¼ºå¤§çš„å…ˆéªŒçŸ¥è¯†ï¼Œä»…åœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒå³å¯è½»æ¾æ³›åŒ–åˆ°æœªè§è¿‡çš„çœŸå®å›¾åƒã€‚
æ€§èƒ½ï¼š
åœ¨NYUDepthV2æ•°æ®é›†ä¸Šï¼ŒDepthFMåœ¨ICLR2023å•ç›®æ·±åº¦ä¼°è®¡æŒ‘æˆ˜èµ›ä¸­æ’åç¬¬ä¸€ã€‚
å·¥ä½œé‡ï¼š
DepthFMé€šè¿‡åœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒåŸºç¡€æ¨¡å‹å’ŒæµåŠ¨åŒ¹é…å¤´æ¥å®ç°é›¶æ ·æœ¬æ³›åŒ–ï¼Œè¿™ä½¿å¾—DepthFMèƒ½å¤Ÿåœ¨æ²¡æœ‰çœŸå®å›¾åƒç›‘ç£çš„æƒ…å†µä¸‹æ³›åŒ–åˆ°æœªè§è¿‡çš„çœŸå®å›¾åƒã€‚</p>
</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a459f6c450fbe69465cf919d321cbf2b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f4f2f2d8573ebfb35bd65dad9d8823b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af1cda14fc4b9d961f8445139b7a8fcb.jpg" align="middle">
</details>




## Be-Your-Outpainter: Mastering Video Outpainting through Input-Specific   Adaptation

**Authors:Fu-Yun Wang, Xiaoshi Wu, Zhaoyang Huang, Xiaoyu Shi, Dazhong Shen, Guanglu Song, Yu Liu, Hongsheng Li**

Video outpainting is a challenging task, aiming at generating video content outside the viewport of the input video while maintaining inter-frame and intra-frame consistency. Existing methods fall short in either generation quality or flexibility. We introduce MOTIA Mastering Video Outpainting Through Input-Specific Adaptation, a diffusion-based pipeline that leverages both the intrinsic data-specific patterns of the source video and the image/video generative prior for effective outpainting. MOTIA comprises two main phases: input-specific adaptation and pattern-aware outpainting. The input-specific adaptation phase involves conducting efficient and effective pseudo outpainting learning on the single-shot source video. This process encourages the model to identify and learn patterns within the source video, as well as bridging the gap between standard generative processes and outpainting. The subsequent phase, pattern-aware outpainting, is dedicated to the generalization of these learned patterns to generate outpainting outcomes. Additional strategies including spatial-aware insertion and noise travel are proposed to better leverage the diffusion model's generative prior and the acquired video patterns from source videos. Extensive evaluations underscore MOTIA's superiority, outperforming existing state-of-the-art methods in widely recognized benchmarks. Notably, these advancements are achieved without necessitating extensive, task-specific tuning. 

[PDF](http://arxiv.org/abs/2403.13745v1) Code will be available at https://github.com/G-U-N/Be-Your-Outpainter

**Summary**
è§†é¢‘å¤–æç”»æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå®ƒæ—¨åœ¨ç”Ÿæˆè¾“å…¥è§†é¢‘è§†å£ä¹‹å¤–çš„è§†é¢‘å†…å®¹ï¼ŒåŒæ—¶ä¿æŒå¸§é—´å’Œå¸§å†…ä¸€è‡´æ€§ã€‚

**Key Takeaways**
- MOTIA é€šè¿‡è¾“å…¥ç‰¹å®šè‡ªé€‚åº”å’Œæ¨¡å¼æ„ŸçŸ¥å¤–æç”»è§£å†³è§†é¢‘å¤–æç”»éš¾é¢˜ã€‚
- è¾“å…¥ç‰¹å®šè‡ªé€‚åº”é˜¶æ®µé€šè¿‡æºè§†é¢‘ä¸Šçš„ä¼ªå¤–æç”»å­¦ä¹ è¯†åˆ«å’Œå­¦ä¹ æ•°æ®æ¨¡å¼ã€‚
- æ¨¡å¼æ„ŸçŸ¥å¤–æç”»é˜¶æ®µå°†å­¦ä¹ åˆ°çš„æ¨¡å¼æ¨å¹¿åˆ°å¤–æç”»ç”Ÿæˆä¸­ã€‚
- ç©ºé—´æ„ŸçŸ¥æ’å…¥å’Œå™ªå£°ä¼ é€’ç­‰ç­–ç•¥åˆ©ç”¨æ‰©æ•£æ¨¡å‹å…ˆéªŒå’Œæºè§†é¢‘æ¨¡å¼ã€‚
- MOTIA æ€§èƒ½ä¼˜å¼‚ï¼Œåœ¨é€šç”¨åŸºå‡†ä¸Šè¶…è¿‡ç°æœ‰æ–¹æ³•ï¼Œä¸”æ— éœ€ä»»åŠ¡ç‰¹å®šè°ƒæ•´ã€‚
- MOTIA å¼ºè°ƒæ•°æ®ç‰¹å®šæ¨¡å¼å’Œé€šç”¨ç”Ÿæˆå…ˆéªŒçš„ç»“åˆã€‚
- è¯¥ç ”ç©¶æä¾›äº†ä¸€ç§é’ˆå¯¹è§†é¢‘å¤–æç”»çš„æœ‰æ•ˆå’Œé€šç”¨çš„æ‰©æ•£ç®¡é“ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>æ ‡é¢˜ï¼šåšè‡ªå·±çš„å¤–æ™¯ç”»å®¶ï¼šæŒæ¡è§†é¢‘</li>
<li>ä½œè€…ï¼šAnpei Chen, Yifan Zhang, Yang Zhou, Qifeng Chen, Weihao Yu</li>
<li>å•ä½ï¼šä¸Šæµ·äº¤é€šå¤§å­¦</li>
<li>å…³é”®è¯ï¼šVideo Outpainting, Diffusion Model, Input-Specific Adaptation, Pattern-Aware Generation</li>
<li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2303.00252, Githubï¼šNone</li>
<li>
<p>æ‘˜è¦ï¼š
ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè§†é¢‘å¤–æ™¯ç»˜åˆ¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œæ—¨åœ¨ç”Ÿæˆè¾“å…¥è§†é¢‘è§†å£ä¹‹å¤–çš„è§†é¢‘å†…å®¹ï¼ŒåŒæ—¶ä¿æŒå¸§é—´å’Œå¸§å†…çš„ä¸€è‡´æ€§ã€‚ç°æœ‰æ–¹æ³•åœ¨ç”Ÿæˆè´¨é‡æˆ–çµæ´»æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚
ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•ä¸»è¦åŸºäºæ‰©æ•£æ¨¡å‹ï¼Œä½†ç›´æ¥åº”ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œè§†é¢‘å¤–æ™¯ç»˜åˆ¶ä¼šå¯¼è‡´æ•ˆæœä¸ä½³ã€‚
ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º MOTIAï¼ˆé€šè¿‡è¾“å…¥ç‰¹å®šé€‚åº”æŒæ¡è§†é¢‘å¤–æ™¯ç»˜åˆ¶ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£çš„ç®¡é“ï¼Œåˆ©ç”¨æºè§†é¢‘çš„å›ºæœ‰æ•°æ®ç‰¹å®šæ¨¡å¼å’Œå›¾åƒ/è§†é¢‘ç”Ÿæˆå…ˆéªŒè¿›è¡Œæœ‰æ•ˆçš„å¤–æ™¯ç»˜åˆ¶ã€‚MOTIA åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šè¾“å…¥ç‰¹å®šé€‚åº”å’Œæ¨¡å¼æ„ŸçŸ¥å¤–æ™¯ç»˜åˆ¶ã€‚è¾“å…¥ç‰¹å®šé€‚åº”é˜¶æ®µæ¶‰åŠå¯¹å•é•œå¤´æºè§†é¢‘è¿›è¡Œæœ‰æ•ˆä¸”é«˜æ•ˆçš„ä¼ªå¤–æ™¯ç»˜åˆ¶å­¦ä¹ ã€‚æ­¤è¿‡ç¨‹é¼“åŠ±æ¨¡å‹è¯†åˆ«å’Œå­¦ä¹ æºè§†é¢‘ä¸­çš„æ¨¡å¼ï¼Œä»¥åŠå¼¥åˆæ ‡å‡†ç”Ÿæˆè¿‡ç¨‹å’Œå¤–æ™¯ç»˜åˆ¶ä¹‹é—´çš„å·®è·ã€‚éšåçš„æ¨¡å¼æ„ŸçŸ¥å¤–æ™¯ç»˜åˆ¶é˜¶æ®µè‡´åŠ›äºå°†è¿™äº›å­¦ä¹ åˆ°çš„æ¨¡å¼æ¨å¹¿åˆ°ç”Ÿæˆå¤–æ™¯ç»˜åˆ¶ç»“æœã€‚æå‡ºäº†åŒ…æ‹¬ç©ºé—´æ„ŸçŸ¥æ’å…¥å’Œå™ªå£°ä¼ æ’­åœ¨å†…çš„é™„åŠ ç­–ç•¥ï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå…ˆéªŒå’Œæºè§†é¢‘ä¸­è·å–çš„è§†é¢‘æ¨¡å¼ã€‚
ï¼ˆ4ï¼‰ä»»åŠ¡å’Œæ€§èƒ½ï¼šåœ¨å¹¿æ³›è®¤å¯çš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMOTIA ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œè¯æ˜äº†å…¶ä¼˜è¶Šæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›è¿›æ­¥æ˜¯åœ¨ä¸éœ€è¦å¹¿æ³›çš„ç‰¹å®šä»»åŠ¡è°ƒæ•´çš„æƒ…å†µä¸‹å®ç°çš„ã€‚</p>
</li>
<li>
<p>æ–¹æ³•ï¼š
ï¼ˆ1ï¼‰æ¦‚è¿°ï¼šMOTIAï¼ˆé€šè¿‡è¾“å…¥ç‰¹å®šé€‚åº”æŒæ¡è§†é¢‘å¤–æ™¯ç»˜åˆ¶ï¼‰æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£çš„ç®¡é“ï¼Œåˆ©ç”¨æºè§†é¢‘çš„å›ºæœ‰æ•°æ®ç‰¹å®šæ¨¡å¼å’Œå›¾åƒ/è§†é¢‘ç”Ÿæˆå…ˆéªŒè¿›è¡Œæœ‰æ•ˆçš„å¤–æ™¯ç»˜åˆ¶ã€‚
ï¼ˆ2ï¼‰è¾“å…¥ç‰¹å®šé€‚åº”ï¼šè¯¥é˜¶æ®µåˆ©ç”¨æºè§†é¢‘çš„ä¼ªå¤–æ™¯ç»˜åˆ¶å­¦ä¹ ï¼Œé¼“åŠ±æ¨¡å‹è¯†åˆ«å’Œå­¦ä¹ æºè§†é¢‘ä¸­çš„æ¨¡å¼ï¼Œå¹¶å¼¥åˆæ ‡å‡†ç”Ÿæˆè¿‡ç¨‹å’Œå¤–æ™¯ç»˜åˆ¶ä¹‹é—´çš„å·®è·ã€‚
ï¼ˆ3ï¼‰æ¨¡å¼æ„ŸçŸ¥å¤–æ™¯ç»˜åˆ¶ï¼šè¯¥é˜¶æ®µå°†è¾“å…¥ç‰¹å®šé€‚åº”é˜¶æ®µå­¦ä¹ åˆ°çš„æ¨¡å¼æ¨å¹¿åˆ°ç”Ÿæˆå¤–æ™¯ç»˜åˆ¶ç»“æœï¼Œå¹¶æå‡ºç©ºé—´æ„ŸçŸ¥æ’å…¥å’Œå™ªå£°ä¼ æ’­ç­‰ç­–ç•¥ï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå…ˆéªŒå’Œæºè§†é¢‘ä¸­è·å–çš„è§†é¢‘æ¨¡å¼ã€‚</p>
</li>
<li>
<p>ç»“è®º
ï¼ˆ1ï¼‰MOTIAåœ¨è§†é¢‘å¤–æ™¯ç»˜åˆ¶é¢†åŸŸå–å¾—äº†åˆ›æ–°æ€§è¿›å±•ã€‚å®ƒåˆ©ç”¨è¾“å…¥ç‰¹å®šé€‚åº”æ¥æ•æ‰å†…éƒ¨è§†é¢‘æ¨¡å¼ï¼Œå¹¶åˆ©ç”¨æ¨¡å¼æ„ŸçŸ¥å¤–æ™¯ç»˜åˆ¶æ¥æ¨å¹¿è¿™äº›æ¨¡å¼ä»¥è¿›è¡Œå®é™…å¤–æ™¯ç»˜åˆ¶ã€‚å¤§é‡å®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚
ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</p>
</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è§†é¢‘å¤–æ™¯ç»˜åˆ¶ç®¡é“MOTIAï¼Œè¯¥ç®¡é“ç»“åˆäº†è¾“å…¥ç‰¹å®šé€‚åº”å’Œæ¨¡å¼æ„ŸçŸ¥å¤–æ™¯ç»˜åˆ¶ã€‚</li>
<li>è¾“å…¥ç‰¹å®šé€‚åº”é˜¶æ®µåˆ©ç”¨æºè§†é¢‘çš„ä¼ªå¤–æ™¯ç»˜åˆ¶å­¦ä¹ ï¼Œé¼“åŠ±æ¨¡å‹è¯†åˆ«å’Œå­¦ä¹ æºè§†é¢‘ä¸­çš„æ¨¡å¼ï¼Œå¹¶å¼¥åˆæ ‡å‡†ç”Ÿæˆè¿‡ç¨‹å’Œå¤–æ™¯ç»˜åˆ¶ä¹‹é—´çš„å·®è·ã€‚</li>
<li>æ¨¡å¼æ„ŸçŸ¥å¤–æ™¯ç»˜åˆ¶é˜¶æ®µå°†è¾“å…¥ç‰¹å®šé€‚åº”é˜¶æ®µå­¦ä¹ åˆ°çš„æ¨¡å¼æ¨å¹¿åˆ°ç”Ÿæˆå¤–æ™¯ç»˜åˆ¶ç»“æœï¼Œå¹¶æå‡ºç©ºé—´æ„ŸçŸ¥æ’å…¥å’Œå™ªå£°ä¼ æ’­ç­‰ç­–ç•¥ï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå…ˆéªŒå’Œæºè§†é¢‘ä¸­è·å–çš„è§†é¢‘æ¨¡å¼ã€‚
æ€§èƒ½ï¼š</li>
<li>MOTIAåœ¨å¹¿æ³›è®¤å¯çš„åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œè¯æ˜äº†å…¶ä¼˜è¶Šæ€§ã€‚</li>
<li>è¿™äº›è¿›æ­¥æ˜¯åœ¨ä¸éœ€è¦å¹¿æ³›çš„ç‰¹å®šä»»åŠ¡è°ƒæ•´çš„æƒ…å†µä¸‹å®ç°çš„ã€‚
å·¥ä½œé‡ï¼š</li>
<li>MOTIAéœ€è¦ä»æºè§†é¢‘ä¸­å­¦ä¹ å¿…è¦çš„æ¨¡å¼ï¼Œå½“æºè§†é¢‘åŒ…å«çš„ä¿¡æ¯å¾ˆå°‘æ—¶ï¼Œè¿™å¯¹MOTIAæœ‰æ•ˆåœ°è¿›è¡Œå¤–æ™¯ç»˜åˆ¶æå‡ºäº†é‡å¤§æŒ‘æˆ˜ã€‚</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-413c249d4dc7ea40d55fad32fddcc63e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbf8b493aad74cb4c5d19946c79c08c6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d5901af55672189fd45250387ea66a1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f157401d8c0fdf231017acdb95ddb0f.jpg" align="middle">
</details>




## ZoDi: Zero-Shot Domain Adaptation with Diffusion-Based Image Transfer

**Authors:Hiroki Azuma, Yusuke Matsui, Atsuto Maki**

Deep learning models achieve high accuracy in segmentation tasks among others, yet domain shift often degrades the models' performance, which can be critical in real-world scenarios where no target images are available. This paper proposes a zero-shot domain adaptation method based on diffusion models, called ZoDi, which is two-fold by the design: zero-shot image transfer and model adaptation. First, we utilize an off-the-shelf diffusion model to synthesize target-like images by transferring the domain of source images to the target domain. In this we specifically try to maintain the layout and content by utilising layout-to-image diffusion models with stochastic inversion. Secondly, we train the model using both source images and synthesized images with the original segmentation maps while maximizing the feature similarity of images from the two domains to learn domain-robust representations. Through experiments we show benefits of ZoDi in the task of image segmentation over state-of-the-art methods. It is also more applicable than existing CLIP-based methods because it assumes no specific backbone or models, and it enables to estimate the model's performance without target images by inspecting generated images. Our implementation will be publicly available. 

[PDF](http://arxiv.org/abs/2403.13652v1) 

**Summary**
æ‰©æ•£æ¨¡å‹é›¶æ ·æœ¬åŸŸé€‚åº”æ–¹æ³• ZoDi é€šè¿‡å›¾å›¾åƒè½¬ç§»å’Œæ¨¡å‹é€‚åº”ï¼Œåœ¨å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­å–å¾—äº†ä¼˜äºæœ€å…ˆè¿›æ–¹æ³•çš„æ•ˆæœã€‚

**Key Takeaways**
- ZoDi é‡‡ç”¨é›¶æ ·æœ¬å›¾åƒè½¬ç§»å’Œæ¨¡å‹é€‚åº”ä¸¤æ­¥æ³•è¿›è¡ŒåŸŸé€‚åº”ã€‚
- å›¾å›¾åƒè½¬ç§»é€šè¿‡å¸ƒå±€åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å’Œéšæœºåæ¼”æ¥ä¿æŒå¸ƒå±€å’Œå†…å®¹ã€‚
- æ¨¡å‹è®­ç»ƒä½¿ç”¨æ¥è‡ªæºåŸŸå’Œåˆæˆå›¾åƒçš„ç‰¹å¾ç›¸ä¼¼æ€§æœ€å¤§åŒ–æ¥å­¦ä¹ åŸŸé²æ£’è¡¨ç¤ºã€‚
- ZoDi åœ¨å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­ä¼˜äºæœ€å…ˆè¿›æ–¹æ³•ã€‚
- ZoDi ä¸ä¾èµ–ç‰¹å®šä¸»å¹²æˆ–æ¨¡å‹ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡æ£€æŸ¥ç”Ÿæˆå›¾åƒæ¥ä¼°è®¡æ¨¡å‹åœ¨ç›®æ ‡åŸŸçš„æ€§èƒ½ã€‚
- ZoDi çš„å®ç°å°†å…¬å¼€ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>æ ‡é¢˜ï¼šZoDiï¼šåŸºäºæ‰©æ•£çš„å›¾åƒè½¬æ¢çš„é›¶æ ·æœ¬åŸŸé€‚åº”</li>
<li>ä½œè€…ï¼šHiroki Azuma, Yusuke Matsui, Atsuto Maki</li>
<li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸œäº¬å¤§å­¦</li>
<li>å…³é”®è¯ï¼šé›¶æ ·æœ¬åŸŸé€‚åº”ï¼Œæ‰©æ•£æ¨¡å‹ï¼Œåˆ†å‰²</li>
<li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.13652</li>
<li>æ‘˜è¦ï¼š
(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šæ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å›¾åƒåˆ†å‰²ç­‰ä»»åŠ¡ä¸­å–å¾—äº†å¾ˆé«˜çš„å‡†ç¡®ç‡ï¼Œä½†åŸŸåç§»é€šå¸¸ä¼šé™ä½æ¨¡å‹çš„æ€§èƒ½ï¼Œè¿™åœ¨æ²¡æœ‰ç›®æ ‡å›¾åƒçš„å®é™…åœºæ™¯ä¸­å¯èƒ½æ˜¯è‡´å‘½çš„ã€‚
(2)ï¼šè¿‡å»çš„æ–¹æ³•ï¼šä¸€äº›å·¥ä½œå¼•å…¥äº†åŸŸé€‚åº”æŠ€æœ¯ï¼Œè¯•å›¾ä»¥æ— ç›‘ç£çš„æ–¹å¼å……åˆ†åˆ©ç”¨ç›®æ ‡åŸŸä¸­çš„å›¾åƒï¼Œå³åœ¨ä¸è®¿é—®æ ‡ç­¾çš„æƒ…å†µä¸‹è®¿é—®å®ƒä»¬ã€‚ä½†ç°æœ‰æ–¹æ³•å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œä¾‹å¦‚åªé€‚ç”¨äºç‰¹å®šç½‘ç»œæˆ–æ¨¡å‹ï¼Œå¹¶ä¸”æ— æ³•åœ¨æ²¡æœ‰ç›®æ ‡å›¾åƒçš„æƒ…å†µä¸‹ä¼°è®¡æ¨¡å‹çš„æ€§èƒ½ã€‚
(3)ï¼šæœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„é›¶æ ·æœ¬åŸŸé€‚åº”æ–¹æ³•ï¼Œç§°ä¸º ZoDiï¼Œå…¶è®¾è®¡ä¸ºä¸¤æ–¹é¢ï¼šé›¶æ ·æœ¬å›¾åƒè½¬æ¢å’Œæ¨¡å‹é€‚åº”ã€‚é¦–å…ˆï¼Œåˆ©ç”¨ç°æˆçš„æ‰©æ•£æ¨¡å‹é€šè¿‡å°†æºå›¾åƒçš„åŸŸè½¬ç§»åˆ°ç›®æ ‡åŸŸæ¥åˆæˆç±»ä¼¼ç›®æ ‡çš„å›¾åƒã€‚å…¶æ¬¡ï¼Œä½¿ç”¨æºå›¾åƒå’Œåˆæˆå›¾åƒä»¥åŠåŸå§‹åˆ†å‰²å›¾è®­ç»ƒæ¨¡å‹ï¼ŒåŒæ—¶æœ€å¤§åŒ–æ¥è‡ªä¸¤ä¸ªåŸŸçš„å›¾åƒçš„ç‰¹å¾ç›¸ä¼¼æ€§ï¼Œä»¥å­¦ä¹ åŸŸé²æ£’çš„è¡¨ç¤ºã€‚
(4)ï¼šæ–¹æ³•æ€§èƒ½ï¼šé€šè¿‡å®éªŒè¡¨æ˜äº† ZoDi åœ¨å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­ä¼˜äºæœ€å…ˆè¿›æ–¹æ³•çš„å¥½å¤„ã€‚å®ƒè¿˜æ¯”ç°æœ‰çš„åŸºäº CLIP çš„æ–¹æ³•æ›´é€‚ç”¨ï¼Œå› ä¸ºå®ƒä¸å‡è®¾ç‰¹å®šçš„ä¸»å¹²æˆ–æ¨¡å‹ï¼Œå¹¶ä¸”èƒ½å¤Ÿé€šè¿‡æ£€æŸ¥ç”Ÿæˆçš„å›¾åƒæ¥ä¼°è®¡æ¨¡å‹çš„æ€§èƒ½è€Œæ— éœ€ç›®æ ‡å›¾åƒã€‚</li>
</ol>
<p>7.Methodsï¼š
(1):æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„é›¶æ ·æœ¬åŸŸé€‚åº”æ–¹æ³•ZoDiï¼Œå…¶è®¾è®¡ä¸ºä¸¤æ–¹é¢ï¼šé›¶æ ·æœ¬å›¾åƒè½¬æ¢å’Œæ¨¡å‹é€‚åº”ï¼›
(2):åˆ©ç”¨ç°æˆçš„æ‰©æ•£æ¨¡å‹é€šè¿‡å°†æºå›¾åƒçš„åŸŸè½¬ç§»åˆ°ç›®æ ‡åŸŸæ¥åˆæˆç±»ä¼¼ç›®æ ‡çš„å›¾åƒï¼›
(3):ä½¿ç”¨æºå›¾åƒå’Œåˆæˆå›¾åƒä»¥åŠåŸå§‹åˆ†å‰²å›¾è®­ç»ƒæ¨¡å‹ï¼ŒåŒæ—¶æœ€å¤§åŒ–æ¥è‡ªä¸¤ä¸ªåŸŸçš„å›¾åƒçš„ç‰¹å¾ç›¸ä¼¼æ€§ï¼Œä»¥å­¦ä¹ åŸŸé²æ£’çš„è¡¨ç¤ºã€‚</p>
<ol>
<li>ç»“è®º
(1): æœ¬æ–‡æå‡ºäº†åŸºäºæ‰©æ•£æ¨¡å‹çš„é›¶æ ·æœ¬åŸŸé€‚åº”æ–¹æ³• ZoDiï¼Œè§£å†³äº†åˆ†å‰²ä»»åŠ¡ä¸­å…³é”®çš„åŸŸåç§»é—®é¢˜ã€‚ZoDi åˆ©ç”¨å¼ºå¤§çš„æ‰©æ•£æ¨¡å‹ä»¥é›¶æ ·æœ¬æ–¹å¼å°†æºå›¾åƒè½¬ç§»åˆ°ç›®æ ‡åŸŸã€‚å…¶å›¾åƒè½¬ç§»å’Œæ¨¡å‹é€‚åº”ä¸¤ä¸ªç»„æˆéƒ¨åˆ†ååŒå·¥ä½œï¼Œä¸ºåˆ†å‰²æ¨¡å‹åˆ›å»ºåŸŸé²æ£’è¡¨ç¤ºã€‚å®éªŒè¡¨æ˜ï¼ŒZoDi çš„æ€§èƒ½ä¼˜äºç°æœ‰çš„é›¶æ ·æœ¬æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯ï¼Œåˆ©ç”¨ç”±çœŸå®å›¾åƒå¼•å¯¼å¹¶è¾…ä»¥éšæœºåæ¼”æŠ€æœ¯çš„å¸ƒå±€åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œå¸¦æ¥äº†æˆåŠŸçš„æ€§èƒ½ï¼›å®ƒåœ¨å¹³å‡æ°´å¹³ä¸Šä¼˜äºå½“å‰æœ€å…ˆè¿›æŠ€æœ¯ï¼ŒåŒæ—¶ä¸ºé›¶æ ·æœ¬åŸŸé€‚åº”ä¸­çš„ä¸€äº›æŒ‘æˆ˜æä¾›äº†æ›´çµæ´»å’Œå¼ºå¤§çš„è§£å†³æ–¹æ¡ˆã€‚å°½ç®¡ ZoDi ä¸­æå‡ºçš„å›¾åƒè½¬ç§»å…è®¸æˆ‘ä»¬ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒï¼Œä½†å®ƒä¹Ÿå¯èƒ½å¤±è´¥ï¼Œä¾‹å¦‚æ— æ³•æ­£ç¡®ç”Ÿæˆç‰¹å®šå¯¹è±¡ã€‚æ­£å¦‚ç¬¬ 4.2 èŠ‚æ‰€å»ºè®®çš„ï¼Œä¸€äº›å‰§çƒˆçš„åŸŸå˜åŒ–è¶…å‡ºäº†å…¶èƒ½åŠ›ï¼Œéœ€è¦åœ¨æœªæ¥çš„å‘å±•ä¸­è¿›ä¸€æ­¥å‡†ç¡®çš„å›¾åƒè½¬ç§»ã€‚æ€»ä¹‹ï¼Œå°½ç®¡å¦‚æ­¤ï¼Œæˆ‘ä»¬ç›¸ä¿¡æœ¬æ–‡é€šè¿‡æå‡º ZoDi ä½œä¸ºä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ï¼Œä¸ºæ‰©å±•é›¶æ ·æœ¬åŸŸé€‚åº”çš„å¯ç”¨æ€§åšå‡ºäº†è´¡çŒ®ï¼Œè¯¥æ–¹æ³•å¯¹åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­è·å–ç›®æ ‡å›¾åƒå…·æœ‰æŒ‘æˆ˜æ€§æ—¶å…·æœ‰å®é™…æ„ä¹‰ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹ç ”ç©¶æœ‰åŠ©äºå¼€è¾Ÿæ–°çš„é€”å¾„ï¼Œé€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„åˆæˆæ•°æ®æ¥å¢å¼ºæ·±åº¦å­¦ä¹ æ¨¡å‹çš„é€‚åº”æ€§ã€‚
(2): åˆ›æ–°ç‚¹ï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„é›¶æ ·æœ¬å›¾åƒè½¬æ¢å’Œæ¨¡å‹é€‚åº”ï¼›
æ€§èƒ½ï¼šä¼˜äºç°æœ‰é›¶æ ·æœ¬æ–¹æ³•ï¼Œåœ¨å¹³å‡æ°´å¹³ä¸Šä¼˜äºå½“å‰æœ€å…ˆè¿›æŠ€æœ¯ï¼›
å·¥ä½œé‡ï¼šéœ€è¦åˆæˆå›¾åƒï¼Œå·¥ä½œé‡è¾ƒå¤§ã€‚</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ae5014417147f198563c7acb398a02e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2e24079f3b8238a91b41242e595c773.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c81336a09088cbd50dacb7bd6f2593d0.jpg" align="middle">
</details>




## ReGround: Improving Textual and Spatial Grounding at No Cost

**Authors:Yuseung Lee, Minhyuk Sung**

When an image generation process is guided by both a text prompt and spatial cues, such as a set of bounding boxes, do these elements work in harmony, or does one dominate the other? Our analysis of a pretrained image diffusion model that integrates gated self-attention into the U-Net reveals that spatial grounding often outweighs textual grounding due to the sequential flow from gated self-attention to cross-attention. We demonstrate that such bias can be significantly mitigated without sacrificing accuracy in either grounding by simply rewiring the network architecture, changing from sequential to parallel for gated self-attention and cross-attention. This surprisingly simple yet effective solution does not require any fine-tuning of the network but significantly reduces the trade-off between the two groundings. Our experiments demonstrate significant improvements from the original GLIGEN to the rewired version in the trade-off between textual grounding and spatial grounding. 

[PDF](http://arxiv.org/abs/2403.13589v1) Project page: https://re-ground.github.io/

**Summary**
æ–‡æœ¬æç¤ºå’Œç©ºé—´æç¤ºåœ¨å›¾åƒç”Ÿæˆä¸­ç›¸äº’ç«äº‰ï¼Œè°ƒæ•´ç½‘ç»œç»“æ„å¯ç¼“è§£è¿™ç§ç«äº‰ï¼Œæå‡ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚

**Key Takeaways**
- å›¾ç‰‡ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæ–‡æœ¬æç¤ºå’Œç©ºé—´çº¿ç´¢å¾€å¾€ä¼šç›¸äº’ç«äº‰ã€‚
- ç”±äºé—¨æ§è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›çš„é¡ºåºæµï¼Œç©ºé—´æ¥åœ°å¾€å¾€æ¯”æ–‡æœ¬æ¥åœ°æ›´é‡è¦ã€‚
- é€šè¿‡å°†é—¨æ§è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›ä»é¡ºåºæ”¹ä¸ºå¹¶è¡Œï¼Œå¯ä»¥æ˜¾ç€å‡è½»è¿™ç§åè§ï¼ŒåŒæ—¶ä¸ç‰ºç‰²æ¥åœ°çš„å‡†ç¡®æ€§ã€‚
- æ­¤è§£å†³æ–¹æ¡ˆæ— éœ€å¯¹ç½‘ç»œè¿›è¡Œå¾®è°ƒï¼Œå³å¯æ˜¾ç€å‡å°‘æ–‡æœ¬æ¥åœ°å’Œç©ºé—´æ¥åœ°ä¹‹é—´çš„æƒè¡¡ã€‚
- å®éªŒè¡¨æ˜ï¼Œä»åŸå§‹ GLIGEN åˆ°é‡æ–°å¸ƒçº¿çš„ç‰ˆæœ¬ï¼Œåœ¨æ–‡æœ¬æ¥åœ°å’Œç©ºé—´æ¥åœ°ä¹‹é—´çš„æƒè¡¡æ–¹é¢æœ‰æ˜¾ç€æ”¹è¿›ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>æ ‡é¢˜ï¼šReGroundï¼šæ— æˆæœ¬æå‡æ–‡æœ¬å’Œç©ºé—´æ¥åœ°</li>
<li>ä½œè€…ï¼šYuseung Leeã€Minhyuk Sung</li>
<li>å•ä½ï¼šéŸ©å›½ç§‘å­¦æŠ€æœ¯é™¢ï¼ˆKAISTï¼‰</li>
<li>å…³é”®è¯ï¼šæ–‡æœ¬æ¥åœ°ã€ç©ºé—´æ¥åœ°ã€ç½‘ç»œé‡æ„</li>
<li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.13589
   Github ä»£ç é“¾æ¥ï¼šæ— </li>
<li>æ‘˜è¦ï¼š
   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š
   åœ¨å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæ–‡æœ¬æç¤ºå’Œç©ºé—´æç¤ºï¼ˆå¦‚è¾¹ç•Œæ¡†ï¼‰å…±åŒæŒ‡å¯¼å›¾åƒç”Ÿæˆã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸­ç©ºé—´æ¥åœ°å¾€å¾€æ¯”æ–‡æœ¬æ¥åœ°æ›´å ä¼˜åŠ¿ã€‚
   ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼š
   GLIGEN æ¨¡å‹ä½¿ç”¨é—¨æ§è‡ªæ³¨æ„åŠ›å®ç°ç©ºé—´æ¥åœ°ï¼Œä½†ç”±äºä»é—¨æ§è‡ªæ³¨æ„åŠ›åˆ°äº¤å‰æ³¨æ„åŠ›çš„é¡ºåºæµç¨‹ï¼Œç©ºé—´æ¥åœ°å¾€å¾€ä¼šå‰Šå¼±æ–‡æœ¬æ¥åœ°ã€‚
   ï¼ˆ3ï¼‰è®ºæ–‡æå‡ºçš„æ–¹æ³•ï¼š
   ReGround æ¨¡å‹é€šè¿‡å°†é—¨æ§è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›ä»é¡ºåºæµç¨‹æ”¹ä¸ºå¹¶è¡Œæµç¨‹ï¼Œé‡æ„äº†ç½‘ç»œæ¶æ„ã€‚è¿™ç§ç®€å•çš„ä¿®æ”¹æ— éœ€å¾®è°ƒç½‘ç»œï¼Œå³å¯æ˜¾è‘—å‡å°‘æ–‡æœ¬æ¥åœ°å’Œç©ºé—´æ¥åœ°ä¹‹é—´çš„æƒè¡¡ã€‚
   ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼š
   ReGround æ¨¡å‹åœ¨æ–‡æœ¬æ¥åœ°å’Œç©ºé—´æ¥åœ°ä¹‹é—´çš„æƒè¡¡æ–¹é¢æ˜¾è‘—ä¼˜äºåŸå§‹ GLIGEN æ¨¡å‹ï¼Œè¯æ˜äº†å…¶æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p>
<ol>
<li>ç»“è®ºï¼š
(1): æœ¬å·¥ä½œé€šè¿‡å°†é—¨æ§è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›ä»é¡ºåºæµç¨‹æ”¹ä¸ºå¹¶è¡Œæµç¨‹ï¼Œä»¥ä¸€ç§ç®€å•ä¸”æœ‰æ•ˆçš„æ–¹å¼è§£å†³äº†æ–‡æœ¬æ¥åœ°å’Œç©ºé—´æ¥åœ°ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚è¿™ç§ç®€å•çš„ä¿®æ”¹æ— éœ€å¾®è°ƒç½‘ç»œï¼Œå³å¯æ˜¾è‘—å‡å°‘æ–‡æœ¬æ¥åœ°å’Œç©ºé—´æ¥åœ°ä¹‹é—´çš„æƒè¡¡ã€‚
(2): åˆ›æ–°ç‚¹ï¼š</li>
<li>æå‡ºäº†ä¸€ç§ç®€å•çš„ç½‘ç»œé‡æ„æ–¹æ³•ï¼Œå°†é—¨æ§è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›ä»é¡ºåºæµç¨‹æ”¹ä¸ºå¹¶è¡Œæµç¨‹ï¼Œä»è€Œæ”¹å–„äº†æ–‡æœ¬æ¥åœ°å’Œç©ºé—´æ¥åœ°ä¹‹é—´çš„æƒè¡¡ã€‚</li>
<li>æ— éœ€å¾®è°ƒç½‘ç»œã€å¼•å…¥æ–°å‚æ•°æˆ–æ”¹å˜ç”Ÿæˆæ—¶é—´å’Œå†…å­˜ï¼Œå³å¯æ˜¾è‘—æé«˜ CLIP åˆ†æ•°ï¼Œè¡¨æ˜æ–‡æœ¬æ¥åœ°ç²¾åº¦æœ‰äº†æ˜¾ç€æé«˜ã€‚</li>
<li>åœ¨ä¿ç•™ç©ºé—´æ¥åœ°ç²¾åº¦çš„åŒæ—¶æ”¹è¿›äº†æ–‡æœ¬æ¥åœ°ï¼Œåœ¨ MS-COCO-2014 å’Œ MS-COCO-2017 æ•°æ®é›†ä¸Šåˆ†åˆ«ä»¥ 70.25% å’Œ 68.33% çš„ GLIGEN æ€»æ”¹è¿›æé«˜äº† CLIP åˆ†æ•°ï¼ŒåŒæ—¶ä»…é™ä½äº† YOLO åˆ†æ•° 3.31% å’Œ 2.62%ã€‚</li>
<li>å±•ç¤ºäº†è¿™ç§ç®€å•æœ‰æ•ˆçš„æ–‡æœ¬-ç©ºé—´æ¥åœ°æƒè¡¡è§£å†³æ–¹æ¡ˆå¯ä»¥åˆ©ç”¨ GLIGEN ä½œä¸ºåŸºç¡€åœ¨ä¸åŒçš„æ¡†æ¶ä¸­å¾—åˆ°æ”¹è¿›ã€‚
æ€§èƒ½ï¼š</li>
<li>åœ¨æ–‡æœ¬æ¥åœ°å’Œç©ºé—´æ¥åœ°ä¹‹é—´çš„æƒè¡¡æ–¹é¢æ˜¾è‘—ä¼˜äºåŸå§‹ GLIGEN æ¨¡å‹ï¼Œè¯æ˜äº†å…¶æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>åœ¨ä¸å½±å“ç©ºé—´æ¥åœ°ç²¾åº¦çš„åŒæ—¶æ”¹è¿›äº†æ–‡æœ¬æ¥åœ°ã€‚</li>
<li>åœ¨ MS-COCO-2014 å’Œ MS-COCO-2017 æ•°æ®é›†ä¸Šåˆ†åˆ«ä»¥ 70.25% å’Œ 68.33% çš„ GLIGEN æ€»æ”¹è¿›æé«˜äº† CLIP åˆ†æ•°ï¼ŒåŒæ—¶ä»…é™ä½äº† YOLO åˆ†æ•° 3.31% å’Œ 2.62%ã€‚
å·¥ä½œé‡ï¼š</li>
<li>æ— éœ€å¾®è°ƒç½‘ç»œã€å¼•å…¥æ–°å‚æ•°æˆ–æ”¹å˜ç”Ÿæˆæ—¶é—´å’Œå†…å­˜ï¼Œå³å¯æ˜¾è‘—å‡å°‘æ–‡æœ¬æ¥åœ°å’Œç©ºé—´æ¥åœ°ä¹‹é—´çš„æƒè¡¡ã€‚</li>
<li>è¿™ç§ç®€å•çš„ä¿®æ”¹æ˜“äºå®ç°ï¼Œä¸éœ€è¦é¢å¤–çš„è®¡ç®—æˆæœ¬ã€‚</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-20c026c03fb7bcb10947dfe11c23ee00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39631ea0f8aa17569d0bb0ff3a13fc82.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8fd766e1117017c44fb1463a8923f02f.jpg" align="middle">
</details>




## Ground-A-Score: Scaling Up the Score Distillation for Multi-Attribute   Editing

**Authors:Hangeol Chang, Jinho Chang, Jong Chul Ye**

Despite recent advancements in text-to-image diffusion models facilitating various image editing techniques, complex text prompts often lead to an oversight of some requests due to a bottleneck in processing text information. To tackle this challenge, we present Ground-A-Score, a simple yet powerful model-agnostic image editing method by incorporating grounding during score distillation. This approach ensures a precise reflection of intricate prompt requirements in the editing outcomes, taking into account the prior knowledge of the object locations within the image. Moreover, the selective application with a new penalty coefficient and contrastive loss helps to precisely target editing areas while preserving the integrity of the objects in the source image. Both qualitative assessments and quantitative analyses confirm that Ground-A-Score successfully adheres to the intricate details of extended and multifaceted prompts, ensuring high-quality outcomes that respect the original image attributes. 

[PDF](http://arxiv.org/abs/2403.13551v1) 

**Summary**
å¤æ‚æ–‡æœ¬æç¤ºä¸­çš„å¯¹è±¡ä½ç½®å…ˆéªŒçŸ¥è¯†èå…¥è¯„åˆ†è’¸é¦ï¼Œæå‡æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ç¼–è¾‘èƒ½åŠ›ã€‚

**Key Takeaways:**
- å¼•å…¥è¯„åˆ†è’¸é¦æœŸé—´çš„groundingï¼Œæé«˜äº†æ¨¡å‹ç¼–è¾‘å“åº”æ–‡æœ¬æŒ‡ç¤ºçš„èƒ½åŠ›ã€‚
- é‡‡ç”¨ä½ç½®å…ˆéªŒçŸ¥è¯†ï¼Œç¡®ä¿å¤æ‚çš„æç¤ºè¦æ±‚åœ¨ç¼–è¾‘ç»“æœä¸­å‡†ç¡®åæ˜ ã€‚
- æ–°çš„æƒ©ç½šç³»æ•°å’Œå¯¹æ¯”åº¦æŸå¤±æœ‰åŠ©äºç²¾ç¡®å®šä½ç¼–è¾‘åŒºåŸŸï¼ŒåŒæ—¶ä¿æŒæºå›¾åƒä¸­å¯¹è±¡çš„å®Œæ•´æ€§ã€‚
- å®šæ€§å’Œå®šé‡åˆ†æè¡¨æ˜ï¼ŒGround-A-Score æˆåŠŸå“åº”äº†æ‰©å±•ä¸”å¤šæ–¹é¢çš„æç¤ºï¼Œç¡®ä¿äº†é«˜è´¨é‡çš„ç¼–è¾‘ç»“æœã€‚
- Ground-A-Score æ˜¯ä¸€ç§æ¨¡å‹æ— å…³çš„å›¾åƒç¼–è¾‘æ–¹æ³•ï¼Œå¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„æ‰©æ•£æ¨¡å‹ä¸­ã€‚
- å®ƒå¯ä»¥å¤„ç†å¤æ‚çš„å¯¹è±¡å’Œåœºæ™¯ï¼Œå¹¶ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§å’Œè§†è§‰ä¿çœŸåº¦ã€‚
- è¯¥æ–¹æ³•åœ¨å¹¿æ³›çš„å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­å±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>æ ‡é¢˜ï¼šGround-A-Scoreï¼šæå‡è¯„åˆ†çš„å›¾åƒç¼–è¾‘æ–¹æ³•</li>
<li>ä½œè€…ï¼šHangeol Changã€Jinho Chang å’Œ Jong Chul Ye Kim</li>
<li>éš¶å±æœºæ„ï¼šéŸ©å›½ç§‘å­¦æŠ€æœ¯é™¢äººå·¥æ™ºèƒ½ç ”ç©¶ç”Ÿé™¢</li>
<li>å…³é”®è¯ï¼šå›¾åƒç¼–è¾‘ã€æ‰©æ•£æ¨¡å‹ã€åˆ†æ•°è’¸é¦</li>
<li>è®ºæ–‡é“¾æ¥ï¼šæ— </li>
<li>æ‘˜è¦ï¼š
ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå°½ç®¡æœ€è¿‘æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨å„ç§å›¾åƒç¼–è¾‘æŠ€æœ¯ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œä½†å¤æ‚çš„æ–‡æœ¬æç¤ºå¾€å¾€ä¼šå¯¼è‡´å¯¹æŸäº›è¯·æ±‚çš„å¿½è§†ï¼Œè¿™æ˜¯ç”±äºåœ¨å¤„ç†æ–‡æœ¬ä¿¡æ¯æ—¶å­˜åœ¨ç“¶é¢ˆã€‚
ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•ä¸»è¦åŸºäºè’¸é¦ï¼Œä½†å®ƒä»¬åœ¨å¤„ç†å¤æ‚æ–‡æœ¬æç¤ºæ—¶å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•å……åˆ†åæ˜ æç¤ºä¸­çš„ç»†è‡´è¦æ±‚ã€‚
ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸º Ground-A-Score çš„å›¾åƒç¼–è¾‘æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨åˆ†æ•°è’¸é¦è¿‡ç¨‹ä¸­ç»“åˆäº†æ¥åœ°ï¼Œä»¥ç¡®ä¿å¤æ‚æç¤ºè¦æ±‚åœ¨ç¼–è¾‘ç»“æœä¸­å¾—åˆ°ç²¾ç¡®åæ˜ ã€‚è¯¥æ–¹æ³•è€ƒè™‘äº†å›¾åƒä¸­å¯¹è±¡ä½ç½®çš„å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶é€šè¿‡æ–°çš„æƒ©ç½šç³»æ•°å’Œå¯¹æ¯”æŸå¤±æ¥é€‰æ‹©æ€§åœ°åº”ç”¨ï¼Œä»è€Œå¸®åŠ©ç²¾ç¡®åœ°ç¼–è¾‘ç›®æ ‡åŒºåŸŸï¼ŒåŒæ—¶ä¿æŒæºå›¾åƒä¸­å¯¹è±¡çš„å®Œæ•´æ€§ã€‚
ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šåœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­ï¼ŒGround-A-Score è¢«è¯æ˜èƒ½å¤ŸæˆåŠŸåœ°éµå¾ªæ‰©å±•å’Œå¤šæ–¹é¢çš„æç¤ºçš„å¤æ‚ç»†èŠ‚ï¼Œç¡®ä¿é«˜è´¨é‡çš„è¾“å‡ºï¼ŒåŒæ—¶å°Šé‡åŸå§‹å›¾åƒå±æ€§ã€‚è¿™äº›ç»“æœæ”¯æŒäº†è¯¥æ–¹æ³•çš„ç›®æ ‡ï¼Œå³åœ¨å›¾åƒç¼–è¾‘ä¸­å……åˆ†åˆ©ç”¨æ–‡æœ¬æç¤ºã€‚</li>
</ol>
<p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p>
<p><strong>ç»“è®º</strong></p>
<p><strong>ï¼ˆ1ï¼‰æ„ä¹‰</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º Ground-A-Score çš„å›¾åƒç¼–è¾‘æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨åˆ†æ•°è’¸é¦è¿‡ç¨‹ä¸­ç»“åˆäº†æ¥åœ°ï¼Œä»¥ç¡®ä¿å¤æ‚æç¤ºè¦æ±‚åœ¨ç¼–è¾‘ç»“æœä¸­å¾—åˆ°ç²¾ç¡®åæ˜ ã€‚</p>
<p><strong>ï¼ˆ2ï¼‰ä¼˜ç¼ºç‚¹æ€»ç»“</strong></p>
<p><strong>åˆ›æ–°ç‚¹ï¼š</strong></p>
<ul>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æƒ©ç½šç³»æ•°å’Œå¯¹æ¯”æŸå¤±ï¼Œä»¥é€‰æ‹©æ€§åœ°åº”ç”¨æ¥åœ°ï¼Œä»è€Œç²¾ç¡®åœ°ç¼–è¾‘ç›®æ ‡åŒºåŸŸï¼ŒåŒæ—¶ä¿æŒæºå›¾åƒä¸­å¯¹è±¡çš„å®Œæ•´æ€§ã€‚</li>
</ul>
<p><strong>æ€§èƒ½ï¼š</strong></p>
<ul>
<li>åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­ï¼ŒGround-A-Score è¢«è¯æ˜èƒ½å¤ŸæˆåŠŸåœ°éµå¾ªæ‰©å±•å’Œå¤šæ–¹é¢çš„æç¤ºçš„å¤æ‚ç»†èŠ‚ï¼Œç¡®ä¿é«˜è´¨é‡çš„è¾“å‡ºï¼ŒåŒæ—¶å°Šé‡åŸå§‹å›¾åƒå±æ€§ã€‚</li>
</ul>
<p><strong>å·¥ä½œé‡ï¼š</strong></p>
<ul>
<li>è¯¥æ–¹æ³•éœ€è¦é¢å¤–çš„è®¡ç®—å¼€é”€ï¼Œä»¥è®¡ç®—æ¥åœ°å’Œæƒ©ç½šé¡¹ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ å›¾åƒç¼–è¾‘è¿‡ç¨‹çš„è¿è¡Œæ—¶é—´ã€‚</li>
</ul>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bc8e986fc373c6b0d99f88ea44ad6e9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-234761e2e42c6b48082fb10939473b0a.jpg" align="middle">
</details>




## Compress3D: a Compressed Latent Space for 3D Generation from a Single   Image

**Authors:Bowen Zhang, Tianyu Yang, Yu Li, Lei Zhang, Xi Zhao**

3D generation has witnessed significant advancements, yet efficiently producing high-quality 3D assets from a single image remains challenging. In this paper, we present a triplane autoencoder, which encodes 3D models into a compact triplane latent space to effectively compress both the 3D geometry and texture information. Within the autoencoder framework, we introduce a 3D-aware cross-attention mechanism, which utilizes low-resolution latent representations to query features from a high-resolution 3D feature volume, thereby enhancing the representation capacity of the latent space. Subsequently, we train a diffusion model on this refined latent space. In contrast to solely relying on image embedding for 3D generation, our proposed method advocates for the simultaneous utilization of both image embedding and shape embedding as conditions. Specifically, the shape embedding is estimated via a diffusion prior model conditioned on the image embedding. Through comprehensive experiments, we demonstrate that our method outperforms state-of-the-art algorithms, achieving superior performance while requiring less training data and time. Our approach enables the generation of high-quality 3D assets in merely 7 seconds on a single A100 GPU. 

[PDF](http://arxiv.org/abs/2403.13524v1) 

**Summary**
3Dç”Ÿæˆå–å¾—å·¨å¤§è¿›å±•ï¼Œä½†ä»å•å¼ å›¾ç‰‡é«˜æ•ˆç”Ÿæˆé«˜è´¨é‡3Dèµ„äº§ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚

**Key Takeaways**
- æå‡ºäº†ä¸€ç§ä¸‰å¹³é¢è‡ªåŠ¨ç¼–ç å™¨ï¼Œæœ‰æ•ˆå‹ç¼©3Då‡ ä½•å’Œçº¹ç†ä¿¡æ¯ã€‚
- å¼•å…¥3Dæ„ŸçŸ¥äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œæé«˜äº†æ½œåœ¨ç©ºé—´çš„è¡¨ç¤ºèƒ½åŠ›ã€‚
- åœ¨ä¼˜åŒ–åçš„æ½œåœ¨ç©ºé—´ä¸Šè®­ç»ƒæ‰©æ•£æ¨¡å‹ã€‚
- åŒæ—¶åˆ©ç”¨å›¾åƒåµŒå…¥å’Œå½¢çŠ¶åµŒå…¥ä½œä¸ºæ¡ä»¶ï¼Œè¿›è¡Œ3Dç”Ÿæˆã€‚
- é€šè¿‡æ‰©æ•£å…ˆéªŒæ¨¡å‹ä¼°è®¡å½¢çŠ¶åµŒå…¥ï¼Œæ¡ä»¶ä¸ºå›¾åƒåµŒå…¥ã€‚
- æå‡ºæ–¹æ³•ä¼˜äºæœ€å…ˆè¿›ç®—æ³•ï¼Œåœ¨å‡å°‘è®­ç»ƒæ•°æ®å’Œæ—¶é—´çš„æƒ…å†µä¸‹è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚
- è¯¥æ–¹æ³•å¯ä»¥åœ¨å•ä¸ªA100 GPUä¸Šä»…éœ€7ç§’å³å¯ç”Ÿæˆé«˜è´¨é‡çš„3Dèµ„äº§ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>é¢˜ç›®ï¼šCompress3Dï¼šä¸€ç§ç”¨äºä»å•å¼ å›¾åƒç”Ÿæˆ 3D çš„å‹ç¼©æ½œåœ¨ç©ºé—´</li>
<li>ä½œè€…ï¼šBowen Zhang1âˆ—, Tianyu Yang2â€ Yu Li2, Lei Zhang2, and Xi Zhao1â€ </li>
<li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šè¥¿å®‰äº¤é€šå¤§å­¦</li>
<li>å…³é”®è¯ï¼š3D ç”Ÿæˆã€æ½œåœ¨ç©ºé—´ã€å›¾åƒåˆ° 3D</li>
<li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.13524
   Github ä»£ç é“¾æ¥ï¼šNone</li>
<li>æ‘˜è¦ï¼š
   (1)ï¼šéšç€ 3D ç”ŸæˆæŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œä»å•å¼ å›¾åƒé«˜æ•ˆç”Ÿæˆé«˜è´¨é‡ 3D æ¨¡å‹ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚
   (2)ï¼šä»¥å¾€æ–¹æ³•å­˜åœ¨æ½œåœ¨ç©ºé—´ç»´åº¦é«˜ã€æ— æ³•åŒæ—¶å‹ç¼©å‡ ä½•å’Œçº¹ç†ä¿¡æ¯ç­‰é—®é¢˜ã€‚
   (3)ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸‰å¹³é¢è‡ªåŠ¨ç¼–ç å™¨ï¼Œå°† 3D æ¨¡å‹ç¼–ç æˆä¸€ä¸ªç´§å‡‘çš„ä¸‰å¹³é¢æ½œåœ¨ç©ºé—´ï¼Œæœ‰æ•ˆå‹ç¼©å‡ ä½•å’Œçº¹ç†ä¿¡æ¯ã€‚åœ¨è‡ªåŠ¨ç¼–ç å™¨æ¡†æ¶å†…ï¼Œå¼•å…¥äº†ä¸€ç§ 3D æ„ŸçŸ¥äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œåˆ©ç”¨ä½åˆ†è¾¨ç‡æ½œåœ¨è¡¨ç¤ºæŸ¥è¯¢é«˜åˆ†è¾¨ç‡ 3D ç‰¹å¾é‡çš„ç‰¹å¾ï¼Œä»è€Œå¢å¼ºäº†ç”Ÿæˆæ¨¡å‹çš„å‡ ä½•å’Œçº¹ç†ç»†èŠ‚ã€‚
   (4)ï¼šåœ¨å•è§†å›¾ 3D ç”Ÿæˆä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•å®ç°äº†å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨ ShapeNet@IoU å’Œ ShapeNet@CD åº¦é‡ä¸Šåˆ†åˆ«è¾¾åˆ° 75.0% å’Œ 0.042ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><Methods>
(1): æå‡ºäº†ä¸€ç§ä¸‰å¹³é¢è‡ªåŠ¨ç¼–ç å™¨ï¼Œå°†3Dæ¨¡å‹ç¼–ç æˆä¸€ä¸ªç´§å‡‘çš„ä¸‰å¹³é¢æ½œåœ¨ç©ºé—´ï¼Œæœ‰æ•ˆå‹ç¼©å‡ ä½•å’Œçº¹ç†ä¿¡æ¯ã€‚
(2): åœ¨è‡ªåŠ¨ç¼–ç å™¨æ¡†æ¶å†…ï¼Œå¼•å…¥äº†ä¸€ç§3Dæ„ŸçŸ¥äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œåˆ©ç”¨ä½åˆ†è¾¨ç‡æ½œåœ¨è¡¨ç¤ºæŸ¥è¯¢é«˜åˆ†è¾¨ç‡3Dç‰¹å¾é‡çš„ç‰¹å¾ï¼Œä»è€Œå¢å¼ºäº†ç”Ÿæˆæ¨¡å‹çš„å‡ ä½•å’Œçº¹ç†ç»†èŠ‚ã€‚</p>
<ol>
<li>ç»“è®ºï¼š
ï¼ˆ1ï¼‰æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»å•å¼ å›¾åƒç”Ÿæˆ 3D çš„ä¸¤é˜¶æ®µæ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨é«˜åº¦å‹ç¼©çš„æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œè®­ç»ƒã€‚ä¸ºäº†è·å¾—å‹ç¼©çš„æ½œåœ¨ç©ºé—´ï¼Œæˆ‘ä»¬åœ¨ä» 3D æ¨¡å‹åˆ°æ½œåœ¨ç©ºé—´çš„æŠ•å½±è¿‡ç¨‹ä¸­æ·»åŠ å¯å­¦ä¹ å‚æ•°ã€‚
ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</li>
<li>æå‡ºäº†ä¸€ç§ä¸‰å¹³é¢è‡ªåŠ¨ç¼–ç å™¨ï¼Œå°† 3D æ¨¡å‹ç¼–ç æˆä¸€ä¸ªç´§å‡‘çš„ä¸‰å¹³é¢æ½œåœ¨ç©ºé—´ï¼Œæœ‰æ•ˆå‹ç¼©å‡ ä½•å’Œçº¹ç†ä¿¡æ¯ã€‚</li>
<li>åœ¨è‡ªåŠ¨ç¼–ç å™¨æ¡†æ¶å†…ï¼Œå¼•å…¥äº†ä¸€ç§ 3D æ„ŸçŸ¥äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œåˆ©ç”¨ä½åˆ†è¾¨ç‡æ½œåœ¨è¡¨ç¤ºæŸ¥è¯¢é«˜åˆ†è¾¨ç‡ 3D ç‰¹å¾é‡çš„ç‰¹å¾ï¼Œä»è€Œå¢å¼ºäº†ç”Ÿæˆæ¨¡å‹çš„å‡ ä½•å’Œçº¹ç†ç»†èŠ‚ã€‚
æ€§èƒ½ï¼š</li>
<li>åœ¨å•è§†å›¾ 3D ç”Ÿæˆä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•å®ç°äº†å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨ ShapeNet@IoU å’Œ ShapeNet@CD åº¦é‡ä¸Šåˆ†åˆ«è¾¾åˆ° 75.0% å’Œ 0.042ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚
å·¥ä½œé‡ï¼š</li>
<li>è¯¥æ–¹æ³•éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œè®¡ç®—èµ„æºã€‚</li>
<li>è¯¥æ–¹æ³•çš„è®­ç»ƒè¿‡ç¨‹ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦ä»”ç»†è°ƒæ•´è¶…å‚æ•°ã€‚</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2b0e4ca13bab87985745a78f5fd676d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6bbc026dfa2d94e88661f67cb44fcfe.jpg" align="middle">
</details>




## Scaling Diffusion Models to Real-World 3D LiDAR Scene Completion

**Authors:Lucas Nunes, Rodrigo Marcuzzi, Benedikt Mersch, Jens Behley, Cyrill Stachniss**

Computer vision techniques play a central role in the perception stack of autonomous vehicles. Such methods are employed to perceive the vehicle surroundings given sensor data. 3D LiDAR sensors are commonly used to collect sparse 3D point clouds from the scene. However, compared to human perception, such systems struggle to deduce the unseen parts of the scene given those sparse point clouds. In this matter, the scene completion task aims at predicting the gaps in the LiDAR measurements to achieve a more complete scene representation. Given the promising results of recent diffusion models as generative models for images, we propose extending them to achieve scene completion from a single 3D LiDAR scan. Previous works used diffusion models over range images extracted from LiDAR data, directly applying image-based diffusion methods. Distinctly, we propose to directly operate on the points, reformulating the noising and denoising diffusion process such that it can efficiently work at scene scale. Together with our approach, we propose a regularization loss to stabilize the noise predicted during the denoising process. Our experimental evaluation shows that our method can complete the scene given a single LiDAR scan as input, producing a scene with more details compared to state-of-the-art scene completion methods. We believe that our proposed diffusion process formulation can support further research in diffusion models applied to scene-scale point cloud data. 

[PDF](http://arxiv.org/abs/2403.13470v1) 

**Summary**
ä½¿ç”¨ 3D LiDAR æ‰«æå•æ¬¡å®Œæˆåœºæ™¯ç‚¹äº‘ï¼Œæ‰©å……æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒé¢†åŸŸçš„åº”ç”¨ã€‚

**Key Takeaways**
- æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä¸Šçš„æˆåŠŸåº”ç”¨å¯å‘äº†å…¶åœ¨ç‚¹äº‘åœºæ™¯å®Œæˆä»»åŠ¡ä¸Šçš„æ½œåŠ›ã€‚
- ä»¥å¾€å°† LiDAR æ•°æ®æå–èŒƒå›´å›¾åƒçš„æ–¹æ³•ä¸é€‚ç”¨äºåœºæ™¯å°ºåº¦çš„æ•°æ®å¤„ç†ã€‚
- è¯¥ç ”ç©¶ç›´æ¥å¯¹ç‚¹äº‘æ“ä½œï¼Œé‡æ–°åˆ¶å®šäº†æ‰©æ•£è¿‡ç¨‹ï¼Œä»¥æœ‰æ•ˆå¤„ç†åœºæ™¯å°ºåº¦æ•°æ®ã€‚
- æå‡ºæ­£åˆ™åŒ–æŸå¤±æ¥ç¨³å®šå»å™ªè¿‡ç¨‹ä¸­çš„é¢„æµ‹å™ªå£°ã€‚
- å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥å•æ¬¡ LiDAR æ‰«æå®Œæˆåœºæ™¯ï¼Œç”Ÿæˆæ›´ç²¾ç»†çš„åœºæ™¯ã€‚
- è¯¥ç ”ç©¶æå‡ºçš„æ‰©æ•£è¿‡ç¨‹å…¬å¼å¯ä¸ºåŸºäºç‚¹äº‘æ•°æ®çš„æ‰©æ•£æ¨¡å‹ç ”ç©¶æä¾›æ”¯æŒã€‚
- ç›´æ¥æ“ä½œç‚¹äº‘çš„æ–¹æ³•é¿å…äº†å›¾åƒå¤„ç†ä¸­çš„é‡‡æ ·å’Œé‡åŒ–è¯¯å·®ï¼Œä¿ç•™äº†åœºæ™¯çš„å‡ ä½•ä¿¡æ¯ã€‚

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>æ ‡é¢˜ï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„çœŸå®ä¸–ç•Œ 3D æ¿€å…‰é›·è¾¾åœºæ™¯è¡¥å…¨</li>
<li>ä½œè€…ï¼šLukas Lyu, Alexander Meuleman, Christian Haene</li>
<li>éš¶å±æœºæ„ï¼šæ³¢æ©å¤§å­¦</li>
<li>å…³é”®è¯ï¼šæ¿€å…‰é›·è¾¾åœºæ™¯è¡¥å…¨ï¼Œæ‰©æ•£æ¨¡å‹ï¼Œç‚¹äº‘å¤„ç†</li>
<li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.13470
   Github ä»£ç é“¾æ¥ï¼šhttps://github.com/PRBonn/LiDiff</li>
<li>æ‘˜è¦ï¼š
(1) ç ”ç©¶èƒŒæ™¯ï¼šè®¡ç®—æœºè§†è§‰æŠ€æœ¯åœ¨è‡ªåŠ¨é©¾é©¶çš„æ„ŸçŸ¥å †æ ˆä¸­å‘æŒ¥ç€æ ¸å¿ƒä½œç”¨ã€‚è¿™äº›æ–¹æ³•ç”¨äºç»™å®šä¼ æ„Ÿå™¨æ•°æ®æ„ŸçŸ¥è½¦è¾†å‘¨å›´ç¯å¢ƒã€‚3D æ¿€å…‰é›·è¾¾ä¼ æ„Ÿå™¨é€šå¸¸ç”¨äºä»åœºæ™¯ä¸­æ”¶é›†ç¨€ç– 3D ç‚¹äº‘ã€‚ç„¶è€Œï¼Œä¸äººç±»æ„ŸçŸ¥ç›¸æ¯”ï¼Œè¿™äº›ç³»ç»Ÿéš¾ä»¥ä»…å‡­è¿™äº›ç¨€ç–ç‚¹äº‘æ¨æ–­å‡ºåœºæ™¯ä¸­ä¸å¯è§çš„éƒ¨åˆ†ã€‚åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œåœºæ™¯è¡¥å…¨ä»»åŠ¡æ—¨åœ¨é¢„æµ‹æ¿€å…‰é›·è¾¾æµ‹é‡ä¸­çš„ç©ºç™½ï¼Œä»¥å®ç°æ›´å®Œæ•´çš„åœºæ™¯è¡¨ç¤ºã€‚é‰´äºæœ€è¿‘æ‰©æ•£æ¨¡å‹ä½œä¸ºå›¾åƒç”Ÿæˆæ¨¡å‹å–å¾—çš„è‰¯å¥½ç»“æœï¼Œæˆ‘ä»¬æå‡ºå°†å®ƒä»¬æ‰©å±•åˆ°å®ç°å•æ¬¡ 3D æ¿€å…‰é›·è¾¾æ‰«æçš„åœºæ™¯è¡¥å…¨ã€‚
(2) è¿‡å»çš„æ–¹æ³•ï¼šä»¥å‰çš„å·¥ä½œä½¿ç”¨ä»æ¿€å…‰é›·è¾¾æ•°æ®ä¸­æå–çš„èŒƒå›´å›¾åƒä¸Šçš„æ‰©æ•£æ¨¡å‹ï¼Œç›´æ¥åº”ç”¨åŸºäºå›¾åƒçš„æ‰©æ•£æ–¹æ³•ã€‚ä¸åŒçš„æ˜¯ï¼Œæˆ‘ä»¬æå‡ºç›´æ¥å¯¹ç‚¹è¿›è¡Œæ“ä½œï¼Œé‡æ–°è¡¨è¿°åŠ å™ªå’Œå»å™ªæ‰©æ•£è¿‡ç¨‹ï¼Œä½¿å…¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†åœºæ™¯è§„æ¨¡ã€‚
(3) ç ”ç©¶æ–¹æ³•ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ­£åˆ™åŒ–æŸå¤±æ¥ç¨³å®šå»å™ªè¿‡ç¨‹ä¸­é¢„æµ‹çš„å™ªå£°ã€‚æˆ‘ä»¬çš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ä»…ä»¥å•æ¬¡æ¿€å…‰é›·è¾¾æ‰«æä½œä¸ºè¾“å…¥æ¥å®Œæˆåœºæ™¯ï¼Œä¸æœ€å…ˆè¿›çš„åœºæ™¯è¡¥å…¨æ–¹æ³•ç›¸æ¯”ï¼Œç”Ÿæˆçš„åœºæ™¯å…·æœ‰æ›´å¤šç»†èŠ‚ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œæˆ‘ä»¬æå‡ºçš„æ‰©æ•£è¿‡ç¨‹å…¬å¼å¯ä»¥æ”¯æŒå°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºåœºæ™¯è§„æ¨¡ç‚¹äº‘æ•°æ®çš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚
(4) æ–¹æ³•çš„åº”ç”¨å’Œæ€§èƒ½ï¼šæˆ‘ä»¬çš„æ–¹æ³•åœ¨åœºæ™¯è¡¥å…¨ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å®ƒå¯ä»¥ç”Ÿæˆå…·æœ‰æ›´å¤šç»†èŠ‚çš„å®Œæ•´åœºæ™¯ï¼Œå¹¶ä¸”æ¯”ç°æœ‰æ–¹æ³•æ›´èƒ½ä¿ç•™åœºæ™¯çš„å‡ ä½•ç»“æ„ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•çš„ç›®æ ‡ï¼Œå³å¼€å‘ä¸€ç§æœ‰æ•ˆä¸”å‡†ç¡®çš„åœºæ™¯è¡¥å…¨æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥ä»…ä½¿ç”¨å•æ¬¡æ¿€å…‰é›·è¾¾æ‰«ææ¥ç”Ÿæˆé«˜è´¨é‡çš„åœºæ™¯è¡¨ç¤ºã€‚</li>
</ol>
<p><strong>æ–¹æ³•</strong></p>
<p>ï¼ˆ1ï¼‰æˆ‘ä»¬æå‡ºä½¿ç”¨ DDPM ä»å•ä¸ª 3D æ¿€å…‰é›·è¾¾æ‰«æä½œä¸ºè¾“å…¥æ¥å®ç°åœºæ™¯è¡¥å…¨ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°† DDPM [19,20,47] é‡æ–°è¡¨è¿°ä¸ºé€‚ç”¨äºåœºæ™¯è§„æ¨¡ã€‚æˆ‘ä»¬ä¸å½’ä¸€åŒ–è¾“å…¥ç‚¹äº‘ï¼Œè€Œæ˜¯é’ˆå¯¹æ¯ä¸ªç‚¹å±€éƒ¨æ·»åŠ å’Œé¢„æµ‹å™ªå£°ã€‚åœ¨å»å™ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨è¾“å…¥æ‰«æå¯¹å™ªå£°é¢„æµ‹è¿›è¡Œæ¡ä»¶åŒ–ï¼Œä»¥ä¾¿æœ€ç»ˆåœºæ™¯ä¿ç•™è¾“å…¥æ‰«æçš„ç»“æ„ä¿¡æ¯ï¼ŒåŒæ—¶æ¨æ–­å‡ºç¼ºå¤±éƒ¨åˆ†ã€‚åœ¨è¿™ç§è¡¨è¿°ä¸­ï¼Œåˆå§‹ç‚¹äº‘æ˜¯è¾“å…¥æ‰«æçš„å™ªå£°ç‰ˆæœ¬ï¼Œç„¶åç½‘ç»œçš„ä»»åŠ¡æ˜¯å»å™ªä»¥è·å¾—å®Œæ•´åœºæ™¯ï¼Œå¦‚å›¾ 1 æ‰€ç¤ºã€‚</p>
<p>ï¼ˆ2ï¼‰æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æä¾›äº†æ‰©æ•£æ¨¡å‹çš„å¿…è¦èƒŒæ™¯ï¼Œå¹¶æè¿°äº†æˆ‘ä»¬æ–¹æ³•çš„å„ä¸ªç»„æˆéƒ¨åˆ†ã€‚</p>
<p>ï¼ˆ3ï¼‰å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼šå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ [6,11,27] å°†æ•°æ®ç”Ÿæˆè¡¨è¿°ä¸ºä¸€ä¸ªè¿­ä»£å»å™ªè¿‡ç¨‹ã€‚é€šå¸¸ï¼Œæ¨¡å‹ä»é«˜æ–¯å™ªå£° [6,11,27] å¼€å§‹ï¼Œå¹¶ä»è¾“å…¥ä¸­è¿­ä»£ç§»é™¤å™ªå£°ï¼Œç›´åˆ°æ”¶æ•›åˆ°ç›®æ ‡è¾“å‡ºï¼ˆä¾‹å¦‚ï¼Œå›¾åƒ [6,11,27,28,30,33,48,49] æˆ–å½¢çŠ¶ [19,20,35,36,43,45,47]ï¼‰ã€‚è¿™å¯ä»¥é€šè¿‡å®šä¹‰ä¸€ä¸ªå‰å‘æ‰©æ•£è¿‡ç¨‹æ¥å®ç°ï¼Œå…¶ä¸­å™ªå£°åœ¨ T æ¬¡ä¸­è¿­ä»£æ·»åŠ åˆ°ç›®æ ‡æ•°æ®ä¸­ã€‚ç„¶åï¼Œè®­ç»ƒæ¨¡å‹æ¥é¢„æµ‹åœ¨æ¯ä¸ªæ­¥éª¤ä¸­æ·»åŠ çš„å™ªå£°ã€‚é€šè¿‡é¢„æµ‹æ¯ä¸ªæ­¥éª¤çš„å™ªå£°å¹¶å°†å…¶ç§»é™¤ï¼Œå»å™ªæ ·æœ¬åº”è¯¥æ›´æ¥è¿‘ç›®æ ‡è®­ç»ƒæ•°æ®ã€‚Ho ç­‰äºº [11] è¡¨è¿°çš„æ‰©æ•£è¿‡ç¨‹é€šå¸¸å¯ä»¥å†™æˆå¦‚ä¸‹å½¢å¼ã€‚ç»™å®šä»ç›®æ ‡æ•°æ®åˆ†å¸ƒä¸­æŠ½å–çš„æ ·æœ¬ x0âˆ¼q(x)ï¼Œæ‰©æ•£è¿‡ç¨‹åœ¨ T æ­¥ä¸­å‘ x0 æ·»åŠ å™ªå£°ï¼Œå¾—åˆ° x1,...,xTï¼Œå…¶ä¸­ qï¿½xTï¿½â‰ˆN(0,I)ï¼Œå…¶ä¸­ N(0,I) æ˜¯å‡å€¼ä¸º 0ã€å¯¹è§’åæ–¹å·®ä¸ºå•ä½çŸ©é˜µ I çš„æ­£æ€åˆ†å¸ƒã€‚è¿™ä¸ªæ‰©æ•£è¿‡ç¨‹ç”±ä¸€ç³»åˆ—å®šä¹‰çš„å™ªå£°å› å­ Î²1,...,Î²T å‚æ•°åŒ–ï¼Œå…¶ä¸­åœ¨æ¯ä¸ªæ­¥éª¤ t ä¸­ï¼Œè¿­ä»£é‡‡æ ·é«˜æ–¯å™ªå£°å¹¶æ ¹æ® Î²t æ·»åŠ åˆ° xtâˆ’1 ä¸­ã€‚è¿™å¯ä»¥ç®€åŒ–ä¸ºä» x0 é‡‡æ · xtï¼Œè€Œæ— éœ€è®¡ç®—ä¸­é—´æ­¥éª¤ x1,...,xtâˆ’1ã€‚ä¸ºæ­¤ï¼ŒHo ç­‰äºº [11] å®šä¹‰ Î±t=1âˆ’Î²t å’Œ Î±t=ï¿½ti=1Î±iï¼Œå¹¶ä¸” xt å¯ä»¥é‡‡æ ·ä¸ºï¼šxt=âˆšÎ±tx0+âˆš1âˆ’Î±tÏµ,(1)å…¶ä¸­ Ïµâˆ¼N(0,I)ã€‚æ³¨æ„ï¼Œå½“ T è¶³å¤Ÿå¤§æ—¶ qï¿½xTï¿½â‰ˆN(0,I)ï¼Œå› ä¸º Î±T æ¥è¿‘äºé›¶ã€‚å»å™ªè¿‡ç¨‹æ—¨åœ¨é€šè¿‡é¢„æµ‹åœ¨æ¯ä¸ªæ­¥éª¤æ·»åŠ çš„å™ªå£° Ïµ æ¥æ’¤æ¶ˆ T ä¸ªå™ªå£°æ­¥éª¤ [11]ã€‚ç»™å®šä¸€ä¸ªåˆå§‹ xTï¼Œæˆ‘ä»¬å¸Œæœ›é€†è½¬æ‰©æ•£è¿‡ç¨‹å¹¶å¾—åˆ° x0ã€‚åå‘æ‰©æ•£æ­¥éª¤å¯ä»¥å†™æˆï¼šxtâˆ’1=xtâˆ’1âˆ’Î±tâˆš1âˆ’Î±tÏµÎ¸ï¿½xt,tï¿½+1âˆ’Î±tâˆ’11âˆ’Î±tÎ²tN(0,I),(2)å…¶ä¸­ ÏµÎ¸(xt,t) æ˜¯åœ¨æ­¥éª¤ t ä» xt é¢„æµ‹çš„å™ªå£°ã€‚è¿™ä¸ªç”Ÿæˆä¹Ÿå¯ä»¥åœ¨ç»™å®šæ¡ä»¶ c çš„æƒ…å†µä¸‹è¿›è¡Œå¼•å¯¼ã€‚è¿™ç§æ¡ä»¶ç”Ÿæˆå¯ä»¥æ¥è‡ªé¢„è®­ç»ƒçš„ç¼–ç å™¨ [6] æˆ–æ— åˆ†ç±»å™¨æŒ‡å¯¼ [10]ï¼Œå…¶ä¸­ç¼–ç å™¨ä¸å™ªå£°é¢„æµ‹å™¨ä¸€èµ·è®­ç»ƒã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æ— åˆ†ç±»å™¨æŒ‡å¯¼ï¼Œå› ä¸ºå®ƒä¸éœ€è¦é¢„è®­ç»ƒçš„ç¼–ç å™¨ã€‚ä½¿ç”¨æ— åˆ†ç±»å™¨æŒ‡å¯¼ï¼Œæ¨¡å‹è¢«è®­ç»ƒæ¥å­¦ä¹ æ¡ä»¶å’Œæ— æ¡ä»¶å™ªå£°åˆ†å¸ƒã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤ä¸­ï¼Œæ¨¡å‹éƒ½æœ‰ä¸€å®šçš„æ¦‚ç‡ p é¢„æµ‹æ— æ¡ä»¶å™ªå£°åˆ†å¸ƒï¼Œå…¶ä¸­æ¡ä»¶è®¾ç½®ä¸º null ä»¤ç‰Œï¼Œå³ c=âˆ…ã€‚è®­ç»ƒè¿‡ç¨‹ä¼˜åŒ–å»å™ªæ¨¡å‹ä»¥é¢„æµ‹ç»™å®šè¾“å…¥æ·»åŠ åˆ°æ­¥éª¤çš„å™ªå£° Ïµã€‚ç»™å®šè¾“å…¥ x0 å’Œæ¡ä»¶ cï¼Œéšæœºé‡‡æ ·æ­¥éª¤ tâˆˆ[0,T]ï¼Œå¹¶ä½¿ç”¨é«˜æ–¯å™ªå£° Ïµ ä»æ–¹ç¨‹å¼ (1) é‡‡æ · xtã€‚ç„¶åï¼Œä» xtã€c å’Œ tï¼Œæ¨¡å‹è®¡ç®—å™ªå£°é¢„æµ‹ï¼Œå¹¶ä½¿ç”¨ L2 æŸå¤±å¯¹å…¶è¿›è¡Œç›‘ç£ï¼šLï¿½xt,Ëœc,tï¿½=ï¿½ï¿½Ïµâˆ’ÏµÎ¸ï¿½xt,Ëœc,tï¿½ï¿½ï¿½2,(3)å…¶ä¸­ Ëœcâˆ¼B(p) å…¶ä¸­ B æ˜¯ä¼¯åŠªåˆ©åˆ†å¸ƒï¼Œæ²¡æœ‰ç»“æœ {âˆ…,c}ï¼Œâˆ… å‘ç”Ÿçš„æ¦‚ç‡ä¸º pã€‚æ¨æ–­ä»åˆå§‹ xTâˆ¼N(0,I) å¼€å§‹ï¼Œå¹¶è¿­ä»£å»å™ªä»¥è·å¾— x0ã€‚å¯¹äºæ— åˆ†ç±»å™¨æŒ‡å¯¼ [10]ï¼Œæˆ‘ä»¬é¢„æµ‹æ¡ä»¶å’Œæ— æ¡ä»¶å™ªå£°åˆ†å¸ƒï¼Œå¹¶è®¡ç®—æœ€ç»ˆé¢„æµ‹çš„å™ªå£°ä¸ºï¼šÏµâ€²Î¸ï¿½xt,c,tï¿½=ÏµÎ¸ï¿½xt,âˆ…,tï¿½+sï¿½ÏµÎ¸ï¿½xt,c,tï¿½âˆ’ÏµÎ¸ï¿½xt,âˆ…,tï¿½ï¿½,(4)å…¶ä¸­ sâˆˆR æ˜¯å¯¹ c è¿›è¡ŒåŠ æƒçš„æ¡ä»¶å‚æ•°ï¼ŒÏµÎ¸(xt,âˆ…,t) æ˜¯æ— æ¡ä»¶å™ªå£°é¢„æµ‹ã€‚ä½¿ç”¨æ–¹ç¨‹å¼ (4)ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä»»ä½•æ­¥éª¤ä¸­è®¡ç®—å™ªå£°ï¼Œä»ä¸­æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ–¹ç¨‹å¼ (2) è®¡ç®— xTâˆ’1,...,x0ï¼Œå…¶ä¸­ x0 æ˜¯æ¡ä»¶ä¸º c çš„æ–°ç”Ÿæˆæ ·æœ¬ã€‚</p>
<p>ï¼ˆ4ï¼‰æ‰©æ•£åœºæ™¯è¡¥å…¨ï¼šåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ DDPM çš„ç”Ÿæˆæ–¹é¢æ¥å®Œæˆæ¿€å…‰é›·è¾¾ä¼ æ„Ÿå™¨åœ¨å•ä¸ªæ‰«æä¸­æµ‹é‡çš„åœºæ™¯ã€‚ä¸å½¢çŠ¶è¡¥å…¨ [19,20,47] ç±»ä¼¼ï¼Œè¾“å…¥æ˜¯ä¸€ä¸ªéƒ¨åˆ†ç‚¹äº‘ P={p1,...,pN} å…¶ä¸­ pâˆˆR3ï¼Œè¾“å‡ºåº”è¯¥æ˜¯å®Œæ•´ç‚¹äº‘ Pâ€²={pâ€²1,...,pâ€²M} å…¶ä¸­ pâ€²âˆˆR3ã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œéƒ¨åˆ†ç‚¹äº‘æ˜¯å•ä¸ªæ¿€å…‰é›·è¾¾æ‰«æï¼Œæˆ‘ä»¬å¸Œæœ›ä»ä¸­å®ç°åœºæ™¯è¡¥å…¨ã€‚ç»™å®šä¸€ç³»åˆ—è¿ç»­çš„æ¿€å…‰é›·è¾¾æ‰«æåŠå…¶å§¿æ€ï¼Œæˆ‘ä»¬å¯ä»¥æ„å»ºä¸€ä¸ªåœ°å›¾ï¼Œå¹¶é’ˆå¯¹å•ä¸ªæ‰«æ P é‡‡æ ·å®Œæ•´åœºæ™¯ ground truth Gï¼Œå…¶ä¸­æˆ‘ä»¬çš„åœºæ™¯è¡¥å…¨ Pâ€² åº”è¯¥å°½å¯èƒ½æ¥è¿‘ Gã€‚ç»™å®šè¾“å…¥æ‰«æ P å’Œ ground truth G çš„å¯¹ï¼Œæˆ‘ä»¬å¯ä»¥è®­ç»ƒ DDPM æ¥å®ç°åœºæ™¯è¡¥å…¨ã€‚æ­£å¦‚ç¬¬ 3.1 èŠ‚æ‰€è¿°ï¼Œæˆ‘ä»¬å¯ä»¥ä»å®Œæ•´åœºæ™¯ G ä¸­è®¡ç®—æ­¥éª¤ t çš„å™ªå£°ç‚¹äº‘ Gtï¼šptm=âˆšÎ±tpm+âˆš1âˆ’Î±tÏµ,âˆ€pmâˆˆG,(5)å…¶ä¸­ Gt={pt1,...,ptM}ã€‚</p>
<p>ï¼ˆ5ï¼‰å±€éƒ¨ç‚¹å»å™ªï¼šç¬¬ 3.2 èŠ‚ä¸­è¯¦ç»†æè¿°çš„è¡¨è¿°é€šå¸¸ç”¨äºå½¢çŠ¶è¡¥å…¨ [20,47]ã€‚å°½ç®¡å½¢çŠ¶è¡¥å…¨å–å¾—äº†æœ‰å¸Œæœ›çš„ç»“æœï¼Œä½†è¯¥è¡¨è¿°å¯èƒ½ä¸ç›´æ¥é€‚ç”¨äºåœºæ™¯è§„æ¨¡ã€‚å¯¹äºå•ä¸ªç‰©ä½“å½¢çŠ¶ï¼Œæ•°æ®è¦ä¹ˆå½’ä¸€åŒ–ï¼Œè¦ä¹ˆå¤„äºæ¥è¿‘å‡å€¼ Âµ=0 å’Œæ ‡å‡†å·® Î£=I çš„é«˜æ–¯åˆ†å¸ƒçš„å°èŒƒå›´å†…ã€‚å¯¹äºåœºæ™¯è§„æ¨¡ï¼Œæ¿€å…‰é›·è¾¾æ•°æ®å…·æœ‰æ›´å¤§çš„æ¯”ä¾‹ï¼Œå¹¶ä¸”æ•°æ®èŒƒå›´å› ç‚¹äº‘è½´è€Œå¼‚ã€‚å› æ­¤ï¼Œè¾“å…¥æ•°æ®åˆ†å¸ƒè¿œéé«˜æ–¯åˆ†å¸ƒ N(0,I)ï¼Œå¦‚æœæˆ‘ä»¬å¯¹æ•°æ®è¿›è¡Œå½’ä¸€åŒ–ï¼Œæˆ‘ä»¬å°†ä¸¢å¤±åœºæ™¯ä¸­çš„è®¸å¤šç»†èŠ‚ï¼Œå› ä¸ºåœºæ™¯è¢«å‹ç¼©åˆ°ä¸€ä¸ª much smaller range ä¸­ï¼Œå¦‚å›¾ 2 æ‰€ç¤ºã€‚ä¸ºäº†å…‹æœè¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å°†æ‰©æ•£è¿‡ç¨‹é‡æ–°è¡¨è¿°ä¸ºä¸€ä¸ªå±€éƒ¨é—®é¢˜ã€‚æˆ‘ä»¬ä¸å°† xt é‡‡æ ·ä¸º Ïµâˆ¼N(0,I) å’Œ x0 ä¹‹é—´çš„æ··åˆåˆ†å¸ƒï¼Œå¦‚æ–¹ç¨‹å¼ (1) æ‰€ç¤ºï¼Œè€Œæ˜¯å°†æ‰©æ•£è¿‡ç¨‹è¡¨è¿°ä¸ºå±€éƒ¨æ·»åŠ åˆ°æ¯ä¸ªç‚¹ pm çš„å™ªå£°åç§»ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä»æ–¹ç¨‹å¼ (1) ä¸­ï¼Œæˆ‘ä»¬è®¾ç½® x0=0 å¹¶å°† xt æ·»åŠ åˆ° pmï¼šptm=pm+ï¿½âˆšÎ±t0+âˆš1âˆ’Î±tÏµï¿½,(8)=pm+âˆš1âˆ’Î±tÏµ.(9)</p>
<ol>
<li>ç»“è®ºï¼š
(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ¿€å…‰é›·è¾¾åœºæ™¯è¡¥å…¨æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ä»å•ä¸ªç¨€ç–æ¿€å…‰é›·è¾¾æ‰«æä¸­ç”Ÿæˆç¼ºå¤±éƒ¨åˆ†ã€‚æˆ‘ä»¬åœ¨å±€éƒ¨ç‚¹å»å™ªä¸­é‡æ–°è¡¨è¿°äº†æ‰©æ•£è¿‡ç¨‹ï¼Œå°†æ¯ä¸ªç‚¹å®šä¹‰ä¸ºé‡‡æ ·é«˜æ–¯å™ªå£°çš„åŸç‚¹ï¼Œå­¦ä¹ äº†ä¸€ä¸ªè¿­ä»£å»å™ªè¿‡ç¨‹ï¼Œä»¥é€æ­¥é¢„æµ‹åç§»é‡ï¼Œä»è€Œä»è¾“å…¥çš„å™ªå£°æ¿€å…‰é›·è¾¾æ‰«æé‡å»ºåœºæ™¯ã€‚è¿™ç§è¡¨è¿°ä½¿å¾—å¤„ç†åœºæ™¯è§„æ¨¡çš„ 3D æ•°æ®æˆä¸ºå¯èƒ½ï¼Œåœ¨å»å™ªè¿‡ç¨‹ä¸­ä¿ç•™äº†æ›´å¤šç»†èŠ‚ã€‚
(2): åˆ›æ–°ç‚¹ï¼š</li>
<li>æå‡ºäº†ä¸€ç§å±€éƒ¨ç‚¹å»å™ªæ–¹æ³•ï¼Œå°†æ‰©æ•£è¿‡ç¨‹é‡æ–°è¡¨è¿°ä¸ºå±€éƒ¨é—®é¢˜ï¼Œå°†æ¯ä¸ªç‚¹å®šä¹‰ä¸ºé‡‡æ ·é«˜æ–¯å™ªå£°çš„åŸç‚¹ï¼Œå­¦ä¹ äº†ä¸€ä¸ªè¿­ä»£å»å™ªè¿‡ç¨‹ï¼Œä»¥é€æ­¥é¢„æµ‹åç§»é‡ï¼Œä»è€Œä»è¾“å…¥çš„å™ªå£°æ¿€å…‰é›·è¾¾æ‰«æé‡å»ºåœºæ™¯ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿå¤„ç†åœºæ™¯è§„æ¨¡çš„ 3D æ•°æ®ï¼Œåœ¨å»å™ªè¿‡ç¨‹ä¸­ä¿ç•™äº†æ›´å¤šç»†èŠ‚ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨åœºæ™¯è¡¥å…¨ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸æœ€å…ˆè¿›çš„æ‰©æ•£å’Œéæ‰©æ•£æ–¹æ³•ç›¸æ¯”ï¼Œç”Ÿæˆçš„åœºæ™¯å…·æœ‰æ›´å¤šç»†èŠ‚ã€‚
æ€§èƒ½ï¼š</li>
<li>è¯¥æ–¹æ³•åœ¨åœºæ™¯è¡¥å…¨ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>ä¸æœ€å…ˆè¿›çš„æ‰©æ•£å’Œéæ‰©æ•£æ–¹æ³•ç›¸æ¯”ï¼Œç”Ÿæˆçš„åœºæ™¯å…·æœ‰æ›´å¤šç»†èŠ‚ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸åŒçš„æ•°æ®é›†ä¸Šå®ç°åœºæ™¯è¡¥å…¨ï¼Œå› ä¸ºå…¶ç”Ÿæˆæ˜¯æ ¹æ®è¾“å…¥æ¿€å…‰é›·è¾¾æ‰«æè¿›è¡Œè°ƒèŠ‚çš„ã€‚
å·¥ä½œé‡ï¼š</li>
<li>è¯¥æ–¹æ³•éœ€è¦ä¸€ä¸ªè¾“å…¥æ‰«ææ¥æŒ‡å¯¼ç”Ÿæˆï¼Œè¿™é™åˆ¶äº†æ•°æ®ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•ç›®å‰è¿˜ä¸èƒ½ç”Ÿæˆæ— æ¡ä»¶æ•°æ®ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨ç”Ÿæˆæ–°é¢–çš„ 3D ç‚¹äº‘åœºæ™¯æ–¹é¢çš„åº”ç”¨ã€‚</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-53c84c0442dd3d75fd891aa17f099a89.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7912e5b6f5a348c3d71de1a3cf80265b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b2627632f0f47e537f8969486c44b93a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8380fe238cf40cd25f36e52373bb013.jpg" align="middle">
</details>




